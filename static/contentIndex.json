{"adr/ADR-0001_backlog-structure-and-moc":{"title":"Backlog structure: per-type folders and Obsidian MOC","links":["items/task/0000/KABSD-TSK-0001_project-backlog-skill"],"tags":[],"content":"Decision\nAdopt per-type folders for backlog items and use Obsidian-style MOC (manual links),\r\nwith Dataview as a supplemental auto list. Keep index files at the Epic level\r\nonly to reduce file count. Bucket items by ID range (per 100) under each type\r\nto reduce large folder sizes.\nContext\nThe backlog system needs to be readable without heavy tooling and should render\r\nclearly in Obsidian. DataviewJS was not reliable in the current setup.\nLinks\n\nRelated: KABSD-TSK-0001 Create project-backlog skill\n\nOptions Considered\n\nFlat items/ folder + DataviewJS-only index\nPer-type folders + DataviewJS index\nPer-type folders + manual MOC links + Dataview (non-JS) lists\n\nPros / Cons\n\nOption 1: simple path; but JS rendering was unreliable.\nOption 2: clearer organization; still depends on DataviewJS.\nOption 3: stable MOC links (Graph-friendly) and optional Dataview lists; slightly more manual upkeep.\n\nConsequences\n\nEpic index files must be updated when items move or are renamed.\nFeature/UserStory rely on Epic MOC links instead of their own index files.\nItem files live under _kano/backlog/items/&lt;type&gt;/&lt;bucket&gt;/ to avoid large directories.\nGraph view will reflect MOC link structure.\n\nFollow-ups\n\nKeep skills/kano-agent-backlog-skill/SKILL.md aligned with this structure.\n"},"adr/ADR-0002_decisions-as-adr-links":{"title":"Decision handling: ADRs stay in decisions/ with item links","links":["items/feature/0000/KABSD-FTR-0001_local-backlog-system"],"tags":[],"content":"Decision\nKeep decisions as ADR documents under _kano/backlog/decisions/ and link them\r\nfrom related work items via the decisions frontmatter field and a Links entry.\r\nDo not model ADRs as backlog work items for now.\nContext\nWe discussed whether decisions should be treated as work items. The current\r\nbacklog uses ADRs to capture durable rationale without turning them into\r\nworkflow tickets. We want to preserve clarity while avoiding ticket sprawl.\nLinks\n\nRelated: KABSD-FTR-0001 Local-first backlog system\n\nOptions Considered\n\nKeep ADRs as separate docs under decisions/ and link from items.\nTreat decisions as a new work item type under items/ with states.\nHybrid: keep ADRs under decisions/ plus optional lightweight decision tickets.\n\nPros / Cons\n\nOption 1: clear separation and low overhead; requires linking discipline.\nOption 2: full backlog visibility; risks turning decisions into ticket noise.\nOption 3: flexible; extra ceremony and more items to maintain.\n\nConsequences\n\nADRs remain outside the work item state machine.\nWork items should record ADR IDs in decisions: [] and Links.\nDashboards may need a separate decision view if visibility is required.\n\nFollow-ups\n\nUpdate related items to link ADR-0002.\n"},"adr/ADR-0003-appendix_collision-report-cli-spec":{"title":"ADR-0003-appendix_collision-report-cli-spec","links":[],"tags":[],"content":"Collision Report &amp; Resolver CLI\nID collision reporting and resolver CLI tool specifications\nOverview\nProvides two tools:\n\nworkitem_collision_report.py - Scan and report display ID collisions\nworkitem_resolve_ref.py - Interactive reference resolution\n\nworkitem_collision_report.py\nFeatures\nScans all items under _kano/backlog/items/ to find duplicate display id cases.\nUsage\n# Basic report\npython scripts/backlog/workitem_collision_report.py\n \n# JSON output\npython scripts/backlog/workitem_collision_report.py --format json\n \n# Show collisions only\npython scripts/backlog/workitem_collision_report.py --collisions-only\n \n# Specify backlog path\npython scripts/backlog/workitem_collision_report.py --backlog-root _kano/backlog\nOutput example\nID Collision Report\r\n===================\r\nGenerated: 2026-01-06 01:30\r\nScanned: 85 items\r\n\r\nCollisions Found: 1\r\n\r\nID: KABSD-TSK-0100 (2 items)\r\n  1. 019473f2 | Task | Done  | First implementation | items/tasks/0000/...\r\n  2. 01947428 | Task | New   | Second attempt      | items/tasks/0000/...\r\n  Suggestion: Use KABSD-TSK-0100@019473f2 or KABSD-TSK-0100@01947428\r\n\r\nNo other collisions found.\n\nJSON output format\n{\n  &quot;generated&quot;: &quot;2026-01-06T01:30:00&quot;,\n  &quot;total_items&quot;: 85,\n  &quot;collision_count&quot;: 1,\n  &quot;collisions&quot;: [\n    {\n      &quot;id&quot;: &quot;KABSD-TSK-0100&quot;,\n      &quot;items&quot;: [\n        {\n          &quot;uid&quot;: &quot;019473f2-79b0-7cc3-98c4-dc0c0c07398f&quot;,\n          &quot;uidshort&quot;: &quot;019473f2&quot;,\n          &quot;type&quot;: &quot;Task&quot;,\n          &quot;state&quot;: &quot;Done&quot;,\n          &quot;title&quot;: &quot;First implementation&quot;,\n          &quot;path&quot;: &quot;items/tasks/0000/KABSD-TSK-0100_first-impl.md&quot;\n        },\n        {\n          &quot;uid&quot;: &quot;01947428-1234-7abc-5678-def012345678&quot;,\n          &quot;uidshort&quot;: &quot;01947428&quot;,\n          &quot;type&quot;: &quot;Task&quot;,\n          &quot;state&quot;: &quot;New&quot;,\n          &quot;title&quot;: &quot;Second attempt&quot;,\n          &quot;path&quot;: &quot;items/tasks/0000/KABSD-TSK-0100_second-attempt.md&quot;\n        }\n      ]\n    }\n  ]\n}\nworkitem_resolve_ref.py\nFeatures\nResolves reference strings with interactive disambiguation support.\nUsage\n# Resolve reference\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0059\n \n# Use uidshort for exact match\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0100@019473f2\n \n# Interactive mode\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0100 --interactive\n \n# Output format\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0059 --format path   # path only\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0059 --format json   # JSON format\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0059 --format uid    # uid only\nOutput examples\nUnique match:\nResolved: KABSD-TSK-0059\r\n\r\n  UID:      019473f2-79b0-7cc3-98c4-dc0c0c07398f\r\n  ID:       KABSD-TSK-0059\r\n  Type:     Task\r\n  State:    Done\r\n  Title:    ULID vs UUIDv7 comparison document\r\n  Path:     items/tasks/0000/KABSD-TSK-0059_ulid-vs-uuidv7-comparison.md\r\n  Created:  2026-01-06\r\n  Updated:  2026-01-06\n\nMultiple matches (interactive mode):\nAmbiguous: 2 items match &quot;KABSD-TSK-0100&quot;\r\n\r\n  # | UID (short) | Type | State | Title\r\n  --|-------------|------|-------|------\r\n  1 | 019473f2    | Task | Done  | First implementation\r\n  2 | 01947428    | Task | New   | Second attempt\r\n\r\nEnter number to select (or &#039;q&#039; to quit): 1\r\n\r\nSelected: KABSD-TSK-0100@019473f2\r\nPath: items/tasks/0000/KABSD-TSK-0100_first-impl.md\n\nImplementation notes\nShared module\n# scripts/backlog/lib/index.py\n \nclass BacklogIndex:\n    def __init__(self, backlog_root: str):\n        self.items = self._scan_items(backlog_root)\n        self._build_indexes()\n    \n    def get_by_uid(self, uid: str) -&gt; Optional[BacklogItem]: ...\n    def get_by_uidshort(self, prefix: str) -&gt; List[BacklogItem]: ...\n    def get_by_id(self, display_id: str) -&gt; List[BacklogItem]: ...\n    def get_collisions(self) -&gt; Dict[str, List[BacklogItem]]: ...\nIntegration with existing skill scripts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExisting scriptIntegration approachworkitem_update_state.pyUse resolve_ref to allow id@uidshort parametersworkitem_create.pyAuto-generate UUIDv7 uidview_generate.pyOptionally display uidshort\nDeliverables\n\n scripts/backlog/workitem_collision_report.py\n scripts/backlog/workitem_resolve_ref.py\n scripts/backlog/lib/index.py (shared module)\n Unit tests\n"},"adr/ADR-0003-appendix_id-resolver-spec":{"title":"ADR-0003-appendix_id-resolver-spec","links":[],"tags":[],"content":"ID Resolver Specification\nResolveRef() function specification - for resolving backlog item references\nOverview\nPer ADR-0003, all reference resolution must go through the Resolver to support:\n\nFull uid exact match\nuidshort prefix match\nDisplay id match (may return multiple items)\n\nResolveRef() function specification\nSignature\ndef resolve_ref(\n    ref: str,\n    index: BacklogIndex,\n    interactive: bool = False\n) -&gt; ResolveResult:\n    &quot;&quot;&quot;\n    Resolve a reference to one or more backlog items.\n    \n    Args:\n        ref: Reference string (uid, uidshort, id, or id@uidshort format)\n        index: Backlog index instance\n        interactive: If True, prompt user for disambiguation\n        \n    Returns:\n        ResolveResult with matched item(s) or error\n    &quot;&quot;&quot;\nInput formats\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFormatExampleDescriptionFull uid019473f2-79b0-7cc3-98c4-dc0c0c07398f36 chars, hyphensuidshort019473f28 hex charsDisplay idKABSD-TSK-0059Project prefix + type + numberid@uidshortKABSD-TSK-0059@019473f2Human-friendly format\nResolution logic\ndef resolve_ref(ref: str, index: BacklogIndex) -&gt; ResolveResult:\n    # 1. Check if ref is full uid (36 chars with hyphens)\n    if is_full_uid(ref):\n        item = index.get_by_uid(ref)\n        if item:\n            return ResolveResult(matches=[item], exact=True)\n        return ResolveResult(error=f&quot;UID not found: {ref}&quot;)\n    \n    # 2. Check if ref contains @uidshort (e.g., KABSD-TSK-0059@019473f2)\n    if &quot;@&quot; in ref:\n        id_part, uidshort = ref.split(&quot;@&quot;, 1)\n        matches = index.get_by_id(id_part)\n        matches = [m for m in matches if m.uid.startswith(uidshort)]\n        if len(matches) == 1:\n            return ResolveResult(matches=matches, exact=True)\n        elif len(matches) &gt; 1:\n            return ResolveResult(matches=matches, exact=False, \n                error=&quot;Multiple matches even with uidshort&quot;)\n        return ResolveResult(error=f&quot;No match for {ref}&quot;)\n    \n    # 3. Check if ref is uidshort (8 hex chars)\n    if is_uidshort(ref):\n        matches = index.get_by_uidshort(ref)\n        if len(matches) == 1:\n            return ResolveResult(matches=matches, exact=True)\n        elif len(matches) &gt; 1:\n            return ResolveResult(matches=matches, exact=False)\n        return ResolveResult(error=f&quot;uidshort not found: {ref}&quot;)\n    \n    # 4. Assume ref is display id\n    matches = index.get_by_id(ref)\n    if len(matches) == 1:\n        return ResolveResult(matches=matches, exact=True)\n    elif len(matches) &gt; 1:\n        return ResolveResult(matches=matches, exact=False)\n    \n    return ResolveResult(error=f&quot;ID not found: {ref}&quot;)\nOutput structure\n@dataclass\nclass ResolveResult:\n    matches: List[BacklogItem] = field(default_factory=list)\n    exact: bool = False\n    error: Optional[str] = None\n    \n@dataclass\nclass BacklogItem:\n    uid: str\n    id: str\n    uidshort: str  # derived from uid[:8]\n    type: str\n    title: str\n    state: str\n    path: str\n    created: str\n    updated: str\nIndex requirements\nResolver requires the following index query capabilities:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQueryMethodDescriptionuid -&gt; itemget_by_uid(uid)Unique matchuidshort -&gt; [items]get_by_uidshort(prefix)Prefix matchid -&gt; [items]get_by_id(id)May return multiple items\nIndex Schema (SQLite)\nCREATE TABLE items (\n    uid TEXT PRIMARY KEY,\n    id TEXT NOT NULL,\n    uidshort TEXT NOT NULL,  -- first 8 hex chars\n    type TEXT,\n    title TEXT,\n    state TEXT,\n    path TEXT UNIQUE,\n    created TEXT,\n    updated TEXT\n);\n \nCREATE INDEX idx_id ON items(id);\nCREATE INDEX idx_uidshort ON items(uidshort);\nDisambiguation\nWhen exact=False and multiple matches exist, output candidate list:\nMultiple matches for &quot;KABSD-TSK-0100&quot;:\r\n\r\n  # | ID              | UID (short)  | Type | State | Title\r\n---------------------------------------------------------------\r\n  1 | KABSD-TSK-0100 | 019473f2     | Task | Done  | First task\r\n  2 | KABSD-TSK-0100 | 01947428     | Task | New   | Second task\r\n\r\nEnter number to select, or use: KABSD-TSK-0100@019473f2\n\nCLI integration\n# Resolve and show item details\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0059\n \n# Resolve with uidshort hint\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0100@019473f2\n \n# Interactive mode\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0100 --interactive\n \n# Output format\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0059 --format json\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0059 --format path\nError handling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaseError messageUID not foundError: UID not found: {uid}ID not foundError: ID not found: {id}Multiple matchesAmbiguous: {count} items match &quot;{ref}&quot;. Use id@uidshort format.Invalid formatError: Invalid reference format: {ref}"},"adr/ADR-0003-appendix_migration-plan-uid":{"title":"ADR-0003-appendix_migration-plan-uid","links":[],"tags":[],"content":"Migration Plan: Add uid to Existing Items\nA migration plan to add a UUIDv7 uid field to existing backlog items.\nOverview\nExisting backlog items currently only have id (display ID). Per ADR-0003, each item needs a uid (UUIDv7) as the immutable primary key.\nKey Decisions (ADR-0003):\n\nuid format: UUIDv7 (RFC 9562)\nFilenames remain unchanged (&lt;id&gt;_&lt;slug&gt;.md)\nuid is added only in frontmatter\n\nMigration Steps\nPhase 1: Preparation\n\n\nBack up the current backlog\ncp -r _kano/backlog _kano/backlog_backup_$(date +%Y%m%d)\n\n\nEnsure a UUIDv7 library is available\n\nPython: uuid6 package or Python 3.12+ built-in\nInstall: pip install uuid6\n\n\n\nPhase 2: Migration script\nCreate scripts/backlog/migration_add_uid.py:\n#!/usr/bin/env python3\n&quot;&quot;&quot;\nAdd uid (UUIDv7) to existing backlog items.\n \nUsage:\n    python migration_add_uid.py --dry-run  # Preview changes\n    python migration_add_uid.py --apply    # Apply changes\n&quot;&quot;&quot;\nimport uuid6  # or uuid (Python 3.12+)\n \ndef generate_uid():\n    &quot;&quot;&quot;Generate a UUIDv7 string.&quot;&quot;&quot;\n    return str(uuid6.uuid7())\n \ndef extract_uidshort(uid: str, length: int = 8) -&gt; str:\n    &quot;&quot;&quot;Extract uidshort (first N hex chars, no hyphens).&quot;&quot;&quot;\n    return uid.replace(&quot;-&quot;, &quot;&quot;)[:length]\nScript capabilities:\n\nScan all .md files under _kano/backlog/items/\nParse frontmatter\nIf uid is missing, generate a UUIDv7 and add it\nUpdate the updated field\nWrite changes back to the file\n\nPhase 3: Handle parent/link references\nBackward-compatibility strategy (incremental):\n\nKeep the existing parent field (by display id)\nOptionally add parent_uid\nA Resolver tool maps parent to the actual uid\n\n# Before migration\nparent: KABSD-FTR-0042\n \n# After migration (backward compatible)\nparent: KABSD-FTR-0042\nparent_uid: 019473f2-79b0-7cc3-98c4-dc0c0c07398f  # optional\nPhase 4: Validation\n\nRun a verification script to ensure every item has a uid\nVerify uid format correctness (UUIDv7)\nVerify dashboards render correctly\n\nFrontmatter schema changes\nNew fields\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFieldTypeRequiredNotesuidstringRequired (post-migration)UUIDv7, immutableparent_uidstringOptionalParent item’s uidaliaseslistOptionalLegacy IDs or alternate names\nExample\n---\nid: KABSD-TSK-0059\nuid: 019473f2-79b0-7cc3-98c4-dc0c0c07398f\ntype: Task\ntitle: &quot;ULID vs UUIDv7 comparison document&quot;\nstate: Done\npriority: P3\nparent: KABSD-FTR-0042\nparent_uid: 019473e8-1234-7abc-5678-def012345678  # optional\n# ... rest of frontmatter\n---\nuidshort specification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPropertyValueLength8 charactersSourceFirst 8 hex characters of uid (no hyphens)Example019473f2UsageHuman-friendly reference KABSD-TSK-0059@019473f2\nRisks / Mitigations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRiskMitigationMigration failure causing data corruptionBack up first; provide a dry-run modeuid collisionsExtremely unlikely with UUIDv7; add validation checksTool incompatibilityBackward-compatible: keep parent, add parent_uid\nImplementation order\n\n Create migration_add_uid.py\n Test in sandbox\n Dry-run preview\n Back up and perform migration\n Verify results\n Update related tools to support uid resolution\n"},"adr/ADR-0003-appendix_ulid-vs-uuidv7-comparison":{"title":"ADR-0003-appendix_ulid-vs-uuidv7-comparison","links":[],"tags":[],"content":"ULID vs UUIDv7 Comparison\nTechnical comparison to inform ADR-0003 uid format choice\nSummary\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristicULIDUUIDv7Length128 bits128 bitsString length26 chars (Base32)36 chars (hex + hyphens)Timestamp48 bits (ms)48 bits (ms)Random part80 bits74 bits (minus version/variant)StandardizationCommunity conventionIETF RFC 9562OrderingLexicographically sortableLexicographically sortableReadabilityShorter, no hyphensStandard UUID format\nDetailed comparison\n1. Format and readability\nULID\n01AN4Z07BY79KA1307SR9X4MV3\r\n|----------|----------------|\r\n Timestamp    Randomness\r\n  (10 ch)      (16 ch)\n\n\nUses Crockford’s Base32 (excludes I, L, O, U to avoid confusion)\nUppercase, no hyphens\n26 characters\n\nUUIDv7\n017f22e2-79b0-7cc3-98c4-dc0c0c07398f\r\n|-------|    |  |    |\r\n  time  ver  var  random\n\n\nStandard UUID format (8-4-4-4-12)\nHexadecimal with hyphens\n36 characters\n\n2. Ordering characteristics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAspectULIDUUIDv7Lexicographic ordering✅ Fully supported✅ Fully supportedSame-millisecond orderingMonotonic incrementCounter/randomCross-machine orderingTime precision onlyTime precision only\nBoth reflect time order correctly under lexicographic sort.\n3. Collision safety\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAspectULIDUUIDv7Random entropy80 bits~74 bitsSame-ms collision rate2^-802^-74Theoretical securityExtremely highExtremely high\nIn practice, both have negligible collision probability.\n4. Library support\nULID\n\nPython: python-ulid, ulid-py\nJavaScript: ulid (official)\nGo: oklog/ulid\nCommunity-driven, broad multi-language coverage\n\nUUIDv7\n\nPython: uuid6 (backport for &lt;3.x), built-in planned in Python 3.12+\nJavaScript: uuid@9+\nGo: google/uuid\nIETF standardized (RFC 9562); mainstream UUID libraries are adding support\n\n5. uidshort prefix length guidance\nULID\n\nFirst 10 characters = timestamp part\nRecommended uidshort: 8-10 characters (covers time + partial randomness)\nExample: 01AN4Z07BY → 8 characters 01AN4Z07\n\nUUIDv7\n\nFirst 8 characters (no hyphens) = high bits of timestamp\nRecommended uidshort: 8-12 characters (hex)\nExample: 017f22e2-79b0-7... → 8 characters 017f22e2\n\nRecommendation\nInitial analysis recommendation: ULID\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvantageNotesShorter26 vs 36 characters; cleaner filenamesMore readableNo hyphens; visually cleanerSufficient entropy80-bit randomness; extremely low collision riskMature ecosystemYears of use; stable library supportFilename-friendlyNo special characters; compatible across filesystems\nAdvantages of UUIDv7\n\nStandardization: IETF RFC 9562; strong long-term stability\nCompatibility: Works with existing UUID infrastructure (e.g., database UUID columns)\nFuture-proofing: Python 3.12+ and mainstream libraries add native support\n\n\nFinal decision (2026-01-06)\nAdopt UUIDv7\nChoose UUIDv7 for these reasons:\n\nIETF standardization provides stronger long-term stability\nCompatible with existing UUID ecosystems\nNative support in mainstream languages is arriving\n\nuidshort length: 8 characters (hex prefix)\n\nExample: 017f22e2-79b0-7... → 017f22e2\n\nReferences\n\nULID Spec\nRFC 9562 - UUIDv7\npython-ulid\n"},"adr/ADR-0003_identifier-strategy-for-local-first-backlog":{"title":"Identifier strategy: sortable IDs without centralized allocation","links":["adr/ADR-0003-appendix_ulid-vs-uuidv7-comparison"],"tags":[],"content":"Decision\nKeep the backlog file-first (Markdown files in repo as the source of truth) and avoid requiring a\r\ncentralized server for identifier allocation.\nAdopt a hybrid identifier strategy:\n\nuid is the immutable primary key (globally unique). Format: UUIDv7 (36 chars, hex, 48-bit timestamp + 74-bit random, RFC 9562).\nid is a human-readable display ID (sortable, short), and may collide across machines/branches.\nFilenames must include uidshort to avoid git add/add conflicts when id collides.\nAny reference resolution using id must go through a resolver that can disambiguate.\n\nContext\nA strictly increasing counter (ID from 0..N) is convenient for sorting and scanning, but in a\r\nlocal-first workflow it becomes hard to guarantee uniqueness across machines/agents without either:\n\na centralized allocator (service/server/lock), or\ncoordination rules that prevent concurrent creation.\n\nUsing only UUIDs avoids collisions but loses the natural sortable sequence in filenames and dashboards.\r\nWe want to preserve “human-first” readability while keeping future options open for distributed collaboration.\nRequirements\n\nMultiple agents/machines can create items concurrently without a shared allocator.\nReferences should remain stable across renames/renumbers and across derived indexes (SQLite, embeddings).\nAgents should be able to answer “next / what to do next” and support triage using indexes.\n\nOptions Considered\n\nCentralized ID allocator (server or shared lock file)\nUUID-only IDs (globally unique, not naturally ordered)\nTime-sortable unique IDs (ULID / UUIDv7)\nHybrid IDs: keep sortable id + add immutable uid\n\nPros / Cons\n\nOption 1: strongest uniqueness; introduces infrastructure and single-point-of-failure; violates local-first.\nOption 2: simplest uniqueness; hurts readability and natural ordering.\nOption 3: unique + sortable; still less human-friendly than short sequential IDs; ecosystem differences.\nOption 4: keeps human-friendly filenames while enabling reliable uniqueness for merging/indexing; adds complexity.\n\nConsequences\n\nThe default workflow remains file-first and Obsidian-friendly.\n\nWork item frontmatter (minimum)\nThis is the target schema for distributed-safe identifiers (migration required; not implemented everywhere yet):\n\nRequired:\n\nuid: string (ULID or UUIDv7; immutable; unique)\nid: string (display ID; sortable; allowed to collide)\ntype, title, status/state, priority, created, updated\n\n\nRecommended:\n\ntags\nparent_uid (references use uid to avoid ambiguity)\nlinks_uid (same)\naliases (optional; for legacy IDs or future renumbering)\n\n\n\nFilename and path\nTo avoid git add/add conflicts when two branches create the same display id, filenames should be unique.\nCurrent format (sufficient for most cases):\n\n&lt;id&gt;_&lt;slug&gt;.md\nExample: KABSD-TSK-0100_implement-backlog-indexing.md\n\nSince the slug is derived from the title and describes the item’s purpose, collision risk is minimal in practice. Two items would need identical id AND identical slug to conflict.\nOptional extended format (for high-concurrency scenarios):\n\n&lt;id&gt;__&lt;uidshort&gt;_&lt;slug&gt;.md\nExample: KABSD-TSK-0100__01KE72EH4N_implement-backlog-indexing.md\n\nuidshort is a stable prefix (fixed length) derived from uid:\n\nULID: prefix of the ULID (timestamp portion is convenient)\nUUIDv7: prefix of the hex string (length TBD, e.g. 8-12)\n\nDecision (2026-01-06): Filename format remains unchanged (&lt;id&gt;_&lt;slug&gt;.md). The uid field is added to frontmatter only; renaming existing files is not required.\nResolver semantics (reference handling)\nTools must implement ResolveRef(ref):\n\nIf ref is a full uid → unique match.\nIf ref is a uidshort → resolve via index (uidshort -&gt; uid). If multiple matches, list candidates.\nIf ref is a display id (e.g. KANO-000123) → resolve via index (id -&gt; [uid...]):\n\none match: return it\nmultiple matches: list candidates for human selection (type/status/title/path/created/updated).\n\n\n\nRecommended human-friendly reference format to reduce ambiguity:\n\nKANO-000123@01KE72EH4N (display id + uidshort)\n\nIndex implications\nDerived indexes must support:\n\nuid -&gt; path\nuidshort -&gt; uid\nid -&gt; [uid...] (note: potentially multiple)\n\nWhat-next behavior (agent workflow)\nWhen asked “next / what to do next”:\n\nPrefer continuing items in progress.\nOtherwise pick from ready items (e.g. 3-5), ordered by priority then recency/parent context.\nOutput should include id@uidshort, title, type, status/state, priority, and a first actionable step.\n\nOpen Questions / Follow-ups\n\nChoose ULID vs UUIDv7 (sorting, readability, library support, collision safety, short-prefix length).\n\nResolved (2026-01-06): Use UUIDv7. See Comparison Document.\nRationale: IETF standardized (RFC 9562), native Python 3.12+ support planned, compatible with existing UUID infrastructure.\nuidshort length: 8 characters (hex prefix) recommended.\n\n\nMigration plan for existing id-only items (add uid to frontmatter, filenames unchanged).\nShould we store both parent (id) and parent_uid during migration, or hard cutover to uid?\nAdd a collision report (group by display id) and a resolver UI/CLI for disambiguation.\n"},"adr/ADR-0004_file-first-architecture-with-sqlite-index":{"title":"File-First Architecture with SQLite Index","links":["adr/ADR-0012_workset-db-canonical-schema-reuse","_meta/canonical_schema.sql","_meta/canonical_schema.json"],"tags":[],"content":"Decision\nUse a File-First architecture where Markdown files are the single source of truth, augmented by a local SQLite database acting as a disposable, read-optimized index.\n\nSource of Truth: Markdown files tracked in Git.\nDerived Index: Local SQLite database (_kano/backlog/_index/backlog.sqlite3).\nSync Direction: STRICTLY One-Way (File → DB). The DB is never the System of Record.\nPersistence: The SQLite file is not tracked in Git (should be .gitignored). It can be rebuilt from files at any time.\n\nContext\nAs the backlog grows, scanning hundreds or thousands of Markdown files for every query (e.g., “find all active tasks blocking feature X”) becomes too slow (O(N) IO operations).\nHowever, we want to maintain the benefits of text files:\n\nGit-friendly: Diff, merge, blame work natively.\nHuman-readable: Accessible without special tools.\nPortable: No database server dependency for basic access.\n\nWe need a solution that provides relational query speed (O(log N)) without compromising the file-centric workflow.\nDetailed Design\n1. Index Lifecycle\nThe index is a cache of the file state.\n\nBuild: Scan all .md files, parse frontmatter, insert into DB.\nUpdate: Check file mtime against DB record. Only re-parse modified files.\nRebuild: rm backlog.sqlite3 followed by a full build ensures 100% consistency.\n\n2. Database Schema\nThe SQLite schema is designed for query efficiency, not normalization rules that would apply to a primary store.\nitems Table\nCore metadata for filtering and sorting.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumnTypeDescriptionuidTEXT (PK)UUIDv7 (from frontmatter)idTEXTDisplay ID (e.g., KABSD-TSK-0049)typeTEXTWork item type (Feature, Task, etc.)stateTEXTCurrent state (New, InProgress, etc.)titleTEXTItem titlepathTEXTRelative path to file (unique)mtimeREALFile modification timestamp (for sync logic)content_hashTEXTHash of content (for change detection)frontmatterJSONFull frontmatter blob (flexibility)createdTEXTCreation dateupdatedTEXTLast updated date\nlinks Table\nTracks relationships for graph queries (e.g., “all children of X”).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumnTypeDescriptionsource_uidTEXTLink source (child)target_uidTEXTLink target (parent)typeTEXTLink type (e.g., “parent”, “relates_to”)\nembeddings Table (Future/Optional)\nStores vector embeddings for semantic search.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumnTypeDescriptionuidTEXTFK to items.uidchunk_indexINTSequence number of chunkembeddingBLOBFloat32 vector arraycontentTEXTChunk text content\n3. Sync Logic\nA sync script (e.g., update_index.py) runs:\n\nBefore complex operations (e.g., generate_view).\nPeriodically (if running as a daemon/watcher).\nOn-demand by user.\n\nresolve_ref and other CLI tools currently use an in-memory index (lib/index.py). They should be refactored to query SQLite when available for better scaling.\nTrade-offs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrade-offDescriptionLatencyDB state may lag behind file state until sync runs. Tooling must handle “dirty” reads or force sync.ComplexityMaintaining sync logic (especially incremental updates) adds code complexity vs raw file scan.SpaceDuplicates metadata in DB file (negligible for text backlogs).\nConsequences\n\nTooling Update: All queries (Dashboard generation, Reference resolution) should migrate to use SQLite for reads.\nGitignore: Ensure *.sqlite3 is ignored.\nResilience: Tools must degrade gracefully if DB is corrupt or missing (fallback to file scan or auto-rebuild).\nSchema Reuse: Per ADR-0012, this canonical schema is reused by workset DBs to avoid schema drift and maintain portable context.\n\nRelated\n\nCanonical Schema: See canonical_schema.sql and canonical_schema.json for the complete schema definition.\nWorkset Schema: See ADR-0012 for how workset DBs reuse this canonical schema.\n"},"adr/ADR-0004_per-product-isolated-index-architecture":{"title":"Per-Product Isolated Index Architecture","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","items/task/0000/KABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation"],"tags":[],"content":"Context\nIn a multi-product monorepo environment, we needed to decide between two architectural approaches for the SQLite index:\n\nPlatform-level shared index: All products write to a single _kano/backlog/_index/backlog.sqlite3\nPer-product isolated index: Each product owns its own index at products/&lt;name&gt;/_index/backlog.sqlite3\n\nDecision Criteria\nWe evaluated 7 dimensions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimensionPer-ProductPlatformWinnerIsolationComplete separationShared namespacePer-ProductConcurrencyNo locking contentionPotential locksPer-ProductPerformancePredictable, isolatedCan degrade with loadPer-ProductScalabilityLinear per productShared bottleneckPer-ProductComplexitySimple, independentComplex coordinationPer-ProductMaintenanceEasy (per-product)Harder (shared state)Per-ProductFuture ExtensibilitySupports embedding DBDifficult to addPer-Product\nOverall Score: Per-Product = 91%, Platform = 43%\nDecision\nImplement per-product isolated SQLite indexes.\nEach product will:\n\nMaintain its own SQLite database at products/&lt;product-name&gt;/_index/backlog.sqlite3\nHave independent schema with product column for self-documentation\nSupport autonomous indexing/rebuilding without affecting other products\nEnable future cross-product aggregation via product-aware queries\n\nImplementation Details\nSchema Design\nAll tables include product column:\nCREATE TABLE items (\n  product TEXT NOT NULL,\n  id TEXT NOT NULL,\n  PRIMARY KEY (product, id),\n  ...\n);\n \nCREATE TABLE item_tags (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  tag TEXT NOT NULL,\n  PRIMARY KEY (product, item_id, tag),\n  FOREIGN KEY (product, item_id) REFERENCES items(product, id)\n);\nIndex Rebuild Process\n\nscripts/indexing/build_sqlite_index.py --product &lt;name&gt;\nScans products/&lt;name&gt;/items/ directory\nExtracts product from file paths\nUpserts into product-specific SQLite database\nMaintains composite key integrity\n\nResolver Behavior\n\nresolve_ref(ref, index, product=None) accepts optional product filter\nIf product not specified, searches across all products\nProduct column enables cross-product queries when needed\n\nRationale\n\nIsolation ensures safety: No possibility of data leakage between products\nPredictable performance: Each product’s index grows independently\nSupports autonomy: Teams can rebuild their product’s index without coordination\nFuture-proof: Enables embedding databases per product or shared aggregation\nSimpler mental model: Products are truly independent\nBackward compatible: Product column facilitates future migrations\n\nAlternatives Considered\nPlatform-level shared index\n\nPros: Unified search across all products\nCons: Complex coordination, shared bottleneck, potential data leakage\nRejected: Architecture does not support team autonomy\n\nHybrid approach (shared metadata + per-product data)\n\nPros: Balanced complexity\nCons: Still requires coordination layer, adds complexity\nRejected: Per-product isolation is simpler and cleaner\n\nFuture Work\nWhen cross-product features are needed:\n\nCreate aggregation layer on top of per-product indexes\nOr implement global embedding database that reads from per-product sources\nProduct column in schema already prepared for this extensibility\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration feature\nKABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation.md: Indexer and resolver product isolation\n"},"adr/ADR-0005_product-column-retention-rationale":{"title":"Product Column Retention in Per-Product Indexes","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","adr/ADR-0004_file-first-architecture-with-sqlite-index"],"tags":[],"content":"Context\nWhen implementing per-product isolated indexes, a question arose: if each product owns its own SQLite database, why include a product column in the schema?\nArguments for removing it:\n\nRedundant (the file path already indicates product)\nAdds storage overhead\nColumn values are always the same for a given database\n\nArguments for keeping it:\n\nSelf-documentation (data independence from file paths)\nConsistency with composite key patterns\nFuture extensibility (global embedding DB, cross-product queries)\nError detection capability\n\nDecision\nRetain the product column in all schema tables.\nEach product’s schema will include a product column despite the apparent redundancy.\nRationale\n1. Data Self-Documentation\nThe product column makes data self-describing. If a database dump or export is created, the product context is explicit in the data itself, not dependent on file path or metadata.\nSELECT * FROM items WHERE product=&#039;kano-agent-backlog-skill&#039;;\n-- vs.\nSELECT * FROM items;  -- How do you know which product this is from?\n2. Consistency with Composite Key Strategy\nUsing (product, id) as composite primary keys throughout the schema ensures:\n\nAll foreign keys are uniform: FOREIGN KEY (product, item_id)\nConsistent pattern across all tables (items, item_tags, item_links, worklog_entries)\nEasier code generation and migrations\n\n3. Uniform Schema Across Products\nAll products use identical schema structure. A tool that works with KABSD’s index works with any product’s index without modification. This is valuable for:\n\nShared tool development\nSchema migration scripts\nIndex validation and repair tools\n\n4. Future Extensibility: Global Embedding Database\nThe primary long-term value is supporting a future global embedding database:\n-- Global embedding store (future)\nCREATE TABLE embeddings (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  embedding BLOB NOT NULL,\n  PRIMARY KEY (product, item_id),\n  FOREIGN KEY (product, item_id) REFERENCES all_items(product, id)\n);\nThis would aggregate embeddings from all products while maintaining product context. The product column enables this without schema rework.\n5. Error Detection\nIncluding product column in per-product indexes enables validation queries:\n-- Verify database integrity\nSELECT DISTINCT product FROM items;\n-- Should always return single row: kano-agent-backlog-skill\n \n-- Detect misplaced files\nSELECT product FROM items WHERE product != &#039;kano-agent-backlog-skill&#039;;\n-- Should return empty set\n6. Cross-Product Queries (Future)\nWhen analytics or reporting tools are built, they may aggregate across products:\n-- Future: Query across products via union or aggregation\nSELECT product, COUNT(*) as item_count FROM all_items GROUP BY product;\nThe product column is essential for this without painful migrations.\nImplementation\nAll tables maintain the pattern:\nCREATE TABLE items (\n  product TEXT NOT NULL,\n  id TEXT NOT NULL,\n  uid UUID,\n  type TEXT,\n  -- ... other columns\n  PRIMARY KEY (product, id)\n);\n \nCREATE TABLE item_tags (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  tag TEXT NOT NULL,\n  PRIMARY KEY (product, item_id, tag),\n  FOREIGN KEY (product, item_id) REFERENCES items(product, id)\n);\nAlternatives Considered\nRemove product column entirely\n\nPros: Minimal storage, no apparent redundancy\nCons: Breaks future embedding DB design, harder to validate integrity, poor data self-documentation\nRejected: Future extensibility cost is too high\n\nMake product optional/nullable\n\nPros: Allows querying “which product” implicitly\nCons: Makes schema ambiguous, difficult to enforce consistency\nRejected: Product should always be explicit and required\n\nFuture Work\nWhen global embedding database is implemented:\n\nProduct column in schema already prepared\nNo schema migration required\nTools can immediately work with cross-product data\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration\nADR-0004_file-first-architecture-with-sqlite-index: Per-Product Isolated Index Architecture\n"},"adr/ADR-0005_skill-versioning-and-release-policy":{"title":"Skill Versioning and Release Policy","links":[],"tags":[],"content":"Decision\nAdopt a SemVer-inspired versioning policy for kano-agent-backlog-skill with a clear pre-1.0 roadmap:\n\nUse Git tags as the source of truth for released versions: vX.Y.Z.\nWhile &lt;1.0.0, treat releases as milestones and allow faster iteration, but still:\n\nuse Z for bugfixes and non-breaking changes,\nuse Y when we introduce intentional breaking changes (schema/CLI/layout).\n\n\nAfter 1.0.0, follow SemVer strictly:\n\nZ patch = backward-compatible bugfix only\nY minor = backward-compatible feature + optional deprecations\nX major = breaking changes (must provide migration guidance)\n\n\n\nContext\nThis repo is a demo host and development environment for an open-source skill. We need a predictable way to:\n\ncommunicate what changed,\ndecide when changes are breaking,\nalign backlog milestones with releases,\nkeep multi-agent usage stable across time.\n\nDefinitions (what counts as breaking)\nBreaking changes include (non-exhaustive):\n\nFrontmatter schema: renaming/removing required keys, changing meaning of state groups, changing defaults that alter workflow rules.\nPath/layout: moving the canonical backlog root (_kano/backlog/**), changing bucket rules, changing decisions/items separation.\nScript CLI: removing/renaming flags, changing required flags, changing default behavior that affects output determinism.\nConfig schema: renaming/removing keys under _kano/backlog/_config/config.json.\nGenerated output contracts: changing canonical dashboard filenames or section/group meaning.\n\nNon-breaking changes include:\n\nadding optional keys or sections,\nadding new scripts (without changing existing CLI),\nstrengthening validation with clearer error messages (unless it blocks previously valid projects).\n\nRelease artifacts (minimum)\nFor each release tag:\n\nUpdate the skill docs (README/REFERENCE) to match reality.\nEnsure canonical scripts work end-to-end:\n\nscripts/backlog/view_refresh_dashboards.py\nscripts/backlog/view_generate_demo.py (demo dashboards)\nscripts/backlog/workitem_update_state.py\n\n\nEnsure demo views are regenerated.\n\nOptional (recommended as we approach 0.1.0+):\n\nCHANGELOG.md in the skill repo (high-level, human readable).\nA short “upgrade notes” section when there is any migration required.\n\nMilestone mapping (demo backlog)\nWe track releases as milestone Epics:\n\nKABSD-EPIC-0002 = v0.0.1 (core demo)\nKABSD-EPIC-0003 = v0.0.2 (indexing + resolver)\n\nFuture guideline:\n\nPatch releases (v0.0.(Z+1)) do not require new Epics; they are small fixes folded into the current milestone Epic.\nMinor bump in pre-1.0 (v0.(Y+1).0) should have a dedicated milestone Epic if it introduces breaking changes.\n\nConsequences\n\nWe will treat “schema/CLI/layout” changes as versioned contracts.\nEach “milestone epic” must have acceptance criteria that match a release outcome (taggable state).\nWhen breaking changes are introduced, the release must include clear migration guidance (script or documented steps).\n"},"adr/ADR-0006_multi-product-directory-structure":{"title":"Multi-Product Directory Structure and Naming Conventions","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","items/task/0000/KABSD-TSK-0081_execute-directory-restructuring-for-monorepo-platform","adr/ADR-0016_per-product-isolated-index-architecture"],"tags":[],"content":"Context\nA monorepo containing multiple independent products (skills) needs a consistent directory layout that:\n\nKeeps products isolated from each other\nSupports independent configuration and indexing\nAllows shared tools and metadata at the project level\nScales to many products without directory explosion\nMaintains backward compatibility during migration\n\nCompeting Designs\n\n\nProject + Products model (chosen):\n_kano/backlog/\r\n  products/&lt;product-name&gt;/\r\n    _config/\r\n    items/\r\n    decisions/\r\n    views/\r\n  _shared/\r\n    defaults.json\n\n\n\nFlat product namespacing:\n_kano/backlog/\r\n  items/&lt;product&gt;-&lt;type&gt;/\r\n  decisions/&lt;product&gt;/\r\n  views/&lt;product&gt;/\n\n\n\nSingle root (pre-migration):\n_kano/backlog/\r\n  items/\r\n  decisions/\r\n  views/\n\n\n\nDecision\nImplement Project + Products hierarchical model.\nDirectory structure:\n_kano/backlog/                          # Project root\r\n├── products/                           # Product container\r\n│   ├── kano-agent-backlog-skill/       # First product\r\n│   │   ├── _config/\r\n│   │   │   └── config.json             # Product-specific config\r\n│   │   ├── items/                      # Product&#039;s backlog items\r\n│   │   │   ├── epics/0000/\r\n│   │   │   ├── features/0000/\r\n│   │   │   ├── userstories/0000/\r\n│   │   │   ├── tasks/0000/\r\n│   │   │   ├── tasks/0100/             # Buckets per 100 items\r\n│   │   │   └── bugs/0000/\r\n│   │   ├── decisions/                  # Product&#039;s ADRs\r\n│   │   ├── views/                      # Product&#039;s dashboards\r\n│   │   ├── _index/\r\n│   │   │   └── backlog.sqlite3         # Product-isolated index\r\n│   │   └── _meta/\r\n│   │       ├── schema.md\r\n│   │       ├── conventions.md\r\n│   │       └── indexes.md              # Epic index registry\r\n│   │\r\n│   └── kano-commit-convention-skill/   # Second product\r\n│       └── ... (same structure)\r\n│\r\n├── sandboxes/                          # Isolated test/demo environments\r\n│   ├── kano-agent-backlog-skill/       # Can test schema changes here\r\n│   └── kano-commit-convention-skill/\r\n│\r\n├── _shared/                            # Project-level shared data\r\n│   ├── defaults.json                   # { &quot;default_product&quot;: &quot;...&quot; }\r\n│   └── config_template.json            # Shared config seed\r\n│\r\n├── _meta/                              # Project metadata (if needed)\r\n├── _index/                             # Project index (optional, future)\r\n├── views/                              # Project-level dashboards\r\n│   ├── Dashboard_PlainMarkdown_Active.md\r\n│   ├── Dashboard_PlainMarkdown_New.md\r\n│   └── Dashboard_PlainMarkdown_Done.md\r\n└── _logs/                              # Audit logs (project-level)\r\n    └── agent_tools/\r\n        └── tool_invocations.jsonl\n\nRationale\n1. Isolation and Autonomy\nProducts live under products/&lt;name&gt;/:\n\nTeam A manages products/product-a/\nTeam B manages products/product-b/\nNo namespace collisions, no coordination needed\nClear ownership boundaries\n\n2. Scalability\nHierarchical structure scales linearly:\n\n2 products: 2 directories\n10 products: 10 directories\n100 products: 100 directories\nNo explosion of files at top level\n\n3. Unified Schema Across Products\nEach product has identical internal structure:\n\nAll products use items/, decisions/, views/, _config/, _meta/\nTools can be generic: “for each product, scan items/”\nReduces special-case logic in scripts\n\n4. Backward Compatibility\nExisting KABSD backlog migrates as:\n\nOld: _kano/backlog/items/task/0000/KABSD-TSK-0007.md\nNew: _kano/backlog/products/kano-agent-backlog-skill/items/task/0000/KABSD-TSK-0007.md\n\nPath change is clean; no file modifications required. Git correctly tracks as renames.\n5. Per-Product Isolation in Indexing\nEach product gets:\n\nOwn SQLite database: products/&lt;name&gt;/_index/backlog.sqlite3\nOwn metadata: products/&lt;name&gt;/_meta/indexes.md\nOwn config: products/&lt;name&gt;/_config/config.json\n\nRebuild product A’s index without touching product B.\n6. Flexible Sandboxing\nsandboxes/&lt;product-name&gt;/ allows safe testing:\n\nDevelop schema changes on test data\nRun migration scripts without affecting production backlog\nEasy cleanup: just delete sandbox directory\n\n7. Project-Level Aggregation (Future)\n_shared/ and _index/ support future features:\n\nGlobal embedding database\nCross-product analytics dashboards\nUnified search index (optional, opt-in)\n\nProducts remain independent; project layer is additive.\nImplementation\nPath Resolution\nAll scripts use context.py for product-aware resolution:\nfrom context import get_product_root, get_items_dir\n \nproduct_name = args.product or os.getenv(&quot;KANO_PRODUCT&quot;) or defaults[&quot;default_product&quot;]\nproduct_root = get_product_root(product_name)  # _kano/backlog/products/&lt;name&gt;\nitems_dir = get_items_dir(product_name)        # products/&lt;name&gt;/items\nConfiguration\n\nProject level: _kano/backlog/_shared/defaults.json (default product)\nProduct level: products/&lt;name&gt;/_config/config.json (product-specific)\n\nFallback chain:\n\nCLI --product flag\nKANO_PRODUCT environment variable\nProduct embedded in filename (e.g., task parsing)\ndefaults.json default_product\nHardcoded fallback: “kano-agent-backlog-skill”\n\nCLI Integration\nAll major scripts accept --product flag:\nscripts/backlog/workitem_create.py --product kano-agent-backlog-skill --type task --title &quot;...&quot;\nscripts/backlog/index_db.py --product kano-commit-convention-skill\nAlternatives Considered\nFlat namespacing\nitems/kabsd-tasks/0000/\r\nitems/kccs-features/0000/\n\n\nCons: No clear product boundaries, harder to extend to 100 products\nRejected: Doesn’t scale well\n\nSingle legacy structure\n\nCons: Cannot coexist multiple products, forces coordination\nRejected: Defeats purpose of monorepo autonomy\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration feature\nKABSD-TSK-0081_execute-directory-restructuring-for-monorepo-platform.md: Directory migration implementation\nADR-0016_per-product-isolated-index-architecture: Per-Product Index Architecture\n"},"adr/ADR-0007_vcs-as-source-of-truth-derived-commit-data":{"title":"VCS as Source of Truth: Derived Commit Data","links":["items/feature/0000/KABSD-FTR-0017_traceability-commit-refs-worklog-backfill","items/userstory/0000/KABSD-USR-0018_vcs-adapter-abstraction-layer","items/task/0100/KABSD-TSK-0110_evaluate-vcs-query-cache-layer"],"tags":["architecture","vcs","traceability","derived-data"],"content":"Status\nAccepted (2026-01-07)\nImplemented in Feature KABSD-FTR-0017 (Traceability: Commit Refs → Worklog Backfill).\nContext\nBacklog items need traceability to VCS commits to answer:\n\n“Which commits contributed to this item?”\n“What’s the latest activity timestamp for this item?”\n“Generate a commit timeline view filtered by item state”\n\nTwo competing approaches:\n\nWorklog Backfill: Parse VCS commits containing Refs: &lt;item-id&gt; and append them to item worklog\nDerived Data Query: Keep VCS as source of truth; backlog queries VCS on-demand for commit data\n\nDecision\nWe adopt the Derived Data Query approach:\n\nVCS commits remain the canonical source of commit history\nBacklog items do NOT store commit data in worklog (no backfill)\nQuery tools (query_commits.py, view_generate_commits.py) dynamically fetch commit data from VCS\nCommit messages use Refs: &lt;item-id&gt;, &lt;item-id&gt; pattern for traceable references\nMulti-VCS abstraction layer (scripts/vcs/) supports Git, Perforce, SVN\n\nArchitecture:\n┌─────────────┐\r\n│ VCS (Git)   │ ← Source of Truth (commit hash, author, date, message)\r\n└──────┬──────┘\r\n       │ query via adapter\r\n       ▼\r\n┌─────────────────────┐\r\n│ VCS Adapter Layer   │ (Git/Perforce/SVN)\r\n│ - base.py           │\r\n│ - git_adapter.py    │\r\n│ - perforce_adapter.py │\r\n│ - svn_adapter.py    │\r\n└──────┬──────────────┘\r\n       │ query by ID/UID\r\n       ▼\r\n┌─────────────────────┐\r\n│ Query Tools         │\r\n│ - query_commits.py  │ (item → commits list)\r\n│ - view_generate_commits.py │ (state → commit timeline)\r\n└─────────────────────┘\n\nRationale\nWhy Derived Data (NOT Worklog Backfill)?\nPros:\n\nNo Worklog Pollution: Worklog stays clean for human-authored entries (decisions, state changes, manual notes)\nVCS is Authoritative: No sync issues between VCS history and backlog; VCS is already immutable and auditable\nDeduplication: Single commit referencing multiple items doesn’t create N duplicate worklog entries\nTime-travel Queries: Can query commits by date range without modifying backlog files\nMulti-VCS Support: Abstraction layer allows querying Git, Perforce, SVN uniformly\n\nCons:\n\nQuery Cost: Every view generation requires VCS query (mitigated by future cache layer, see TSK-0110)\nVCS Dependency: Backlog alone doesn’t show commit history (requires VCS access)\nComplexity: Multi-VCS adapter abstraction adds code complexity\n\nWhy Multi-VCS Abstraction?\nReal-world projects may use multiple VCS systems (monorepos with Git, legacy Perforce depots, SVN archives). The adapter pattern provides:\n\nUniform interface: VCSAdapter.query_commits(ref_pattern, since, until, max_count)\nFuture-proof: Easy to add new VCS types without changing query tools\nTestable: Mock adapters for unit tests\n\nConsequences\nImmediate Impact (Feature 0017)\nImplemented:\n\n✅ scripts/vcs/base.py: VCSAdapter abstract class, Commit dataclass, detect_vcs()\n✅ scripts/vcs/git_adapter.py: Git implementation using git log --grep\n✅ scripts/vcs/perforce_adapter.py: Perforce using p4 changes -l\n✅ scripts/vcs/svn_adapter.py: SVN using svn log --xml\n✅ scripts/backlog/query_commits.py: Resolve item → query VCS → output text/JSON\n✅ scripts/backlog/view_generate_commits.py: Generate commit timeline views by state\n\nCommit Message Convention:\nfeat: implement VCS adapter abstraction\r\n\r\nRefs: KABSD-TSK-0105, KABSD-FTR-0017\n\nPattern: Refs: &lt;id&gt;[, &lt;id&gt;]* (case-insensitive, extracted via regex)\nFuture Work\nCache Layer (TSK-0110):\n\nEvaluation pending: SQLite cache vs. file-based cache\nCache invalidation: TTL-based + manual clear\nConfig: vcs.cache.enabled, vcs.cache.backend, vcs.cache.ttl\nNote: Cache is derived data; VCS remains source of truth\n\nIntegration:\n\nDashboard auto-refresh: View generators can be called from view_refresh_dashboards.py\nWorklog hints: Query tools can suggest worklog entries (human decides whether to add)\nADR references: Commits referencing ADRs can link to decision artifacts\n\nBreaking Changes\nNone. This is a new capability; existing backlog items are unaffected.\nAlternatives Considered\n1. Worklog Backfill (Original Design)\nApproach: Parse VCS commits and append to item worklog:\n2026-01-07 14:08 [agent=vcs-bot] Commit 2048e1c: Test commit for VCS adapter\nRejected because:\n\nWorklog pollution: Noisy with many commits\nSync burden: Requires periodic backfill script runs\nDeduplication issue: Multi-item commits create duplicate entries\nNot time-travel friendly: Can’t query “commits since yesterday” without re-parsing\n\n2. Commit Index Table (Persistent Storage)\nApproach: Store commits in SQLite commits(hash, author, date, message, item_refs) table.\nDeferred to TSK-0110 (cache evaluation):\n\nWould solve query performance\nRequires cache invalidation strategy\nSchema migration burden (needs TSK-0111 framework first)\n\n3. VCS-Native Tools Only\nApproach: Use git log --grep &quot;KABSD-&quot; directly; no backlog integration.\nRejected because:\n\nNot multi-VCS portable\nNo item-to-commits resolution (requires manual filtering)\nNo state-based filtering (can’t generate “InProgress items with commits” view)\n\nReferences\n\nFeature: KABSD-FTR-0017\nUserStory: KABSD-USR-0018\nTasks: KABSD-TSK-0105 (Git), KABSD-TSK-0106 (Perforce), KABSD-TSK-0107 (SVN), KABSD-TSK-0108 (query_commits.py), KABSD-TSK-0109 (view_generate_commits.py)\nFuture Work: KABSD-TSK-0110 (VCS Query Cache Evaluation)\nDepends On: None (standalone capability)\n\nAppendix: Example Usage\nQuery Commits for an Item\n# Text format\npython skills/kano-agent-backlog-skill/scripts/backlog/query_commits.py \\\n  --item KABSD-TSK-0105\n \n# JSON format\npython skills/kano-agent-backlog-skill/scripts/backlog/query_commits.py \\\n  --item KABSD-TSK-0105 --format json\nGenerate Commit Timeline View\n# All &quot;Done&quot; items with commits\npython skills/kano-agent-backlog-skill/scripts/backlog/view_generate_commits.py \\\n  --state Done --output _kano/backlog/views/commits_done.md\n \n# All &quot;InProgress&quot; items (useful for daily standup)\npython skills/kano-agent-backlog-skill/scripts/backlog/view_generate_commits.py \\\n  --state InProgress --output _kano/backlog/views/commits_active.md\nCommit Message Pattern\nfeat(vcs): add Perforce adapter with p4 changes parsing\r\n\r\nLong description of the change...\r\n\r\nRefs: KABSD-TSK-0106, KABSD-FTR-0017\n\n\nPattern is case-insensitive: refs:, Refs:, REFS: all work\nMultiple items: comma-separated Refs: ITEM-1, ITEM-2, ITEM-3\nDeduplication: Same commit appears once even if queried by multiple item IDs\n"},"adr/ADR-0008_sqlite-schema-migration-framework":{"title":"SQLite Schema Migration Framework","links":["items/task/0100/KABSD-TSK-0111_implement-sqlite-schema-migration-framework","adr/ADR-0004_file-first-architecture-with-sqlite-index","adr/ADR-0012_workset-db-canonical-schema-reuse","items/task/0100/KABSD-TSK-0110_evaluate-vcs-query-cache-layer"],"tags":["architecture","database","migration","schema-evolution"],"content":"Status\nAccepted (2026-01-07)\nImplemented in Task KABSD-TSK-0111 (Implement SQLite Schema Migration Framework).\nContext\nThe SQLite index schema (introduced in ADR-0004) needs to evolve:\n\nCurrent need: Add VCS cache tables (vcs_commits, vcs_cache_metadata) for TSK-0110\nFuture needs: Embeddings tables, worklog full-text search indexes, external system sync tables\n\nExisting mechanism (ad-hoc):\n# build_sqlite_index.py (before ADR-0008)\ntry:\n    cols = [row[1] for row in conn.execute(&quot;PRAGMA table_info(item_links)&quot;).fetchall()]\n    if &quot;target_uid&quot; not in cols:\n        conn.execute(&quot;ALTER TABLE item_links ADD COLUMN target_uid TEXT&quot;)\nexcept sqlite3.OperationalError:\n    pass\nProblems:\n\nNo version tracking (schema_version is written but never read)\nHard-coded migrations in apply_schema() (not scalable)\nNo migration ordering or idempotency guarantees\nFragile try-except wrapping breaks on constraint violations\n\nRisk: Adding VCS cache tables without a migration framework could break existing DBs or create inconsistent schemas across environments.\nDecision\nWe adopt a Flyway-style migration framework:\n\nNumbered SQL migration files in references/migrations/\nVersion detection via schema_meta.schema_version (integer)\nAuto-upgrade on build_sqlite_index.py --mode rebuild\nMigration runner applies pending migrations sequentially\nBase schema (indexing_schema.sql) initializes version to 0\n\nArchitecture:\nreferences/\r\n  indexing_schema.sql          ← Base schema (creates tables, version=0)\r\n  migrations/\r\n    001_add_vcs_cache_tables.sql      ← Version 1\r\n    002_add_embeddings_fts.sql        ← Version 2\r\n    003_add_external_sync.sql         ← Version 3 (future)\n\nMigration Runner Logic:\ndef get_current_version(conn) -&gt; int:\n    &quot;&quot;&quot;Return schema version (0 if fresh DB).&quot;&quot;&quot;\n    try:\n        row = conn.execute(\n            &quot;SELECT value FROM schema_meta WHERE key=&#039;schema_version&#039;&quot;\n        ).fetchone()\n        return int(row[0]) if row else 0\n    except sqlite3.OperationalError:\n        return 0  # schema_meta doesn&#039;t exist yet\n \ndef apply_migrations(conn):\n    &quot;&quot;&quot;Apply pending migrations in order.&quot;&quot;&quot;\n    current_version = get_current_version(conn)\n    migration_dir = Path(&quot;references/migrations&quot;)\n    migrations = sorted(migration_dir.glob(&quot;*.sql&quot;))\n    \n    for migration_file in migrations:\n        version = int(migration_file.stem.split(&quot;_&quot;)[0])  # &quot;001_*.sql&quot; → 1\n        if version &gt; current_version:\n            print(f&quot;Applying migration {version}: {migration_file.name}&quot;)\n            conn.executescript(migration_file.read_text())\n            conn.execute(\n                &quot;INSERT OR REPLACE INTO schema_meta(key, value) VALUES(?, ?)&quot;,\n                (&quot;schema_version&quot;, str(version))\n            )\n            conn.commit()\n \ndef apply_schema(conn):\n    &quot;&quot;&quot;Apply base schema + migrations.&quot;&quot;&quot;\n    conn.executescript(load_schema_sql())  # Creates schema_meta with version=0\n    apply_migrations(conn)                 # Upgrade to latest\nRationale\nWhy Flyway-Style Migrations?\nPros:\n\nExplicit Versioning: Each migration increments version; easy to track schema state\nIdempotent: Migrations run exactly once (version check prevents re-runs)\nOrdered Execution: Sorted filenames guarantee deterministic application order\nGit-Friendly: Migration files are plain SQL, diff-able and reviewable\nSimple Mental Model: Familiar to developers (like Alembic, Liquibase, Django migrations)\n\nCons:\n\nNo Rollback: Down migrations not supported (users must delete DB and rebuild)\nManual Numbering: Developers must coordinate version numbers (low risk in local-first design)\n\nWhy Not Alembic/SQLAlchemy?\n\nOverkill: Requires ORM layer; we use raw SQL for simplicity\nPython-based migrations: SQL migrations are easier to audit and portable\nDependency bloat: Alembic + SQLAlchemy adds significant dependencies\n\nWhy Not “Delete and Rebuild”?\nCurrent approach is rm backlog.sqlite3 &amp;&amp; rebuild. Why not keep this?\nFor small backlogs (&lt;100 items): Delete-and-rebuild is fine (fast, simple).\nFor large backlogs (&gt;1000 items): Rebuild takes &gt;10 seconds. Migrations enable:\n\nIncremental mode: Only re-index changed files (faster)\nPreserve derived data: Cache tables (VCS commits, embeddings) don’t need full rebuild\nProduction stability: Breaking schema changes are painful if rebuild is the only option\n\nDecision: Support both. Migrations for gradual evolution; delete-rebuild as nuclear option.\nConsequences\nImmediate Impact (Task 0111)\nImplemented:\n\n✅ get_current_version(conn): Read schema_meta.schema_version\n✅ apply_migrations(conn): Apply pending migrations from references/migrations/\n✅ apply_schema(conn) refactored: Base schema → migrations\n✅ Base schema (indexing_schema.sql) initializes schema_version = &#039;0&#039;\n✅ Tested: Fresh DB (v0), migration upgrade (v0→v1), idempotent re-runs\n\nMigration Directory:\nreferences/migrations/\r\n  (empty - ready for 001_add_vcs_cache_tables.sql when TSK-0110 completes)\n\nFuture Work\nVersion Compatibility Check:\ndef check_schema_compatibility(conn):\n    &quot;&quot;&quot;Warn if DB schema is newer than skill version.&quot;&quot;&quot;\n    db_version = get_current_version(conn)\n    skill_max_version = 2  # Hard-coded or read from VERSION file\n    if db_version &gt; skill_max_version:\n        print(f&quot;Warning: DB schema v{db_version} newer than skill v{skill_max_version}.&quot;)\nMigration Naming Convention:\n{version:03d}_{description}.sql\r\n001_add_vcs_cache_tables.sql\r\n002_add_embeddings_fts.sql\r\n003_add_worklog_search_index.sql\n\nTransaction Safety:\r\nAll migrations wrapped in BEGIN TRANSACTION / COMMIT (SQLite default for executescript()). If migration fails, rollback prevents partial application.\nBreaking Changes\nNone. Existing DBs without migrations are version 0; migrations upgrade them gracefully.\nBackward Compatibility:\n\nOld skill (no migration runner) + new DB (v1+): Read queries work; writes may fail on new constraints\nNew skill (with migration runner) + old DB (v0): Auto-upgrades on rebuild\n\nAlternatives Considered\n1. Hard-Coded Migrations in Code\nCurrent approach (before ADR-0008):\nif &quot;target_uid&quot; not in cols:\n    conn.execute(&quot;ALTER TABLE item_links ADD COLUMN target_uid TEXT&quot;)\nRejected because:\n\nNot scalable (code bloat)\nNo version tracking (can’t detect schema state)\nError-prone (forgotten migrations leave inconsistent DBs)\n\n2. Alembic (Python-Based Migrations)\nApproach: Use Alembic for ORM-style migrations.\nRejected because:\n\nRequires SQLAlchemy ORM (we use raw SQL)\nPython-based migrations harder to audit (prefer declarative SQL)\nOverkill for local-first use case\n\n3. Schema Versioning Without Migrations\nApproach: Store version but require manual schema updates.\nRejected because:\n\nShifts migration burden to users (error-prone)\nNo automated upgrade path\n\n4. Delete-and-Rebuild Only\nApproach: Always rm backlog.sqlite3 &amp;&amp; rebuild on schema change.\nPartially Retained: Still supported as nuclear option.\nWhy Not Sufficient:\n\nLarge backlogs: Rebuild too slow (&gt;10s for 1000+ items)\nCache tables: VCS commits, embeddings would be lost (no incremental updates)\n\nReferences\n\nTask: KABSD-TSK-0111\nRelated ADR: ADR-0004 (SQLite Index Architecture)\nRelated ADR: ADR-0012 (Workset DB Schema - migrations apply to worksets too)\nFuture Work: KABSD-TSK-0110 (VCS Cache - first migration user)\nDependency: None (standalone framework)\n\nWorkset DB Migration\nPer ADR-0012, workset DBs MUST reuse the canonical schema and apply the same migrations.\nWorkset Migration Strategy:\n\nWhen building a workset, detect canonical index schema version\nApply same migrations to workset DB in order\nStore canonical_index_version in workset_manifest table\nIf canonical schema is upgraded, worksets MUST be rebuilt or auto-migrated\n\nConstraint: Workset schema_version MUST NOT exceed canonical schema_version.\nAppendix: Migration File Template\n-- Migration 001: Add VCS cache tables (2026-01-07)\n-- Context: Support derived VCS commit data caching (TSK-0110)\n \nCREATE TABLE vcs_commits (\n  item_uid TEXT NOT NULL,\n  commit_hash TEXT NOT NULL,\n  author TEXT NOT NULL,\n  date TEXT NOT NULL,\n  message TEXT NOT NULL,\n  cached_at TEXT NOT NULL,\n  PRIMARY KEY(item_uid, commit_hash)\n);\n \nCREATE INDEX idx_vcs_commits_cached_at ON vcs_commits(cached_at);\n \nCREATE TABLE vcs_cache_metadata (\n  item_uid TEXT PRIMARY KEY,\n  last_query_at TEXT NOT NULL,\n  vcs_type TEXT NOT NULL  -- &#039;git&#039;, &#039;perforce&#039;, &#039;svn&#039;\n);\nNaming: {version:03d}_{description}.sql\nTesting:\n# Fresh DB (should apply migration)\nrm -f _kano/backlog/_index/backlog.sqlite3\npython skills/kano-agent-backlog-skill/scripts/indexing/build_sqlite_index.py \\\n  --agent copilot --mode rebuild\n \n# Re-run (should skip migration, idempotent)\npython skills/kano-agent-backlog-skill/scripts/indexing/build_sqlite_index.py \\\n  --agent copilot --mode rebuild"},"adr/ADR-0009_local-first-embedding-search-architecture":{"title":"Local-First Embedding Search Strategic Evaluation","links":["items/userstory/0000/KABSD-USR-0015_generate-embeddings-for-backlog-items-derivative-index","adr/ADR-0004_file-first-architecture-with-sqlite-index","adr/ADR-0011_graph-assisted-retrieval-and-context-graph","items/feature/0000/KABSD-FTR-0023_graph-assisted-rag-planning-and-minimal-implementation"],"tags":[],"content":"Local-First Embedding Search Strategic Evaluation\nContext and Problem Statement\nAs the backlog grows with hundreds of items, agents need a way to perform semantic search to find relevant context (e.g., “Why did we decide to use ULID?” or “Find similar tasks for refactoring the indexer”).\nOur architecture is “Local-First”:\n\nCanonical Store: Markdown files.\nDerived Store: SQLite index (rebuildable).\n\nWe need an embedding search solution that:\n\nIntegrates well with the existing local-first workflow.\nMinimizes complex binary dependencies for cross-platform support.\nFollows the “derived data” philosophy (indices can be thrown away and rebuilt).\n\nDecision Drivers\n\nDeployment Simplicity: Zero-install or easy-install on developer machines.\nConsistency: The vector index must stay in sync with the Markdown/SQLite data.\nPhilosophical Alignment: Keep the source of truth in files; everything else is a performance optimization.\n\nConsidered Options\nRoute A: SQLite + Vector Extension (e.g., sqlite-vec)\nUse a SQLite extension to handle vector storage and ANN (Approximate Nearest Neighbor) search directly in the database.\n\nGood, because: Single database item; relational + vector joins in one query.\nBad, because: Loading binary extensions in Python (sqlite3.load_extension) is notoriously finicky across platforms (Windows vs Linux vs macOS). Packaging these binaries into the skill makes it “heavy”.\n\nRoute B: SQLite (Metadata) + Sidecar ANN Index (e.g., FAISS / HNSWlib)\nKeep the metadata (ID, title, state) in the existing SQLite index. Store the high-dimensional vectors in a separate, dedicated index file (sidecar).\n\nGood, because:\n\nHighly decoupled: We can swap FAISS for HNSWlib or even a plain NumPy file without touching the SQLite schema.\nFits the “Kano Philosophy”: The vector index is just another derived artifact.\nPerformance: Sidecar indices like HNSWlib are extremely fast for mmap-based local search.\n\n\nBad, because: Requires a “two-step” lookup (Search Sidecar → Map IDs → Fetch SQLite) and a dual-sync process during ingestion.\n\nRoute C: Postgres + pgvector (Shared Derived Store)\nMove the derived index to a remote Postgres instance with the pgvector extension.\n\nGood, because: Perfect for multi-agent/multi-remote collaboration where a shared “claim” or “lock” system is needed anyway.\nBad, because: Requires a server. Not “local-first” in the spirit of the project. High latency for simple local tasks.\n\nDecision Outcome\nChosen option: Route B (Sidecar ANN Index) for local-first environments, with an optional path to Route C for shared/remote usage.\nImplementation Strategy\n\n\nUnified Ingestion:\n\nDocTypes: Cover WorkItem, ADR, Worklog, Workset (local cache), and Skill Docs.\nMetadata Store: SQLite documents table tracks uid, doctype, product, path, and content_hash.\nChunking: Document-aware chunking (e.g., ADR sections, Worklog per-day). Stores in chunks table with parent_doc and section metadata.\nFTS5: Index chunk text in SQLite FTS5 for sub-millisecond keyword search and BM25 ranking.\n\n\n\nEmbedding &amp; Vector Sidecar:\n\nSidecar: HNSWlib or FAISS index file (index_&lt;product&gt;.bin).\nIncremental Sync: Only compute embeddings for chunks where text_hash has changed.\nMapping: SQLite stores chunk_id -&gt; vector_id to bridge the sidecar back to metadata.\n\n\n\nHybrid Search &amp; Ranking:\n\nQuery Path:\n\nStructural: SQLite B-Tree (product/type/status).\nKeyword: SQLite FTS5 (BM25).\nSemantic: Sidecar ANN (Cosine similarity).\n\n\nFusion: Combine scores using weighted logic:\n\nw_exact: High weight for ID matches.\nw_type: Priority for ADR Decisions and WorkItem Titles.\nw_recency: Decay score for older content.\nw_visibility: Distinguish between canonical and local_cache.\n\n\n\n\n\nPros and Cons of the Consequences\nGood\n\nPortable: The sidecar can be shared or ignored by git easily.\nFast: Local search is sub-millisecond.\nRobust: If the sidecar breaks, we just delete it and rerun the indexing script.\nComprehensive: Covers “everything” in the repo while preserving visibility boundaries (Local Workset vs Canonical ADRs).\n\nBad\n\nSync Logic: Need to handle incremental updates (delete old vectors if file is deleted/moved).\nTooling: Requires an ANN library in the dependencies (e.g., hnswlib or sentence-transformers/faiss-cpu).\n\nReferences\n\nKABSD-USR-0015: Generate embeddings for backlog items\nADR-0004: File-first architecture with SQLite index\n\nGraph-assisted retrieval (Context Graph)\nIn addition to keyword/semantic retrieval, we can improve precision and traceability by expanding the seed set\r\nvia a derived Context Graph (parent chain, ADR refs, dependency links).\nMinimal strategy:\n\nRetrieve seeds via FTS/ANN\nExpand k-hop over allowlisted edges\nRe-rank and pack context (seed + neighbors)\n\nSee:\n\nADR-0011 Graph-assisted retrieval with a derived Context Graph\nKABSD-FTR-0023 Graph-assisted RAG planning\n"},"adr/ADR-0010_project-babylon-global-scale-collaboration-vision":{"title":"Kano-Babylon Project","links":[],"tags":["vision","scaling","agents","babylon"],"content":"Kano-Babylon Project\nThe project has established a solid “Local-First” foundation with the Kano Commit Convention (KCC) and the Backlog Skill. However, the ultimate goal transcends a single repo or a single user. We envision “Project Babylon”—a vast, distributed project execution system where thousands of humans and agents collaborate on a scale previously impossible.\nVision: The Babylon Tower\nThe metaphor of the Tower of Babel represents a project so grand it reaches the heavens. Unlike the biblical story, our “Babylon” uses technology to ensure that a multitude of voices and languages (human and machine) can work in perfect harmony.\nThe Workforce (Agents)\nAgents are the “masons” of the tower. They:\n\nExecute high-velocity code, doc, and test changes.\nMaintain the backlog discipline autonomously.\nCommunicate via structured data (JSON/MD) and auditable worklogs.\nOperate locally, minimizing latency and avoiding central bottlenecks.\n\nThe Overseers (Humans)\nHumans are the “architects” and “supervisors”. They:\n\nProvide high-level context and intent.\nReview critical ADRs and release candidates.\nResolve high-level priority conflicts.\nDefine the “Temporary Clauses” and guardrails for the agent workforce.\n\nArchitectural Pillars for Babylon\n\nVCS-Agostic Distribution: Git/Perforce/Subversion act as the transport layer. The “state” of the project is a forest of local-first backlogs.\nEventually Consistent Coordination: Moving away from central “locking” towards a “claim/lease” protocol where agents can claim segments of work and sync changes asynchronously.\nCanonical File-First Truth: The “Source of Truth” remains readable files (.md, .json). DBs (SQLite/Vector) are only ever derived caches for performance.\nAgent Semantic Indexing: Global search across thousands of products using decentralized vector embeddings and cross-repo referencing.\nUniversal Linter/Compliance: Every “brick” added to the tower must pass the KCC and Backlog Quality gates (STCC), ensuring the tower never crumbles from internal inconsistency.\n\nOvercoming the “Babylon Curse” (Counter-measures)\nThe historical Babylon fell because of linguistic fragmentation and loss of common purpose. Our architecture is designed to proactively avoid this “curse”:\n\nSTCC as a Universal Language: By strictly enforcing the Standardized Technical Communication Convention (STCC), we ensure that an agent in one part of the project produces output that is perfectly understood by an agent (or human) in another, regardless of their internal processing “dialect”.\nLocal-First Resilience: If central coordination (cloud/server) fails, the “builders” (local nodes) don’t stop. They continue working based on local truth and re-sync whenever possible, preventing total project paralysis.\nAuditable Reconstruction: The append-only Worklog and immutable Git history act as a permanent record. If coordination is temporarily lost, the project can be “re-aligned” by traversing the decision trail.\nThe Human Context Anchor: Humans serve as the source of “Grand Intent”, preventing the workforce from diverging into irrelevant or conflicting optimizations.\n\nRationale\nBy documenting this now, we ensure that every local-first decision we make (ID strategy, path resolution, i18n) is a “pre-fit” for a global-scale architecture. We are building the scaffold to support the weight of the heavens.\nStatus\nProposed/Visionary. This ADR serves as the north star for all future development. It justifies the strictness of our current local-first hardening while preparing the logic for the “Great Sync”.\n2026-01-08 18:55 [agent=antigravity] Created based on user’s vision of reaching the divine through collaborative scale."},"adr/ADR-0011_graph-assisted-retrieval-and-context-graph":{"title":"Graph-assisted retrieval with a derived Context Graph (weak graph first)","links":["adr/ADR-0004_file-first-architecture-with-sqlite-index","adr/ADR-0009_local-first-embedding-search-architecture","items/feature/0000/KABSD-FTR-0007_optional-db-index-and-embedding-rag-pipeline","items/feature/0000/KABSD-FTR-0023_graph-assisted-rag-planning-and-minimal-implementation"],"tags":[],"content":"Decision\nAdopt Graph-assisted retrieval as a minimal, local-first improvement to context quality:\n\nUse FTS/embeddings to retrieve seed nodes\nUse a derived Context Graph to expand to load-bearing neighbors (k-hop traversal)\nKeep everything derived/rebuildable from canonical Markdown (file-first)\n\nThis ADR explicitly chooses weak graph first: only structured relationships (no LLM entity extraction).\nContext\nWe already have a file-first backlog with optional derived indexes (SQLite / FTS / embeddings).\r\nVector-only retrieval often returns text-similar chunks but misses the structural context (parents, ADR decisions, dependency chains).\nWe want a deterministic, auditable way to expand context that is:\n\nlocal-first\nderived/rebuildable\nincrementally maintainable\nsafe (bounded expansion to avoid prompt bloat)\n\nDefinitions\n\nContext Graph: a derived, typed graph of artifact relationships (items, ADRs, dependencies, etc.).\nSeed set: top-N nodes from FTS/embedding retrieval.\nGraph expansion: k-hop traversal from seeds over allowlisted edges with limits.\n\nGraph model (v1)\nNodes\nMinimum node types:\n\nwork_item (Epic/Feature/UserStory/Task/Bug)\nadr\n\nOptional (for embedding/fts pipelines):\n\nchunk (document chunk tied to a parent doc)\n\nEdges\nMinimum edge types:\n\nparent (child → parent)\ndecision_ref (work_item → adr)\nrelates (work_item → work_item)\nblocks / blocked_by\n\nStorage (derived)\nThe Context Graph is derived data. Implementations may:\n\n\nMaterialize into SQLite\n\nreuse items as the node registry\nstore edges in a links-style table (source_uid, target_uid, type, optional weight, source_path)\n\n\n\nSidecar graph artifacts\n\n&lt;backlog-root&gt;/_index/graph_nodes.jsonl\n&lt;backlog-root&gt;/_index/graph_edges.jsonl\n\n\n\nBoth must be safe to delete and rebuild.\nRetrieval strategy (Graph-assisted RAG)\n\nSeed retrieval\n\nFTS and/or embeddings return top-N seed nodes/chunks\n\n\nExpand\n\ntraverse k-hop (default k=1)\nedge allowlist and fanout caps\n\n\nRe-rank\n\nweights by doctype and section (ADR decision &gt; item title/acceptance &gt; worklog)\noptionally prioritize Ready/InProgress items\n\n\nContext packing\n\nemit a context pack describing:\n\nseeds (why selected)\nneighbors (which edge pulled them in)\nminimal excerpts/anchors (title/ids/links)\n\n\n\n\n\nConfig surface (indicative)\n\nretrieval.graph.enabled\nretrieval.graph.k_hop\nretrieval.graph.edge_allowlist\nretrieval.graph.max_neighbors_per_seed\nretrieval.weights.* (doctype/section/state weights)\n\nConsequences\n\nGraph-assisted retrieval becomes the preferred way to preserve traceability (seed + neighbors).\nTooling must keep expansion bounded to avoid context explosions.\nThe design stays compatible with file-scan fallback and optional SQLite/embedding acceleration.\n\nNon-goals\n\nLLM/NLP entity extraction and automatic relation mining\nserver/MCP mode or cross-repo graphs\n\nReferences\n\nADR-0004 File-first + SQLite index\nADR-0009 Local-first embedding search\nRAG pipeline\nKABSD-FTR-0023 Graph-assisted RAG planning\n"},"adr/ADR-0011_workset-graphrag-context-graph-separation-of-responsibilities":{"title":"Workset vs GraphRAG / Context Graph — Separation of Responsibilities","links":["items/feature/0000/KABSD-FTR-0013_add-derived-index-cache-layer-and-peragent-workset-cache-ttl","items/feature/0000/KABSD-FTR-0015_execution-layer-workset-cache-promote","adr/ADR-0004_file-first-architecture-with-sqlite-index","adr/ADR-0009_local-first-embedding-search-architecture","artifacts/workset_evaluation_report","items/task/0200/KABSD-TSK-0217_clarify-spec-workset-vs-graphrag-context-graph-responsibilities-no-conflict"],"tags":[],"content":"Decision\nWe adopt a clear separation of responsibilities between:\n\nWorkset: Per-agent/per-task materialized cache bundle (local, ephemeral, task-scoped)\nGraphRAG / Metadata Graph: Repo-level derived navigation/retrieval structure (nodes + edges, shared, rebuildable)\nContext Graph: Either knowledge graph (same as GraphRAG) or agent workflow/planning graph (different layer, no conflict)\n\nCore Principle: Workset and Graph are BOTH derived data. Neither is the source of truth. The canonical backlog/ADR files remain the single system of record.\nContext and Problem Statement\nAs Kano evolves to support:\n\nMulti-agent collaboration with context management (KABSD-FTR-0013, KABSD-FTR-0015)\nGraph-based retrieval and semantic search (ADR-0009)\nWorkset-based execution memory (workset_evaluation_report.md)\n\nWe face a critical architectural risk: role confusion between components.\nThe Risk: Role Confusion\nWithout clear boundaries, future implementations might:\n\nTreat per-agent worksets as the “truth” instead of rebuildable cache\nStore the authoritative graph structure ONLY inside worksets (leading to divergence across agents)\nMix retrieval logic (graph expansion) with cache storage (workset)\nCreate worksets that cannot be rebuilt from canonical data\n\nThis ADR prevents these failure modes by establishing hard constraints and data flow patterns.\nDefinitions\n1. Workset (Working Set)\nWhat it is:\n\nA materialized bundle (typically a SQLite file + optional filesystem cache) containing a selected subset of items, chunks, and summaries.\nUsually per-agent or per-task, scoped to a specific time window or work session.\nStored in _kano/backlog/.cache/worksets/&lt;item-id&gt;/ (one directory per backlog item; agent recorded in manifest) and NOT tracked in Git.\n\nPurpose:\n\nMaximize context relevance and reduce repeated retrieval cost during task execution.\nProvide stable, fast access to “current working context” without re-querying repo-level indices.\nSupport execution-layer memory patterns (plan.md, notes.md, deliverable.md) as described in workset_evaluation_report.md.\n\nKey Properties:\n\nDerived: Built from canonical files + repo-level derived index\nRebuildable: Can be deleted and reconstructed at any time\nEphemeral: May have TTL (time-to-live) and automatic cleanup\nLocal: Not the source of truth; promotes back to canonical on important updates\n\nWhat it is NOT:\n\nNOT the system of record (canonical files are)\nNOT the authoritative graph store (repo-level graph index is)\nNOT shared across agents (each agent/task has its own)\nNOT version-controlled in Git\n\n2. GraphRAG / Metadata Graph\nWhat it is:\n\nA derived navigation/index structure with nodes and edges:\n\nNodes: workitems, ADRs (optionally commits, worklog entries, skill docs later)\nEdges: parent_of, references, depends_on, blocked_by, relates_to\n\n\nUsed for retrieval expansion and context assembly.\nStored at repo level (e.g., in SQLite links table, or separate graph DB file).\n\nPurpose:\n\nEnable graph-based queries: “Find all tasks blocking feature X”\nSupport k-hop expansion: “Given seed items, expand to related context”\nProvide structured navigation for RAG (Retrieval-Augmented Generation)\n\nKey Properties:\n\nShared: One graph per product/repo (not per-agent)\nDerived: Built from canonical file frontmatter (parent, links.relates, etc.)\nRebuildable: Can be rebuilt from files + frontmatter\nQueryable: Supports graph queries, traversal, expansion\n\nWhat it is NOT:\n\nNOT stored only inside worksets (worksets may include subgraph slices, but the authoritative graph is repo-level)\nNOT “strong KG” with LLM-extracted entities (that’s a future enhancement, not the base metadata graph)\nNOT the source of truth (canonical files are)\n\n3. Context Graph (Dual Meaning)\nThe term “Context Graph” can mean two different things, both valid and non-conflicting:\n3a. Context Graph = Knowledge Graph (Same as GraphRAG)\nIn RAG/retrieval contexts, “context graph” often means the knowledge graph used for retrieval.\n\nSame as: GraphRAG / Metadata Graph (defined above)\nPurpose: Navigate and expand context for LLM queries\n\n3b. Context Graph = Agent Workflow / Planning Graph\nIn agent orchestration contexts, “context graph” can mean the DAG (Directed Acyclic Graph) of agent tasks/steps.\n\nDifferent layer: This is about agent execution flow, not backlog item relationships\nPurpose: Plan and coordinate multi-step agent workflows\nNo conflict with Workset or GraphRAG: This is a workflow orchestration concept, not a data indexing concept\n\nClarification: Both meanings are valid. They address different layers and do not conflict with the Workset/GraphRAG separation.\nHard Constraints (Enforceable in Future Tickets)\n\n\nSource of Truth = Canonical Backlog/ADR Files\n\nAll writes MUST go to Markdown files in _kano/backlog/products/&lt;product&gt;/items/ or decisions/\nNeither Workset nor Graph can become the primary write target\n\n\n\nGraph and Workset are Derived and Must be Rebuildable\n\nBoth can be deleted and reconstructed from canonical files\nNo essential data lives ONLY in cache or index\n\n\n\nWorkset Must Not Become the Only Place Where Graph Truth Lives\n\nRepo-level graph index (shared derived) is the primary graph\nWorkset may include only a subgraph slice or expansion results\nWorkset does NOT store the authoritative full graph\n\n\n\nRetrieval Strategy (Workset-First with Fallback)\n\nQuery workset first (fast, stable context)\nFallback to repo-level derived index (vector/FTS/graph) when insufficient\nOptionally “incrementally enrich” the workset after fallback\nNever skip repo-level index and rely solely on workset\n\n\n\nNon-Goals\nThis specification explicitly does NOT include:\n\nServer/MCP implementation: This is a local-first spec (per AGENTS.md temporary clause)\nStrong graph / LLM-based KG: Entity extraction, relationship mining via LLM (future enhancement)\nWorkset as global indexing authority: Worksets are local/ephemeral, not authoritative\nReal-time sync between worksets: Each agent/task workset is independent\nGraph database engine choice: This spec is agnostic to implementation (SQLite, Neo4j, plain files)\n\nData Flow Architecture\n1. Build/Maintain Repo-Level Derived Index\nCanonical Files (Markdown + frontmatter)\r\n    ↓\r\n  Parse &amp; Extract\r\n    ↓\r\nRepo-Level Derived Index (SQLite + sidecar ANN)\r\n├── items table (metadata)\r\n├── links table (graph edges)  ← PRIMARY GRAPH\r\n├── chunks table (text chunks)\r\n├── FTS5 index (keyword search)\r\n└── Sidecar ANN (vector embeddings) ← per ADR-0009\n\nGraph Tables (in SQLite or separate graph DB):\n\nlinks(source_uid, target_uid, type) stores all edges\nRebuilt from frontmatter: parent, links.relates, links.blocks, links.blocked_by\n\n2. Build Workset Using Profile Recipe\nWorkset Build Process:\r\n1. Select seeds (e.g., active/in-progress/claimed/recent items)\r\n2. Expand via graph k-hop closure:\r\n   - Follow parent chain upward\r\n   - Follow references (links.relates)\r\n   - Follow dependencies (links.depends_on, links.blocks)\r\n3. Materialize into SQLite workset:\r\n   - Copy relevant items/chunks from repo index\r\n   - Include subgraph slice (only edges relevant to this workset)\r\n   - Add workset manifest (seeds, expansion params, timestamp)\n\nWorkset Structure (SQLite file):\n-- Workset metadata\nCREATE TABLE workset_manifest (\n  workset_id TEXT PRIMARY KEY,\n  agent TEXT,\n  task_id TEXT,\n  created_at TEXT,\n  ttl_hours INTEGER,\n  seed_items TEXT -- JSON array of seed UIDs\n);\n \n-- Cached items (subset from repo index)\nCREATE TABLE cached_items (\n  uid TEXT PRIMARY KEY,\n  -- ... copy of repo index item fields\n);\n \n-- Subgraph slice (only edges relevant to this workset)\nCREATE TABLE cached_links (\n  source_uid TEXT,\n  target_uid TEXT,\n  type TEXT,\n  PRIMARY KEY (source_uid, target_uid, type)\n);\n \n-- Cached chunks (for semantic search within workset)\nCREATE TABLE cached_chunks (\n  chunk_id TEXT PRIMARY KEY,\n  parent_uid TEXT,\n  content TEXT,\n  -- ... copy of repo index chunk fields\n);\n \n-- Optional: execution memory (plan, notes, deliverable)\n-- per workset_evaluation_report.md\nWorkset Filesystem Layout (Decision 2026-01-10)\n\nBase Path: _kano/backlog/.cache/worksets/&lt;item-id&gt;/\nContents:\n\nworkset.db — SQLite cache that reuses the canonical schema (ADR-0012)\nplan.md — Execution checklist (three-file pattern)\nnotes.md — Research notes / scratchpad\ndeliverable.md — Draft output waiting for promotion\n\n\nAgent Attribution: workset_manifest.agent records who initialized the workset; directory naming stays per item to keep TTL cleanup simple.\nRationale: Local-first workflows typically have a single active agent per backlog item. Owner locking (KABSD-TSK-0036) prevents concurrent edits; adding agent IDs to the filesystem path would duplicate manifest data and complicate cleanup.\nFuture Extension: If multiple agents must share a task concurrently, we can add optional &lt;agent_id&gt; suffixes, but the default is per-item directories for deterministic paths.\n\n3. Query Path\nAgent Query\r\n    ↓\r\n1. Search Workset (local SQLite)\r\n   ├── Fast: all relevant context already materialized\r\n   └── If sufficient → Return results\r\n    ↓\r\n2. Fallback to Repo-Level Index (if workset insufficient)\r\n   ├── Query repo-level SQLite (items, links, chunks, FTS5)\r\n   ├── Query sidecar ANN (vector search)\r\n   └── Expand via repo-level graph (k-hop from new seeds)\r\n    ↓\r\n3. Optionally Update Workset (incremental enrichment)\r\n   ├── Add newly discovered items/chunks to workset\r\n   └── Extend subgraph slice with new edges\r\n    ↓\r\nReturn results to agent\n\nResponsibilities (Unambiguous)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComponentResponsibilityWhat It IsWhat It Is NOTCanonical FilesSource of truthMarkdown + frontmatter in GitNOT queryable at scaleRepo-Level GraphPrimary graph structureShared, rebuildable index of all edgesNOT per-agent cacheWorksetPer-task cache bundleLocal, ephemeral, task-scoped materialized contextNOT source of truth, NOT authoritative graphSidecar ANNVector similarity searchFast semantic search (per ADR-0009)NOT metadata storeSQLite IndexFast relational queriesDerived metadata + FTS (per ADR-0004)NOT source of truth\nRetrieval Strategy (Detailed)\nWorkset-First Strategy\nWhen to use Workset-first:\n\nDuring active task execution (agent has claimed a task)\nWhen workset is fresh (within TTL window)\nWhen working context is stable (no major scope changes)\n\nBenefits:\n\nFast: No re-querying repo-level index\nStable: Context doesn’t change mid-task\nOffline-friendly: Workset can be pre-built and used offline\n\nRepo-Index Fallback\nWhen to fallback to repo-level index:\n\nWorkset expired or missing\nQuery requires cross-cutting view (e.g., “all items blocking any active task”)\nNew information needed that wasn’t in initial workset seeds\n\nFallback process:\n\nQuery repo-level SQLite (items, links, chunks, FTS5)\nQuery sidecar ANN if semantic search needed\nExpand via graph if relationship traversal needed\nCache results in workset for future queries (optional incremental enrichment)\n\nIncremental Enrichment (Optional)\nAfter fallback, agent MAY update workset:\n\nAdd newly discovered items/chunks\nExtend subgraph slice with new edges\nUpdate workset manifest (enrichment timestamp)\n\nGuardrails:\n\nWorkset size limits (prevent unbounded growth)\nEnrichment policy (e.g., only add items within 2-hop distance)\nTTL still applies (workset expires regardless of enrichment)\n\nTrade-offs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrade-offDescriptionWorkset StalenessWorkset may become stale if canonical files change during task execution. Mitigation: TTL + periodic rebuild.Dual MaintenanceNeed to maintain both repo-level index and workset build logic. Mitigation: Shared indexing code, clear derivation rules.Subgraph Slice ComplexityDeciding which edges to include in workset subgraph is non-trivial. Mitigation: Start with simple k-hop expansion, iterate.Storage OverheadWorksets duplicate data from repo index. Mitigation: Worksets are ephemeral, cleaned up by TTL.\nConsequences\nPositive\n\nClear Boundaries: No ambiguity about which component owns what\nRebuildable: All derived data can be deleted and reconstructed\nScalable: Worksets enable efficient multi-agent collaboration without index contention\nComposable: Graph, vector search, and worksets work together without conflict\n\nNegative\n\nComplexity: More components to understand and maintain\nSync Logic: Need careful handling of cache invalidation and TTL\nLearning Curve: Developers must understand the distinction between repo-level and workset-level data\n\nMitigations\n\nDocumentation: This ADR + inline code comments\nTooling: Scripts to rebuild indices, inspect worksets, validate consistency\nDefaults: Worksets are optional; can disable for simple single-agent scenarios\n\nReferences\n\ncache layer and per‑Agent workset cache (TTL)\nKABSD-FTR-0015: Execution Layer: Workset Cache + Promote\nADR-0004: File-First Architecture with SQLite Index\nADR-0009: Local-First Embedding Search Strategic Evaluation\nWorkset Evaluation Report\nKABSD-TSK-0217: Task tracking this specification\n\nFuture Work\nThis ADR establishes the foundation. Future enhancements may include:\n\nStrong Graph / Entity Extraction: LLM-based relationship mining beyond frontmatter\nMulti-Agent Workset Coordination: Shared worksets for pair programming scenarios\nWorkset Templates: Pre-configured recipes for common task types\nGraph Visualization: Tools to visualize repo-level graph and workset subgraphs\nPerformance Benchmarks: Measure workset-first vs repo-index-first query performance\n\nDecision Rationale\nWhy separate Workset and Graph?\n\nDifferent lifecycles: Graph is long-lived and shared; Workset is ephemeral and local\nDifferent query patterns: Graph is for exploration/expansion; Workset is for stable task context\nDifferent consistency models: Graph must stay in sync with canonical files; Workset can be stale within TTL\n\nWhy NOT merge them?\n\nMerging would force either (a) graph to be per-agent (duplication, inconsistency) or (b) workset to be shared (defeats the purpose of local cache)\nClear separation enables independent evolution and optimization of each component\n\nWhy repo-level graph is primary?\n\nGraph relationships are project-wide knowledge (e.g., “what blocks what”)\nPer-agent graphs would diverge and create confusion\nWorksets can include subgraph slices for fast local queries, but authoritative graph must be shared\n\nStatus\nProposed (2026-01-09)\nThis ADR is proposed for review. Once accepted, it becomes the architectural constraint for all future Workset and GraphRAG implementation work.\n\nThis ADR was created as part of KABSD-TSK-0217 to prevent role confusion between Workset, GraphRAG, and Context Graph."},"adr/ADR-0012_workset-db-canonical-schema-reuse":{"title":"Workset DB Uses Canonical Schema (No Parallel Schema)","links":["_meta/canonical_schema.sql","_meta/canonical_schema.json","artifacts/workset_schema_verification_examples","adr/ADR-0003_identifier-strategy-for-local-first-backlog","adr/ADR-0004_file-first-architecture-with-sqlite-index","adr/ADR-0008_sqlite-schema-migration-framework","adr/ADR-0011_workset-graphrag-context-graph-separation-of-responsibilities","items/task/0000/KABSD-TSK-0046_define-db-index-schema-items-links-worklog-decisions","items/feature/0000/KABSD-FTR-0013_add-derived-index-cache-layer-and-peragent-workset-cache-ttl","items/feature/0000/KABSD-FTR-0015_execution-layer-workset-cache-promote"],"tags":[],"content":"Decision\nWorkset DB must reuse the same system schema and semantics as the source-of-truth model, rather than creating a separate “workset-only schema”.\nWorkset DB is a materialized subset view of the canonical model, not a different model.\nContext and Problem Statement\nWe maintain canonical backlog data as local-first files (source of truth). We also plan to generate worksets (per-task/per-agent context bundles) as SQLite DBs for fast retrieval and stable context.\nWe want worksets to be derived data, rebuildable at any time, and we already rely on a globally unique UID for identity (ADR-0003).\nThe Question: Should workset DB have its own schema design, or should it reuse the canonical schema defined for the repo-level derived index?\nThe Risk: If workset has its own schema, it will inevitably diverge from the canonical data model, creating long-term maintenance cost, bugs, and integration friction.\nRationale\nWhy This Is Important\n1. Avoid Schema Drift\n\nIf workset has its own schema, it will diverge from the canonical data model over time\nEvery schema evolution would require parallel changes in two places\nDifferent schemas lead to subtle semantic mismatches and data loss during translation\n\n2. Portable Context with Zero Translation\n\nAgents/tools that understand the canonical schema can read a workset DB without custom mapping logic\nThis reduces integration friction across tools (CLI, future server façade, GUI)\nA workset can be directly queried using the same queries used for the repo-level index\n\n3. Deterministic Rebuild\n\nWorkset is derived: it must be regeneratable from source-of-truth + derived indexes\nReusing schema makes regeneration straightforward and verifiable\nSchema migrations (ADR-0008) apply uniformly to both repo index and worksets\n\n4. Consistent Identity &amp; References\n\nWorkitems/ADRs keep the same UID and same link semantics across canonical and workset DB\nEdges (parent/ref/depends) remain consistent\nNo need for ID translation or mapping tables\n\n5. Future-Proofing for Graph-Assisted Retrieval\n\nGraph expansion can materialize a subgraph into workset without inventing new edge formats\nWorkset becomes a “view slice” of the full graph, not a separate graph model\n\nCanonical Schema (Reused by Workset DB)\nThe canonical schema is defined in ADR-0004 and KABSD-TSK-0046. It represents:\nCore Entities\nitems Table\nCore metadata for all work items (Epic/Feature/Story/Task/Bug) and ADRs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumnTypeDescriptionuidTEXT (PK)UUIDv7 (globally unique, from frontmatter)idTEXTDisplay ID (e.g., KABSD-TSK-0049)typeTEXTWork item type (Epic, Feature, UserStory, Task, Bug, ADR)stateTEXTCurrent state (Proposed, Ready, InProgress, Done, etc.)titleTEXTItem titlepathTEXTRelative path to canonical filemtimeREALFile modification timestampcontent_hashTEXTHash of content (for change detection)frontmatterJSONFull frontmatter blob (flexibility)createdTEXTCreation date (ISO 8601)updatedTEXTLast updated date (ISO 8601)priorityTEXTPriority (P1, P2, P3, etc.)parent_uidTEXTUID of parent item (null if root)ownerTEXTCurrent owner/assigneeareaTEXTFunctional areaiterationTEXTIteration/sprint identifiertagsJSONArray of tags\nlinks Table\nTracks typed relationships for graph queries.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumnTypeDescriptionsource_uidTEXTLink source (referencing item)target_uidTEXTLink target (referenced item)typeTEXTLink type: “parent”, “relates_to”, “blocks”, “blocked_by”, “decision_ref”PRIMARY KEY(source_uid, target_uid, type)\nworklog Table (Optional but Canonical)\nStores append-only worklog entries.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumnTypeDescriptionuidTEXT (PK)Unique worklog entry IDitem_uidTEXTUID of parent itemtimestampTEXTISO 8601 timestampagentTEXTAgent/user who created entrycontentTEXTWorklog entry text\nchunks Table (For Embedding/FTS)\nStores content chunks for semantic search.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumnTypeDescriptionchunk_idTEXT (PK)Unique chunk identifierparent_uidTEXTUID of parent itemchunk_indexINTSequence number within parentcontentTEXTChunk text contentsectionTEXTSection type (Context, Goal, Approach, etc.)embeddingBLOBFloat32 vector array (optional)\nSchema Metadata\nPer ADR-0008, track schema version for migrations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumnTypeDescriptionkeyTEXT (PK)Metadata key (e.g., “schema_version”)valueTEXTMetadata value\nWorkset as a Subset\nA workset DB contains:\n\nIncluded nodes: A filtered subset of items (selected by workset recipe)\nIncluded edges: links restricted to included nodes (or optionally include boundary edges)\nIncluded chunks: Content chunks for included items\nIncluded worklog: Worklog entries for included items (optional)\n\nNOT included in workset:\n\nItems outside the selected scope\nEdges between excluded nodes\n\nWorkset-Specific Metadata (Additive Only)\nWorkset DB MAY add workset-specific metadata tables, but MUST NOT change core entity schemas.\nAllowed: workset_manifest Table\nCREATE TABLE workset_manifest (\n  workset_id TEXT PRIMARY KEY,\n  agent TEXT NOT NULL,\n  task_id TEXT,\n  created_at TEXT NOT NULL,\n  ttl_hours INTEGER,\n  seed_items TEXT,  -- JSON array of seed UIDs\n  expansion_params TEXT,  -- JSON: {k_hop: 2, edge_types: [...]}\n  source_commit_hash TEXT,  -- Git commit of canonical files\n  canonical_index_version TEXT NOT NULL CHECK (canonical_index_version &lt;&gt; &#039;&#039;)  -- Schema version of source index\n);\nAllowed: workset_provenance Table\nCREATE TABLE workset_provenance (\n  item_uid TEXT PRIMARY KEY,\n  selection_reason TEXT NOT NULL,  -- &quot;seed&quot;, &quot;parent_expansion&quot;, &quot;dependency_expansion&quot;, &quot;manual&quot;\n  distance_from_seed INTEGER,  -- Hop count from nearest seed (0 for seeds)\n  included_at TEXT NOT NULL,  -- ISO 8601 timestamp when item was added\n  FOREIGN KEY (item_uid) REFERENCES items(uid) ON DELETE CASCADE\n);\nThese tables are additive — they extend the canonical schema without changing core table definitions.\nContent Storage Strategy\nWorkset DB supports multiple content strategies (choose based on use case):\nOption 1: Full Content (Portable)\n\nStore complete item content in workset DB (in items.frontmatter JSON or separate content column)\nPros: Workset is fully portable, can be used offline\nCons: Larger DB size, duplication of content\n\nOption 2: Pointer-Based (Smaller)\n\nStore only uid, path, and content_hash in workset\nRequire access to canonical files for full content retrieval\nPros: Smaller workset DB, no content duplication\nCons: Not portable, requires canonical file access\n\nOption 3: Hybrid (Recommended)\n\nStore summaries/excerpts in workset (title, first N words of sections)\nStore pointers to canonical files + hashes for verification\nOptionally include full content for “hot” items (recently accessed)\nPros: Balanced size vs portability\nCons: More complex logic\n\nDecision: Support all three strategies via configuration. Default to Hybrid for best balance.\nSchema Evolution and Migrations\nPer ADR-0008, schema migrations apply uniformly:\n\n\nRepo-level index migration:\n\nApply migration 001_add_vcs_cache_tables.sql\nUpdate schema_meta.schema_version = &#039;1&#039;\n\n\n\nWorkset DB migration (when rebuilding workset):\n\nDetect source index schema version from canonical_index_version in manifest\nApply same migrations to workset DB\nEnsure workset schema version matches canonical schema version\n\n\n\nConstraint: Workset DB schema version MUST NOT exceed canonical schema version.\nRebuild Rule: If canonical schema is upgraded, all worksets MUST be rebuilt or auto-migrated.\nGuidelines for Maintaining Schema Compatibility\nDO: Add Workset-Specific Tables\n✅ Add workset_manifest, workset_provenance, or similar metadata tables\r\n✅ These tables MUST be prefixed with workset_ to avoid naming conflicts\r\n✅ Document all workset-specific tables in this ADR or code comments\nDO NOT: Modify Core Table Schemas\n❌ Do NOT change items, links, chunks, worklog, or schema_meta table definitions\r\n❌ Do NOT add columns to core tables specific to worksets\r\n❌ Do NOT rename or remove columns from canonical schema\nDO: Subset Core Tables\n✅ Workset items table contains fewer rows than canonical items (filtering is allowed)\r\n✅ Workset links table only includes edges relevant to included nodes\nDO: Preserve Field Semantics\n✅ uid means the same thing in workset and canonical DB (globally unique identifier)\r\n✅ state values match canonical state vocabulary (Proposed, Ready, InProgress, Done, etc.)\r\n✅ type values match canonical type vocabulary (Epic, Feature, UserStory, Task, Bug, ADR)\nDO: Version Compatibility Checks\n✅ When loading a workset, verify canonical_index_version matches expected schema\r\n✅ If version mismatch, warn or auto-rebuild workset\nAcceptance Criteria\n\n Canonical schema is defined (ADR-0004, this ADR)\n Workset DB reuses canonical items, links, chunks, worklog tables\n Workset-specific metadata is additive only (workset_manifest, workset_provenance)\n Content storage strategy is documented (full/pointer/hybrid)\n Schema migration compatibility is specified (workset follows canonical migrations)\n Guidelines for adding workset metadata are documented (DO/DO NOT rules)\n SQL schema definition created (canonical_schema.sql)\n JSON schema definition created (canonical_schema.json)\n Verification examples documented (workset_schema_verification_examples.md)\n Implementation validates schema compatibility at workset build time (future work)\n Tools that read canonical schema can read workset DB without special-case mapping (verified via examples)\n\nNon-Goals\n\nWorkset DB is NOT a new source-of-truth\nDo NOT implement a separate “workset schema v2”\nDo NOT require workset DB to contain all canonical data (it’s a subset by definition)\nDo NOT implement server runtime (per AGENTS.md temporary clause)\n\nConsequences\nPositive\n\nNo Schema Drift: Single schema definition for all derived DBs\nPortable Context: Worksets can be shared, inspected, queried with standard tools\nSimplified Maintenance: Schema migrations apply uniformly\nConsistent Identity: UIDs and link semantics preserved across canonical and workset\n\nNegative\n\nWorkset Constraints: Workset DB cannot optimize schema for workset-specific use cases\nMigration Coupling: Workset rebuild required when canonical schema changes\n\nMitigations\n\nExtensibility: Workset-specific tables allowed (additive only)\nRebuild Automation: Make workset rebuild fast and deterministic\nVersion Checks: Detect and handle schema version mismatches gracefully\n\nAlternatives Considered\n1. Separate Workset Schema\nApproach: Design a custom schema optimized for workset use cases.\nRejected because:\n\nSchema drift inevitable (maintenance burden)\nTranslation layer required (complexity, bugs)\nBreaks portable context (tools need dual schema support)\n\n2. Denormalized Workset Schema\nApproach: Flatten canonical schema into a denormalized “workset view” (e.g., single table with all fields).\nRejected because:\n\nLoses relational structure (graph queries become difficult)\nContent duplication (same item appears multiple times with different join results)\nStill requires mapping/translation logic\n\n3. Schema-Free (JSON Blobs Only)\nApproach: Store items as raw JSON blobs in workset DB.\nRejected because:\n\nNo relational query support (defeats purpose of SQL index)\nNo FTS or graph traversal without parsing JSON\nStill need consistent JSON schema (same drift problem)\n\nReferences\n\nADR-0003: Identifier Strategy (UID) — Global UID ensures identity consistency\nADR-0004: File-First Architecture with SQLite Index — Canonical schema definition\nADR-0008: SQLite Schema Migration Framework — Migration strategy\nADR-0011: Workset vs GraphRAG Separation — Workset role definition\nKABSD-TSK-0046: Define DB Index Schema — Original schema definition task\nKABSD-FTR-0013: Workset Cache — Workset feature\nKABSD-FTR-0015: Workset Promote — Workset execution layer\n\nFuture Work\n\nDefine JSON schema for canonical frontmatter (complementary to SQL schema)\nBenchmark workset query performance vs canonical index\nImplement schema version compatibility checker for workset loading\nAdd workset content strategy configuration (full/pointer/hybrid)\nCreate workset rebuild automation on schema migration\n\nVerification\nSee Workset Schema Verification Examples for test cases demonstrating:\n\nSchema compatibility checks\nSchema version tracking\nCore table consistency\nWorkset-specific table validation\nSubset semantics verification\nDeterministic rebuild testing\n\nStatus\nProposed (2026-01-09)\nThis ADR is proposed for review. Once accepted, it becomes the architectural constraint for all future Workset DB implementation work.\n\nThis ADR ensures that workset DB remains a true materialized view of the canonical schema, preventing schema drift and maintaining portable, rebuildable context bundles."},"adr/ADR-0013_codebase-architecture-and-module-boundaries":{"title":"Codebase Architecture and Module Boundaries","links":["adr/ADR-0015_skill-scoped-cli-namespace-convention","adr/ADR-0037_inspector-pattern-and-query-surface-architecture","items/feature/0000/KABSD-FTR-0028_refactor-kano-agent-backlog-skill-scripts-into-a-single-cli-entry-library-modules","KABSD-FTR-0025_unified-cli-for-backlog-operations","KABSD-FTR-0019_refactor-kano-backlog-core-cli-server-gui-facades","adr/ADR-0004_file-first-architecture-with-sqlite-index"],"tags":[],"content":"Decision\nEstablish strict separation between executable entrypoints (scripts/) and library modules (src/). All agent-callable operations must go through a single CLI entrypoint (scripts/kano), which delegates to library use-cases.\nHard Rules\n\nscripts/ is executable-only: No reusable module code in scripts/. Scripts must not be imported as libraries.\nSingle CLI entrypoint: Agents call only scripts/kano &lt;subcommand&gt;. All operations are exposed through this interface.\nsrc/ is import-only: Core logic lives in src/kano_backlog_* packages. These are imported by the CLI (and future facades), never executed directly.\nConsistent gating: All write operations run prereqs + initialization checks via a single gate layer in the CLI.\nDeterministic output: Same input state produces stable, reproducible output for views and queries.\n\nContext\nCurrent State (Problems)\nThe scripts/ directory contains 40+ standalone Python scripts with overlapping responsibilities:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCategoryScriptsIssuesbacklog/35+ scriptsMixed executable + library code in same folderbootstrap/1 scriptOKindexing/8 scriptsSome logic should be library codefs/5 scriptsFile operations, OK as thin wrappersvcs/4 adaptersAlready library-style but in scripts/common/4 modulesShared code incorrectly placed in scripts/logging/3 modulesShared code incorrectly placed in scripts/\nProblems:\n\nCommon logic mixed into scripts makes it hard to enforce consistent gating (prereqs/initialized/dry-run).\nCoding agents don’t have a clear architecture reference; new code gets placed inconsistently.\nDifferent scripts may bypass checks or diverge behavior over time.\nNo single entry point exists; agents must know which script to call.\n\nExisting Foundation\nWe already have:\n\nsrc/kano_backlog_core/: Core models, config, errors, refs, state (good foundation)\nsrc/kano_cli/: CLI skeleton with Typer, ~4 commands implemented\nscripts/backlog/lib/: Some shared code (should move to src/)\nscripts/backlog/cli/: Thin wrappers (good pattern, needs expansion)\n\nArchitecture\nLayered Architecture\nflowchart TB\r\n  subgraph Agent[&quot;Coding Agent / Human&quot;]\r\n    A[&quot;calls scripts/kano (CLI)&quot;]\r\n  end\r\n\r\n  subgraph Scripts[&quot;scripts/ (executable-only)&quot;]\r\n    CLI[&quot;scripts/kano\\n├─ parse args\\n├─ run gates (prereqs/init)\\n└─ call lib use-cases&quot;]\r\n  end\r\n\r\n  subgraph Src[&quot;src/ (import-only)&quot;]\r\n    Core[&quot;kano_backlog_core\\n(config/models/ids/errors/refs/state)&quot;]\r\n    Ops[&quot;kano_backlog_ops\\n(use-cases: init/create/update/\\nindex/workset/view)&quot;]\r\n    Adapters[&quot;kano_backlog_adapters\\n(sqlite/fts/faiss/vcs/fs)&quot;]\r\n    CLI_Pkg[&quot;kano_cli\\n(Typer app, commands)&quot;]\r\n    Hooks[&quot;kano_backlog_hooks (future)\\n(pre/post hooks interface)&quot;]\r\n  end\r\n\r\n  subgraph Data[&quot;Data Layer&quot;]\r\n    SoT[&quot;Source of Truth\\n(_kano/backlog/*.md)&quot;]\r\n    Cache[&quot;Derived Cache\\n(_kano/backlog/_index/*.sqlite3)&quot;]\r\n  end\r\n\r\n  A --&gt; CLI\r\n  CLI --&gt; CLI_Pkg\r\n  CLI_Pkg --&gt; Ops\r\n  Ops --&gt; Core\r\n  Ops --&gt; Adapters\r\n  Adapters --&gt; SoT\r\n  Adapters --&gt; Cache\r\n  Ops -. optional .-&gt; Hooks\n\nTarget Folder Structure\nflowchart LR\r\n  R[&quot;skills/kano-agent-backlog-skill/&quot;] --&gt; S[&quot;scripts/&quot;]\r\n  S --&gt; K[&quot;kano (only entrypoint)&quot;]\r\n  S --&gt; B[&quot;backlog/ (deprecated wrappers)&quot;]\r\n  S --&gt; I[&quot;bootstrap/, fs/ (thin utilities)&quot;]\r\n\r\n  R --&gt; SRC[&quot;src/&quot;]\r\n  SRC --&gt; CORE[&quot;kano_backlog_core/\\n(models, ids, config, errors)&quot;]\r\n  SRC --&gt; OPS[&quot;kano_backlog_ops/\\n(use-cases)&quot;]\r\n  SRC --&gt; ADP[&quot;kano_backlog_adapters/\\n(backends)&quot;]\r\n  SRC --&gt; CLIPKG[&quot;kano_cli/\\n(Typer commands)&quot;]\r\n\r\n  R --&gt; REF[&quot;references/\\n(schemas, docs)&quot;]\r\n  R --&gt; TPL[&quot;templates/\\n(markdown templates)&quot;]\r\n  R --&gt; DEC[&quot;decisions/ (this ADR)&quot;]\n\nPackage Responsibilities\nkano_backlog_core (existing, expand)\n\nmodels.py: Pydantic models for work items, ADRs\nconfig.py: Configuration loading, defaults\nids.py: ID parsing, generation, validation\nerrors.py: Custom exceptions\nrefs.py: Reference resolution logic\nstate.py: State machine definitions\naudit.py: Audit logging primitives\n\nkano_backlog_ops (new)\nUse-case functions that orchestrate operations:\n\ninit.py: Initialize backlog structure\nworkitem.py: Create, update, validate work items\nadr.py: Create, list ADRs\nworkset.py: Workset management (init/refresh/promote)\nview.py: Generate views, dashboards\nindex.py: Build/refresh SQLite index\n\nkano_backlog_adapters (new)\nPluggable backends:\n\nfs.py: File system operations (read/write markdown)\nsqlite.py: SQLite index adapter\nfts.py: Full-text search adapter\nembedding.py: Vector embedding adapter (optional)\nvcs/: VCS adapters (git, svn, perforce)\n\nkano_cli (existing, expand)\nTyper-based CLI application:\n\ncli.py: Main app, callback for gating\ncommands/: Subcommand modules (item, worklog, view, adr, index, workset)\nutil.py: CLI utilities (output formatting, path resolution)\n\nCLI Command Structure (Implemented)\nkano\r\n├── doctor              # Check prereqs + initialization\r\n├── backlog             # Backlog administration group\r\n│   ├── init            # Initialize backlog structure\r\n│   ├── index\r\n│   │   ├── build       # Build SQLite index\r\n│   │   └── refresh     # Refresh index (MVP: full rebuild)\r\n│   ├── demo\r\n│   │   └── seed        # Seed demo data for testing\r\n│   ├── persona\r\n│   │   ├── summary     # Generate persona activity summary\r\n│   │   └── report      # Generate persona state report\r\n│   └── sandbox\r\n│       └── init        # Scaffold isolated sandbox environment\r\n├── item\r\n│   ├── create          # Create work item\r\n│   ├── read            # Read item details\r\n│   ├── update-state    # Transition state + worklog append\r\n│   ├── validate        # Check Ready gate\r\n│   └── create-v2       # Alias for create (compatibility)\r\n├── state\r\n│   └── transition      # Declarative state transitions\r\n├── worklog\r\n│   └── append          # Append worklog entry\r\n├── view\r\n│   └── refresh         # Refresh all dashboards\r\n└── init (legacy)       # Alias for `backlog init` (deprecated)\n\nMigration Strategy\nPhase 0: ADR + SKILL Gate (This ADR)\n\n Create this ADR with architecture diagrams\n Update SKILL.md: skill developers must read ADR-0013 before coding\n\nPhase 1: CLI Skeleton ✅ COMPLETE\n\n Expanded src/kano_cli/commands/ to cover all high-frequency operations\n Add kano doctor for prereqs/init checks\n Implemented: item, state, worklog, view commands\n\nPhase 2: Library Migration ✅ COMPLETE\n\n Created src/kano_backlog_ops/ with use-case functions (init, workitem, adr, view, index, demo, persona, sandbox)\n Created src/kano_backlog_adapters/ for backend abstraction (partially)\n Moved logic from scripts/backlog/*.py into library packages\n Added backlog subcommand group with nested commands (index, demo, persona, sandbox)\n\nPhase 3: Deprecation ✅ COMPLETE\n\n Deleted 70+ legacy scripts from scripts/ directory\n Updated all documentation to recommend kano CLI\n Legacy kano init backlog aliased to kano backlog init with deprecation warning\n\nPhase 4: Future Extensions (Deferred)\n\nPlugin/hook system for external integrations\nNative engine option (C++/Rust via pybind11) for performance-critical paths\nHTTP/MCP server facade (reuses same kano_backlog_ops use-cases)\n\nTrade-offs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrade-offDescriptionMigration effortSignificant refactoring of existing scripts. Mitigated by phased approach.Backward compatibilityOld script paths break for agents. Mitigated by keeping thin wrappers.Initial complexityMore packages to maintain. Pays off with clearer boundaries and reusability.\nConsequences\n\nFor skill developers: Must read this ADR before adding code. New logic goes in src/, not scripts/.\nFor agents: Call only scripts/kano-backlog. Direct script calls are deprecated. See ADR-0015_skill-scoped-cli-namespace-convention for skill-scoped CLI naming convention.\nFor future facades: HTTP/MCP/GUI can import kano_backlog_ops directly, no CLI dependency.\nFor testing: Use-case functions in src/ are easier to unit test than CLI scripts.\nNaming convention: This skill follows skill-scoped naming (kano-backlog, kano_backlog_*). The bare kano namespace is reserved for a future umbrella CLI. See ADR-0015_skill-scoped-cli-namespace-convention for full rationale.\nInspector Pattern: External agents (health, review, security) consume query surface APIs from kano_backlog_ops, never write to canonical SoT directly. See ADR-0037_inspector-pattern-and-query-surface-architecture for full architecture.\n\nRelated\n\nKABSD-FTR-0028_refactor-kano-agent-backlog-skill-scripts-into-a-single-cli-entry-library-modules.md: Parent feature for this refactoring\nKABSD-FTR-0025_unified-cli-for-backlog-operations: Unified CLI (subset of this work)\nKABSD-FTR-0019_refactor-kano-backlog-core-cli-server-gui-facades: Core/CLI/Server/GUI facades separation\nADR-0004_file-first-architecture-with-sqlite-index: File-first architecture (complements this ADR)\nADR-0015_skill-scoped-cli-namespace-convention: Skill-scoped CLI namespace convention (naming strategy)\nADR-0037_inspector-pattern-and-query-surface-architecture: Inspector Pattern and Query Surface Architecture (extends module boundaries with external agent integration)\n"},"adr/ADR-0014_plugin-and-hook-system-architecture":{"title":"Plugin and Hook System Architecture (Phase 4 - Deferred)","links":["adr/ADR-0013_codebase-architecture-and-module-boundaries"],"tags":["architecture","extensibility","phase4","deferred"],"content":"Plugin and Hook System Architecture (Phase 4 - Deferred)\nStatus\nProposed - Design documented for future implementation. Phase 4 is deferred until concrete integration needs emerge.\nContext\nPer ADR-0013 Phase 4, we anticipate future needs for:\n\n\nPre/post hooks: External projects may want to inject custom logic before/after backlog operations (e.g., notify external systems, run custom validations, sync with remote databases).\n\n\nCustom engines: Performance-critical operations (workset retrieval, embedding search) may benefit from native (C++/Rust) implementations or external service integration.\n\n\nExternal tool integration: Projects may want to bridge kano backlog operations with external systems (JIRA sync, Slack notifications, custom dashboards).\n\n\nProblem: Hardcoding integrations into the core codebase violates separation of concerns and makes the skill harder to maintain.\nCurrent state: As of Phase 1-3 completion, kano_backlog_ops provides a clean use-case layer, but no hook/plugin mechanism exists.\nDecision\nDesign Principles\n\n\nInterface over Implementation: Define stable contracts (Python protocols/abstract base classes) that plugins must implement.\n\n\nRegistration-based Discovery: Plugins register themselves via entry points or configuration files; no hardcoded imports.\n\n\nFail-safe Defaults: If a plugin fails to load or execute, the system continues with default behavior (log warning but don’t crash).\n\n\nOptional Dependencies: Core skill must function without any plugins installed.\n\n\nProposed Architecture\nflowchart TB\r\n  subgraph CLI[&quot;kano CLI&quot;]\r\n    cmd[kano item create]\r\n  end\r\n  \r\n  subgraph Ops[&quot;kano_backlog_ops (Use-Cases)&quot;]\r\n    uc[create_item]\r\n  end\r\n  \r\n  subgraph Hooks[&quot;kano_backlog_hooks (Optional)&quot;]\r\n    mgr[HookManager]\r\n    pre[PreCreateHook protocol]\r\n    post[PostCreateHook protocol]\r\n  end\r\n  \r\n  subgraph Plugins[&quot;External Plugins (Optional)&quot;]\r\n    jira[jira-sync-plugin]\r\n    slack[slack-notify-plugin]\r\n    custom[custom-validation]\r\n  end\r\n  \r\n  subgraph Core[&quot;kano_backlog_core&quot;]\r\n    models[BacklogItem models]\r\n  end\r\n  \r\n  cmd --&gt; uc\r\n  uc --&gt; mgr\r\n  mgr --&gt; pre\r\n  pre -.optional.-&gt; jira\r\n  pre -.optional.-&gt; custom\r\n  mgr --&gt; post\r\n  post -.optional.-&gt; slack\r\n  uc --&gt; models\n\nHook Types\n\n\nOperation Hooks (Pre/Post):\n\nPreCreateHook: Validate/modify item before creation\nPostCreateHook: Notify external systems after creation\nPreUpdateStateHook: Block invalid state transitions\nPostUpdateStateHook: Trigger workflows on state change\n\n\n\nEngine Replacements:\n\nWorksetEngine: Interface for workset retrieval (default: Python, optional: Rust/C++)\nEmbeddingSearchEngine: Interface for ANN search (default: FAISS Python, optional: Qdrant/Milvus)\nIndexBuilder: Interface for index construction (default: SQLite Python, optional: DuckDB)\n\n\n\nPlugin Discovery\nOption A: Entry Points (Preferred for Python ecosystem)\n# pyproject.toml of external plugin\n[project.entry-points.&quot;kano_backlog.hooks&quot;]\njira-sync = &quot;jira_sync_plugin:JiraSyncHook&quot;\nOption B: Configuration File\n// _kano/backlog/.kano/plugins.json\n{\n  &quot;hooks&quot;: {\n    &quot;post_create&quot;: [&quot;jira_sync_plugin.JiraSyncHook&quot;, &quot;slack.NotifyHook&quot;]\n  },\n  &quot;engines&quot;: {\n    &quot;workset&quot;: &quot;workset_native.RustEngine&quot;\n  }\n}\nHook Protocol Example\nfrom typing import Protocol\nfrom kano_backlog_core.models import BacklogItem\n \nclass PreCreateHook(Protocol):\n    &quot;&quot;&quot;Protocol for pre-creation hooks.&quot;&quot;&quot;\n    \n    def execute(self, item: BacklogItem) -&gt; BacklogItem:\n        &quot;&quot;&quot;\n        Called before item creation.\n        \n        Args:\n            item: Item about to be created (may be modified)\n        \n        Returns:\n            Modified item (or original if no changes)\n        \n        Raises:\n            HookVetoError: If hook rejects the operation\n        &quot;&quot;&quot;\n        ...\n \nclass PostCreateHook(Protocol):\n    &quot;&quot;&quot;Protocol for post-creation hooks.&quot;&quot;&quot;\n    \n    def execute(self, item: BacklogItem) -&gt; None:\n        &quot;&quot;&quot;\n        Called after item creation.\n        \n        Args:\n            item: Newly created item (read-only)\n        \n        Note:\n            This should not raise exceptions; log errors internally\n        &quot;&quot;&quot;\n        ...\nConsequences\nPositive\n\nExtensibility: Projects can integrate custom logic without modifying skill core\nMaintainability: Core skill remains simple; complexity lives in plugins\nFlexibility: Users choose which plugins to install/enable\nEcosystem: Encourages community-contributed integrations\n\nNegative\n\nComplexity: Hook system adds architectural overhead (discovery, error handling, versioning)\nTesting burden: Must test core with/without plugins, handle plugin failures gracefully\nDocumentation: Need clear plugin development guide and hook lifecycle docs\nVersioning: Plugin API must be stable; breaking changes require migration path\n\nRisks\n\nPerformance overhead: Hook execution adds latency to every operation\n\nMitigation: Make hooks optional, measure overhead, provide async execution\n\n\nSecurity: Malicious plugins could corrupt backlog or leak data\n\nMitigation: Sandboxing (future), plugin allowlist/blocklist, audit logging\n\n\nDependency hell: Plugin A requires version X, plugin B requires version Y\n\nMitigation: Clear versioning policy, compatibility matrix\n\n\n\nImplementation Plan (When Needed)\nPhase 4a: Hook Interface (2-3 days)\n\nCreate src/kano_backlog_hooks/ package with protocol definitions\nAdd HookManager class for discovery and execution\nUpdate kano_backlog_ops to call hooks (with feature flag)\nWrite hook development guide\n\nPhase 4b: Engine Abstraction (3-5 days)\n\nDefine WorksetEngine, EmbeddingSearchEngine protocols\nRefactor existing code to use default implementations\nAdd engine registry and discovery\nDocument engine replacement guide\n\nPhase 4c: Plugin Ecosystem (Ongoing)\n\nCreate example plugins (jira-sync, slack-notify)\nPublish plugin template repository\nCurate community plugins list\n\nAlternatives Considered\n\n\nNo plugin system: Rely on users forking the skill\n\nRejected: Makes upgrades difficult, fragments ecosystem\n\n\n\nHardcoded integrations: Add JIRA/Slack support directly\n\nRejected: Violates separation of concerns, bloats codebase\n\n\n\nHTTP webhooks only: Use HTTP POST for all hooks\n\nRejected: Requires running server, adds network latency, no pre-hooks\n\n\n\nMonkey-patching: Let users patch functions at runtime\n\nRejected: Fragile, hard to test, no contract enforcement\n\n\n\nReferences\n\nADR-0013: Establishes use-case layer that hooks will integrate with\nPython Entry Points: packaging.python.org/en/latest/specifications/entry-points/\nProtocol classes (PEP 544): peps.python.org/pep-0544/\n\nRevision History\n\n2026-01-11: Initial proposal (copilot) - documented design for future Phase 4 implementation\n"},"adr/ADR-0015_decision-for-kabsd-tsk-0056":{"title":"Decision for KABSD-TSK-0056","links":[],"tags":[],"content":"Status\nProposed\nContext\nThis decision was extracted from workset notes for task/feature KABSD-TSK-0056.\nDecision\nUse fine-grained chunking with semantic section boundaries instead of fixed-size windows. This preserves logical document structure for better retrieval accuracy.\nRationale\nSee workset notes in _kano\\backlog\\sandboxes.cache\\019b8f52-9fc8-7c94-aa2d-806cacdd9086 for full context.\nConsequences\n\nPositive:\n\n…\n\n\nNegative:\n\n…\n\n\nNeutral:\n\n…\n\n\n\nAlternatives Considered\n\nAlternative A: …\nAlternative B: …\n\nReferences\n\nExtracted from workset for KABSD-TSK-0056 by copilot\n"},"adr/ADR-0015_skill-scoped-cli-namespace-convention":{"title":"Skill-Scoped CLI Namespace Convention","links":[],"tags":[],"content":"Decision\nEach skill repository MUST use skill-scoped naming for its CLI entrypoints and Python packages:\n\nCLI script: scripts/&lt;skill-name&gt; (e.g., scripts/kano-backlog)\nPython CLI package: &lt;skill_name&gt;_cli (e.g., kano_backlog_cli)\nConsole script entrypoint (in pyproject.toml): &lt;skill-name&gt; (e.g., kano-backlog)\n\nThe global name kano is reserved for a future umbrella CLI that will aggregate multiple skill CLIs. Individual skills MUST NOT claim the kano name in their own codebase.\nContext\nThe original implementation used universe-level names (kano, kano_cli) inside kano-agent-backlog-skill, assuming this was the only skill. As we add more skills (e.g., kano-commit-convention-skill), these names will collide.\nProblems with the current approach:\n\nkano is too generic for a single skill’s CLI\nMultiple skills can’t coexist if they all claim kano\nMigration confusion when the umbrella CLI is introduced later\n\nGoals:\n\nEnable multiple skill repos to coexist (each with its own scoped CLI)\nReserve kano as a future umbrella command aggregator\nMaintain consistency across all kano-ecosystem skills\n\nOptions Considered\n\n\nKeep kano as-is and ignore future skills ❌\n\nRejected: causes immediate collision when adding second skill.\n\n\n\nUse skill-scoped naming (kano-backlog, kano_backlog_cli) ✅ (chosen)\n\nEach skill is self-contained and independent.\nkano umbrella CLI can be added later without breaking existing skills.\n\n\n\nUse a monorepo with namespace packages\n\nRejected: conflicts with self-contained skill deployment model; skills are designed to be used as git submodules or standalone repos.\n\n\n\nPros / Cons\nPros:\n\nClear ownership: each skill owns its namespace\nNo collision when multiple skills are used together\nFuture-proof: umbrella CLI can be introduced as a separate repo\nAligns with ADR-0013 (module boundaries)\n\nCons:\n\nRequires renaming existing code (kano → kano-backlog, kano_cli → kano_backlog_cli)\nCommand tree changes (kano item → kano-backlog workitem)\nMigration for existing users (mitigated by deprecation wrapper)\n\nConsequences\nImmediate actions (EPIC-0009):\n\nRename scripts/kano → scripts/kano-backlog\nRename src/kano_cli → src/kano_backlog_cli\nUpdate pyproject.toml entrypoint: kano-backlog instead of kano\nRestructure command groups (item → workitem, backlog → admin)\nUpdate all documentation (SKILL.md, README.md, REFERENCE.md)\nProvide deprecated kano wrapper script with migration warning\n\nLong-term (out of scope for this repo):\n\nFuture kano umbrella CLI repo can implement command delegation (e.g., kano backlog &lt;cmd&gt; → kano-backlog &lt;cmd&gt;)\nEach skill continues to work standalone with its scoped CLI\n\nFollow-ups\n\n Implement renaming per KABSD-FTR-0034 (rename packages/scripts)\n Implement command tree restructuring per KABSD-FTR-0035\n Document reservation and future umbrella CLI design per KABSD-FTR-0036\n Update ADR-0013 to reference this naming convention\n"},"adr/ADR-0016_per-product-isolated-index-architecture":{"title":"Per-Product Isolated Index Architecture","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","items/task/0000/KABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation"],"tags":[],"content":"Context\nIn a multi-product monorepo environment, we needed to decide between two architectural approaches for the SQLite index:\n\nPlatform-level shared index: All products write to a single _kano/backlog/_index/backlog.sqlite3\nPer-product isolated index: Each product owns its own index at products/&lt;name&gt;/_index/backlog.sqlite3\n\nDecision Criteria\nWe evaluated 7 dimensions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimensionPer-ProductPlatformWinnerIsolationComplete separationShared namespacePer-ProductConcurrencyNo locking contentionPotential locksPer-ProductPerformancePredictable, isolatedCan degrade with loadPer-ProductScalabilityLinear per productShared bottleneckPer-ProductComplexitySimple, independentComplex coordinationPer-ProductMaintenanceEasy (per-product)Harder (shared state)Per-ProductFuture ExtensibilitySupports embedding DBDifficult to addPer-Product\nOverall Score: Per-Product = 91%, Platform = 43%\nDecision\nImplement per-product isolated SQLite indexes.\nEach product will:\n\nMaintain its own SQLite database at products/&lt;product-name&gt;/_index/backlog.sqlite3\nHave independent schema with product column for self-documentation\nSupport autonomous indexing/rebuilding without affecting other products\nEnable future cross-product aggregation via product-aware queries\n\nImplementation Details\nSchema Design\nAll tables include product column:\nCREATE TABLE items (\n  product TEXT NOT NULL,\n  id TEXT NOT NULL,\n  PRIMARY KEY (product, id),\n  ...\n);\n \nCREATE TABLE item_tags (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  tag TEXT NOT NULL,\n  PRIMARY KEY (product, item_id, tag),\n  FOREIGN KEY (product, item_id) REFERENCES items(product, id)\n);\nIndex Rebuild Process\n\nscripts/indexing/build_sqlite_index.py --product &lt;name&gt;\nScans products/&lt;name&gt;/items/ directory\nExtracts product from file paths\nUpserts into product-specific SQLite database\nMaintains composite key integrity\n\nResolver Behavior\n\nresolve_ref(ref, index, product=None) accepts optional product filter\nIf product not specified, searches across all products\nProduct column enables cross-product queries when needed\n\nRationale\n\nIsolation ensures safety: No possibility of data leakage between products\nPredictable performance: Each product’s index grows independently\nSupports autonomy: Teams can rebuild their product’s index without coordination\nFuture-proof: Enables embedding databases per product or shared aggregation\nSimpler mental model: Products are truly independent\nBackward compatible: Product column facilitates future migrations\n\nAlternatives Considered\nPlatform-level shared index\n\nPros: Unified search across all products\nCons: Complex coordination, shared bottleneck, potential data leakage\nRejected: Architecture does not support team autonomy\n\nHybrid approach (shared metadata + per-product data)\n\nPros: Balanced complexity\nCons: Still requires coordination layer, adds complexity\nRejected: Per-product isolation is simpler and cleaner\n\nFuture Work\nWhen cross-product features are needed:\n\nCreate aggregation layer on top of per-product indexes\nOr implement global embedding database that reads from per-product sources\nProduct column in schema already prepared for this extensibility\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration feature\nKABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation.md: Indexer and resolver product isolation\n"},"adr/ADR-0017_product-column-retention-rationale":{"title":"Product Column Retention in Per-Product Indexes","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","adr/ADR-0016_per-product-isolated-index-architecture"],"tags":[],"content":"Context\nWhen implementing per-product isolated indexes, a question arose: if each product owns its own SQLite database, why include a product column in the schema?\nArguments for removing it:\n\nRedundant (the file path already indicates product)\nAdds storage overhead\nColumn values are always the same for a given database\n\nArguments for keeping it:\n\nSelf-documentation (data independence from file paths)\nConsistency with composite key patterns\nFuture extensibility (global embedding DB, cross-product queries)\nError detection capability\n\nDecision\nRetain the product column in all schema tables.\nEach product’s schema will include a product column despite the apparent redundancy.\nRationale\n1. Data Self-Documentation\nThe product column makes data self-describing. If a database dump or export is created, the product context is explicit in the data itself, not dependent on file path or metadata.\nSELECT * FROM items WHERE product=&#039;kano-agent-backlog-skill&#039;;\n-- vs.\nSELECT * FROM items;  -- How do you know which product this is from?\n2. Consistency with Composite Key Strategy\nUsing (product, id) as composite primary keys throughout the schema ensures:\n\nAll foreign keys are uniform: FOREIGN KEY (product, item_id)\nConsistent pattern across all tables (items, item_tags, item_links, worklog_entries)\nEasier code generation and migrations\n\n3. Uniform Schema Across Products\nAll products use identical schema structure. A tool that works with KABSD’s index works with any product’s index without modification. This is valuable for:\n\nShared tool development\nSchema migration scripts\nIndex validation and repair tools\n\n4. Future Extensibility: Global Embedding Database\nThe primary long-term value is supporting a future global embedding database:\n-- Global embedding store (future)\nCREATE TABLE embeddings (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  embedding BLOB NOT NULL,\n  PRIMARY KEY (product, item_id),\n  FOREIGN KEY (product, item_id) REFERENCES all_items(product, id)\n);\nThis would aggregate embeddings from all products while maintaining product context. The product column enables this without schema rework.\n5. Error Detection\nIncluding product column in per-product indexes enables validation queries:\n-- Verify database integrity\nSELECT DISTINCT product FROM items;\n-- Should always return single row: kano-agent-backlog-skill\n \n-- Detect misplaced files\nSELECT product FROM items WHERE product != &#039;kano-agent-backlog-skill&#039;;\n-- Should return empty set\n6. Cross-Product Queries (Future)\nWhen analytics or reporting tools are built, they may aggregate across products:\n-- Future: Query across products via union or aggregation\nSELECT product, COUNT(*) as item_count FROM all_items GROUP BY product;\nThe product column is essential for this without painful migrations.\nImplementation\nAll tables maintain the pattern:\nCREATE TABLE items (\n  product TEXT NOT NULL,\n  id TEXT NOT NULL,\n  uid UUID,\n  type TEXT,\n  -- ... other columns\n  PRIMARY KEY (product, id)\n);\n \nCREATE TABLE item_tags (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  tag TEXT NOT NULL,\n  PRIMARY KEY (product, item_id, tag),\n  FOREIGN KEY (product, item_id) REFERENCES items(product, id)\n);\nAlternatives Considered\nRemove product column entirely\n\nPros: Minimal storage, no apparent redundancy\nCons: Breaks future embedding DB design, harder to validate integrity, poor data self-documentation\nRejected: Future extensibility cost is too high\n\nMake product optional/nullable\n\nPros: Allows querying “which product” implicitly\nCons: Makes schema ambiguous, difficult to enforce consistency\nRejected: Product should always be explicit and required\n\nFuture Work\nWhen global embedding database is implemented:\n\nProduct column in schema already prepared\nNo schema migration required\nTools can immediately work with cross-product data\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration\nADR-0016_per-product-isolated-index-architecture: Per-Product Isolated Index Architecture\n"},"adr/ADR-0018_decision-for-kabsd-tsk-0056":{"title":"Decision for KABSD-TSK-0056","links":[],"tags":[],"content":"Status\nProposed\nContext\nThis decision was extracted from workset notes for task/feature KABSD-TSK-0056.\nDecision\nUse fine-grained chunking with semantic section boundaries instead of fixed-size windows. This preserves logical document structure for better retrieval accuracy.\nRationale\nSee workset notes in _kano\\backlog\\sandboxes.cache\\019b8f52-9fc8-7c94-aa2d-806cacdd9086 for full context.\nConsequences\n\nPositive:\n\n…\n\n\nNegative:\n\n…\n\n\nNeutral:\n\n…\n\n\n\nAlternatives Considered\n\nAlternative A: …\nAlternative B: …\n\nReferences\n\nExtracted from workset for KABSD-TSK-0056 by copilot\n"},"adr/ADR-0019_decision-for-kabsd-tsk-0056":{"title":"Decision for KABSD-TSK-0056","links":[],"tags":[],"content":"Status\nProposed\nContext\nThis decision was extracted from workset notes for task/feature KABSD-TSK-0056.\nDecision\nUse fine-grained chunking with semantic section boundaries instead of fixed-size windows. This preserves logical document structure for better retrieval accuracy.\nRationale\nSee workset notes in _kano\\backlog\\sandboxes.cache\\019b8f52-9fc8-7c94-aa2d-806cacdd9086 for full context.\nConsequences\n\nPositive:\n\n…\n\n\nNegative:\n\n…\n\n\nNeutral:\n\n…\n\n\n\nAlternatives Considered\n\nAlternative A: …\nAlternative B: …\n\nReferences\n\nExtracted from workset for KABSD-TSK-0056 by copilot\n"},"adr/ADR-0020_decision-for-kabsd-tsk-0056":{"title":"Decision for KABSD-TSK-0056","links":[],"tags":[],"content":"Status\nProposed\nContext\nThis decision was extracted from workset notes for task/feature KABSD-TSK-0056.\nDecision\nUse fine-grained chunking with semantic section boundaries instead of fixed-size windows. This preserves logical document structure for better retrieval accuracy.\nRationale\nSee workset notes in _kano\\backlog\\sandboxes.cache\\019b8f52-9fc8-7c94-aa2d-806cacdd9086 for full context.\nConsequences\n\nPositive:\n\n…\n\n\nNegative:\n\n…\n\n\nNeutral:\n\n…\n\n\n\nAlternatives Considered\n\nAlternative A: …\nAlternative B: …\n\nReferences\n\nExtracted from workset for KABSD-TSK-0056 by copilot\n"},"adr/ADR-0021_per-product-isolated-index-architecture":{"title":"Per-Product Isolated Index Architecture","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","items/task/0000/KABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation"],"tags":[],"content":"Context\nIn a multi-product monorepo environment, we needed to decide between two architectural approaches for the SQLite index:\n\nPlatform-level shared index: All products write to a single _kano/backlog/_index/backlog.sqlite3\nPer-product isolated index: Each product owns its own index at products/&lt;name&gt;/_index/backlog.sqlite3\n\nDecision Criteria\nWe evaluated 7 dimensions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimensionPer-ProductPlatformWinnerIsolationComplete separationShared namespacePer-ProductConcurrencyNo locking contentionPotential locksPer-ProductPerformancePredictable, isolatedCan degrade with loadPer-ProductScalabilityLinear per productShared bottleneckPer-ProductComplexitySimple, independentComplex coordinationPer-ProductMaintenanceEasy (per-product)Harder (shared state)Per-ProductFuture ExtensibilitySupports embedding DBDifficult to addPer-Product\nOverall Score: Per-Product = 91%, Platform = 43%\nDecision\nImplement per-product isolated SQLite indexes.\nEach product will:\n\nMaintain its own SQLite database at products/&lt;product-name&gt;/_index/backlog.sqlite3\nHave independent schema with product column for self-documentation\nSupport autonomous indexing/rebuilding without affecting other products\nEnable future cross-product aggregation via product-aware queries\n\nImplementation Details\nSchema Design\nAll tables include product column:\nCREATE TABLE items (\n  product TEXT NOT NULL,\n  id TEXT NOT NULL,\n  PRIMARY KEY (product, id),\n  ...\n);\n \nCREATE TABLE item_tags (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  tag TEXT NOT NULL,\n  PRIMARY KEY (product, item_id, tag),\n  FOREIGN KEY (product, item_id) REFERENCES items(product, id)\n);\nIndex Rebuild Process\n\nscripts/indexing/build_sqlite_index.py --product &lt;name&gt;\nScans products/&lt;name&gt;/items/ directory\nExtracts product from file paths\nUpserts into product-specific SQLite database\nMaintains composite key integrity\n\nResolver Behavior\n\nresolve_ref(ref, index, product=None) accepts optional product filter\nIf product not specified, searches across all products\nProduct column enables cross-product queries when needed\n\nRationale\n\nIsolation ensures safety: No possibility of data leakage between products\nPredictable performance: Each product’s index grows independently\nSupports autonomy: Teams can rebuild their product’s index without coordination\nFuture-proof: Enables embedding databases per product or shared aggregation\nSimpler mental model: Products are truly independent\nBackward compatible: Product column facilitates future migrations\n\nAlternatives Considered\nPlatform-level shared index\n\nPros: Unified search across all products\nCons: Complex coordination, shared bottleneck, potential data leakage\nRejected: Architecture does not support team autonomy\n\nHybrid approach (shared metadata + per-product data)\n\nPros: Balanced complexity\nCons: Still requires coordination layer, adds complexity\nRejected: Per-product isolation is simpler and cleaner\n\nFuture Work\nWhen cross-product features are needed:\n\nCreate aggregation layer on top of per-product indexes\nOr implement global embedding database that reads from per-product sources\nProduct column in schema already prepared for this extensibility\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration feature\nKABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation.md: Indexer and resolver product isolation\n"},"adr/ADR-0022_product-column-retention-rationale":{"title":"Product Column Retention in Per-Product Indexes","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","adr/ADR-0004_file-first-architecture-with-sqlite-index"],"tags":[],"content":"Context\nWhen implementing per-product isolated indexes, a question arose: if each product owns its own SQLite database, why include a product column in the schema?\nArguments for removing it:\n\nRedundant (the file path already indicates product)\nAdds storage overhead\nColumn values are always the same for a given database\n\nArguments for keeping it:\n\nSelf-documentation (data independence from file paths)\nConsistency with composite key patterns\nFuture extensibility (global embedding DB, cross-product queries)\nError detection capability\n\nDecision\nRetain the product column in all schema tables.\nEach product’s schema will include a product column despite the apparent redundancy.\nRationale\n1. Data Self-Documentation\nThe product column makes data self-describing. If a database dump or export is created, the product context is explicit in the data itself, not dependent on file path or metadata.\nSELECT * FROM items WHERE product=&#039;kano-agent-backlog-skill&#039;;\n-- vs.\nSELECT * FROM items;  -- How do you know which product this is from?\n2. Consistency with Composite Key Strategy\nUsing (product, id) as composite primary keys throughout the schema ensures:\n\nAll foreign keys are uniform: FOREIGN KEY (product, item_id)\nConsistent pattern across all tables (items, item_tags, item_links, worklog_entries)\nEasier code generation and migrations\n\n3. Uniform Schema Across Products\nAll products use identical schema structure. A tool that works with KABSD’s index works with any product’s index without modification. This is valuable for:\n\nShared tool development\nSchema migration scripts\nIndex validation and repair tools\n\n4. Future Extensibility: Global Embedding Database\nThe primary long-term value is supporting a future global embedding database:\n-- Global embedding store (future)\nCREATE TABLE embeddings (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  embedding BLOB NOT NULL,\n  PRIMARY KEY (product, item_id),\n  FOREIGN KEY (product, item_id) REFERENCES all_items(product, id)\n);\nThis would aggregate embeddings from all products while maintaining product context. The product column enables this without schema rework.\n5. Error Detection\nIncluding product column in per-product indexes enables validation queries:\n-- Verify database integrity\nSELECT DISTINCT product FROM items;\n-- Should always return single row: kano-agent-backlog-skill\n \n-- Detect misplaced files\nSELECT product FROM items WHERE product != &#039;kano-agent-backlog-skill&#039;;\n-- Should return empty set\n6. Cross-Product Queries (Future)\nWhen analytics or reporting tools are built, they may aggregate across products:\n-- Future: Query across products via union or aggregation\nSELECT product, COUNT(*) as item_count FROM all_items GROUP BY product;\nThe product column is essential for this without painful migrations.\nImplementation\nAll tables maintain the pattern:\nCREATE TABLE items (\n  product TEXT NOT NULL,\n  id TEXT NOT NULL,\n  uid UUID,\n  type TEXT,\n  -- ... other columns\n  PRIMARY KEY (product, id)\n);\n \nCREATE TABLE item_tags (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  tag TEXT NOT NULL,\n  PRIMARY KEY (product, item_id, tag),\n  FOREIGN KEY (product, item_id) REFERENCES items(product, id)\n);\nAlternatives Considered\nRemove product column entirely\n\nPros: Minimal storage, no apparent redundancy\nCons: Breaks future embedding DB design, harder to validate integrity, poor data self-documentation\nRejected: Future extensibility cost is too high\n\nMake product optional/nullable\n\nPros: Allows querying “which product” implicitly\nCons: Makes schema ambiguous, difficult to enforce consistency\nRejected: Product should always be explicit and required\n\nFuture Work\nWhen global embedding database is implemented:\n\nProduct column in schema already prepared\nNo schema migration required\nTools can immediately work with cross-product data\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration\nADR-0004_file-first-architecture-with-sqlite-index: Per-Product Isolated Index Architecture\n"},"adr/ADR-0023_graph-assisted-retrieval-and-context-graph":{"title":"Graph-assisted retrieval with a derived Context Graph (weak graph first)","links":["adr/ADR-0004_file-first-architecture-with-sqlite-index","adr/ADR-0009_local-first-embedding-search-architecture","items/feature/0000/KABSD-FTR-0007_optional-db-index-and-embedding-rag-pipeline","items/feature/0000/KABSD-FTR-0023_graph-assisted-rag-planning-and-minimal-implementation"],"tags":[],"content":"Decision\nAdopt Graph-assisted retrieval as a minimal, local-first improvement to context quality:\n\nUse FTS/embeddings to retrieve seed nodes\nUse a derived Context Graph to expand to load-bearing neighbors (k-hop traversal)\nKeep everything derived/rebuildable from canonical Markdown (file-first)\n\nThis ADR explicitly chooses weak graph first: only structured relationships (no LLM entity extraction).\nContext\nWe already have a file-first backlog with optional derived indexes (SQLite / FTS / embeddings).\r\nVector-only retrieval often returns text-similar chunks but misses the structural context (parents, ADR decisions, dependency chains).\nWe want a deterministic, auditable way to expand context that is:\n\nlocal-first\nderived/rebuildable\nincrementally maintainable\nsafe (bounded expansion to avoid prompt bloat)\n\nDefinitions\n\nContext Graph: a derived, typed graph of artifact relationships (items, ADRs, dependencies, etc.).\nSeed set: top-N nodes from FTS/embedding retrieval.\nGraph expansion: k-hop traversal from seeds over allowlisted edges with limits.\n\nGraph model (v1)\nNodes\nMinimum node types:\n\nwork_item (Epic/Feature/UserStory/Task/Bug)\nadr\n\nOptional (for embedding/fts pipelines):\n\nchunk (document chunk tied to a parent doc)\n\nEdges\nMinimum edge types:\n\nparent (child → parent)\ndecision_ref (work_item → adr)\nrelates (work_item → work_item)\nblocks / blocked_by\n\nStorage (derived)\nThe Context Graph is derived data. Implementations may:\n\n\nMaterialize into SQLite\n\nreuse items as the node registry\nstore edges in a links-style table (source_uid, target_uid, type, optional weight, source_path)\n\n\n\nSidecar graph artifacts\n\n&lt;backlog-root&gt;/_index/graph_nodes.jsonl\n&lt;backlog-root&gt;/_index/graph_edges.jsonl\n\n\n\nBoth must be safe to delete and rebuild.\nRetrieval strategy (Graph-assisted RAG)\n\nSeed retrieval\n\nFTS and/or embeddings return top-N seed nodes/chunks\n\n\nExpand\n\ntraverse k-hop (default k=1)\nedge allowlist and fanout caps\n\n\nRe-rank\n\nweights by doctype and section (ADR decision &gt; item title/acceptance &gt; worklog)\noptionally prioritize Ready/InProgress items\n\n\nContext packing\n\nemit a context pack describing:\n\nseeds (why selected)\nneighbors (which edge pulled them in)\nminimal excerpts/anchors (title/ids/links)\n\n\n\n\n\nConfig surface (indicative)\n\nretrieval.graph.enabled\nretrieval.graph.k_hop\nretrieval.graph.edge_allowlist\nretrieval.graph.max_neighbors_per_seed\nretrieval.weights.* (doctype/section/state weights)\n\nConsequences\n\nGraph-assisted retrieval becomes the preferred way to preserve traceability (seed + neighbors).\nTooling must keep expansion bounded to avoid context explosions.\nThe design stays compatible with file-scan fallback and optional SQLite/embedding acceleration.\n\nNon-goals\n\nLLM/NLP entity extraction and automatic relation mining\nserver/MCP mode or cross-repo graphs\n\nReferences\n\nADR-0004 File-first + SQLite index\nADR-0009 Local-first embedding search\nRAG pipeline\nKABSD-FTR-0023 Graph-assisted RAG planning\n"},"adr/ADR-0024_decision-for-kabsd-tsk-0056":{"title":"Decision for KABSD-TSK-0056","links":[],"tags":[],"content":"Status\nProposed\nContext\nThis decision was extracted from workset notes for task/feature KABSD-TSK-0056.\nDecision\nUse fine-grained chunking with semantic section boundaries instead of fixed-size windows. This preserves logical document structure for better retrieval accuracy.\nRationale\nSee workset notes in _kano\\backlog\\sandboxes.cache\\019b8f52-9fc8-7c94-aa2d-806cacdd9086 for full context.\nConsequences\n\nPositive:\n\n…\n\n\nNegative:\n\n…\n\n\nNeutral:\n\n…\n\n\n\nAlternatives Considered\n\nAlternative A: …\nAlternative B: …\n\nReferences\n\nExtracted from workset for KABSD-TSK-0056 by copilot\n"},"adr/ADR-0025_decision-for-kabsd-tsk-0056":{"title":"Decision for KABSD-TSK-0056","links":[],"tags":[],"content":"Status\nProposed\nContext\nThis decision was extracted from workset notes for task/feature KABSD-TSK-0056.\nDecision\nUse fine-grained chunking with semantic section boundaries instead of fixed-size windows. This preserves logical document structure for better retrieval accuracy.\nRationale\nSee workset notes in _kano\\backlog\\sandboxes.cache\\019b8f52-9fc8-7c94-aa2d-806cacdd9086 for full context.\nConsequences\n\nPositive:\n\n…\n\n\nNegative:\n\n…\n\n\nNeutral:\n\n…\n\n\n\nAlternatives Considered\n\nAlternative A: …\nAlternative B: …\n\nReferences\n\nExtracted from workset for KABSD-TSK-0056 by copilot\n"},"adr/ADR-0026_decision-for-kabsd-tsk-0056":{"title":"Decision for KABSD-TSK-0056","links":[],"tags":[],"content":"Status\nProposed\nContext\nThis decision was extracted from workset notes for task/feature KABSD-TSK-0056.\nDecision\nUse fine-grained chunking with semantic section boundaries instead of fixed-size windows. This preserves logical document structure for better retrieval accuracy.\nRationale\nSee workset notes in _kano\\backlog\\sandboxes.cache\\019b8f52-9fc8-7c94-aa2d-806cacdd9086 for full context.\nConsequences\n\nPositive:\n\n…\n\n\nNegative:\n\n…\n\n\nNeutral:\n\n…\n\n\n\nAlternatives Considered\n\nAlternative A: …\nAlternative B: …\n\nReferences\n\nExtracted from workset for KABSD-TSK-0056 by copilot\n"},"adr/ADR-0027_decision-for-kabsd-tsk-0056":{"title":"Decision for KABSD-TSK-0056","links":[],"tags":[],"content":"Status\nProposed\nContext\nThis decision was extracted from workset notes for task/feature KABSD-TSK-0056.\nDecision\nUse fine-grained chunking with semantic section boundaries instead of fixed-size windows. This preserves logical document structure for better retrieval accuracy.\nRationale\nSee workset notes in _kano\\backlog\\sandboxes.cache\\019b8f52-9fc8-7c94-aa2d-806cacdd9086 for full context.\nConsequences\n\nPositive:\n\n…\n\n\nNegative:\n\n…\n\n\nNeutral:\n\n…\n\n\n\nAlternatives Considered\n\nAlternative A: …\nAlternative B: …\n\nReferences\n\nExtracted from workset for KABSD-TSK-0056 by copilot\n"},"adr/ADR-0028_per-product-isolated-index-architecture":{"title":"Per-Product Isolated Index Architecture","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","items/task/0000/KABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation"],"tags":[],"content":"Context\nIn a multi-product monorepo environment, we needed to decide between two architectural approaches for the SQLite index:\n\nPlatform-level shared index: All products write to a single _kano/backlog/_index/backlog.sqlite3\nPer-product isolated index: Each product owns its own index at products/&lt;name&gt;/_index/backlog.sqlite3\n\nDecision Criteria\nWe evaluated 7 dimensions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimensionPer-ProductPlatformWinnerIsolationComplete separationShared namespacePer-ProductConcurrencyNo locking contentionPotential locksPer-ProductPerformancePredictable, isolatedCan degrade with loadPer-ProductScalabilityLinear per productShared bottleneckPer-ProductComplexitySimple, independentComplex coordinationPer-ProductMaintenanceEasy (per-product)Harder (shared state)Per-ProductFuture ExtensibilitySupports embedding DBDifficult to addPer-Product\nOverall Score: Per-Product = 91%, Platform = 43%\nDecision\nImplement per-product isolated SQLite indexes.\nEach product will:\n\nMaintain its own SQLite database at products/&lt;product-name&gt;/_index/backlog.sqlite3\nHave independent schema with product column for self-documentation\nSupport autonomous indexing/rebuilding without affecting other products\nEnable future cross-product aggregation via product-aware queries\n\nImplementation Details\nSchema Design\nAll tables include product column:\nCREATE TABLE items (\n  product TEXT NOT NULL,\n  id TEXT NOT NULL,\n  PRIMARY KEY (product, id),\n  ...\n);\n \nCREATE TABLE item_tags (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  tag TEXT NOT NULL,\n  PRIMARY KEY (product, item_id, tag),\n  FOREIGN KEY (product, item_id) REFERENCES items(product, id)\n);\nIndex Rebuild Process\n\nscripts/indexing/build_sqlite_index.py --product &lt;name&gt;\nScans products/&lt;name&gt;/items/ directory\nExtracts product from file paths\nUpserts into product-specific SQLite database\nMaintains composite key integrity\n\nResolver Behavior\n\nresolve_ref(ref, index, product=None) accepts optional product filter\nIf product not specified, searches across all products\nProduct column enables cross-product queries when needed\n\nRationale\n\nIsolation ensures safety: No possibility of data leakage between products\nPredictable performance: Each product’s index grows independently\nSupports autonomy: Teams can rebuild their product’s index without coordination\nFuture-proof: Enables embedding databases per product or shared aggregation\nSimpler mental model: Products are truly independent\nBackward compatible: Product column facilitates future migrations\n\nAlternatives Considered\nPlatform-level shared index\n\nPros: Unified search across all products\nCons: Complex coordination, shared bottleneck, potential data leakage\nRejected: Architecture does not support team autonomy\n\nHybrid approach (shared metadata + per-product data)\n\nPros: Balanced complexity\nCons: Still requires coordination layer, adds complexity\nRejected: Per-product isolation is simpler and cleaner\n\nFuture Work\nWhen cross-product features are needed:\n\nCreate aggregation layer on top of per-product indexes\nOr implement global embedding database that reads from per-product sources\nProduct column in schema already prepared for this extensibility\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration feature\nKABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation.md: Indexer and resolver product isolation\n"},"adr/ADR-0029_product-column-retention-rationale":{"title":"Product Column Retention in Per-Product Indexes","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","adr/ADR-0004_file-first-architecture-with-sqlite-index"],"tags":[],"content":"Context\nWhen implementing per-product isolated indexes, a question arose: if each product owns its own SQLite database, why include a product column in the schema?\nArguments for removing it:\n\nRedundant (the file path already indicates product)\nAdds storage overhead\nColumn values are always the same for a given database\n\nArguments for keeping it:\n\nSelf-documentation (data independence from file paths)\nConsistency with composite key patterns\nFuture extensibility (global embedding DB, cross-product queries)\nError detection capability\n\nDecision\nRetain the product column in all schema tables.\nEach product’s schema will include a product column despite the apparent redundancy.\nRationale\n1. Data Self-Documentation\nThe product column makes data self-describing. If a database dump or export is created, the product context is explicit in the data itself, not dependent on file path or metadata.\nSELECT * FROM items WHERE product=&#039;kano-agent-backlog-skill&#039;;\n-- vs.\nSELECT * FROM items;  -- How do you know which product this is from?\n2. Consistency with Composite Key Strategy\nUsing (product, id) as composite primary keys throughout the schema ensures:\n\nAll foreign keys are uniform: FOREIGN KEY (product, item_id)\nConsistent pattern across all tables (items, item_tags, item_links, worklog_entries)\nEasier code generation and migrations\n\n3. Uniform Schema Across Products\nAll products use identical schema structure. A tool that works with KABSD’s index works with any product’s index without modification. This is valuable for:\n\nShared tool development\nSchema migration scripts\nIndex validation and repair tools\n\n4. Future Extensibility: Global Embedding Database\nThe primary long-term value is supporting a future global embedding database:\n-- Global embedding store (future)\nCREATE TABLE embeddings (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  embedding BLOB NOT NULL,\n  PRIMARY KEY (product, item_id),\n  FOREIGN KEY (product, item_id) REFERENCES all_items(product, id)\n);\nThis would aggregate embeddings from all products while maintaining product context. The product column enables this without schema rework.\n5. Error Detection\nIncluding product column in per-product indexes enables validation queries:\n-- Verify database integrity\nSELECT DISTINCT product FROM items;\n-- Should always return single row: kano-agent-backlog-skill\n \n-- Detect misplaced files\nSELECT product FROM items WHERE product != &#039;kano-agent-backlog-skill&#039;;\n-- Should return empty set\n6. Cross-Product Queries (Future)\nWhen analytics or reporting tools are built, they may aggregate across products:\n-- Future: Query across products via union or aggregation\nSELECT product, COUNT(*) as item_count FROM all_items GROUP BY product;\nThe product column is essential for this without painful migrations.\nImplementation\nAll tables maintain the pattern:\nCREATE TABLE items (\n  product TEXT NOT NULL,\n  id TEXT NOT NULL,\n  uid UUID,\n  type TEXT,\n  -- ... other columns\n  PRIMARY KEY (product, id)\n);\n \nCREATE TABLE item_tags (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  tag TEXT NOT NULL,\n  PRIMARY KEY (product, item_id, tag),\n  FOREIGN KEY (product, item_id) REFERENCES items(product, id)\n);\nAlternatives Considered\nRemove product column entirely\n\nPros: Minimal storage, no apparent redundancy\nCons: Breaks future embedding DB design, harder to validate integrity, poor data self-documentation\nRejected: Future extensibility cost is too high\n\nMake product optional/nullable\n\nPros: Allows querying “which product” implicitly\nCons: Makes schema ambiguous, difficult to enforce consistency\nRejected: Product should always be explicit and required\n\nFuture Work\nWhen global embedding database is implemented:\n\nProduct column in schema already prepared\nNo schema migration required\nTools can immediately work with cross-product data\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration\nADR-0004_file-first-architecture-with-sqlite-index: Per-Product Isolated Index Architecture\n"},"adr/ADR-0030_graph-assisted-retrieval-and-context-graph":{"title":"Graph-assisted retrieval with a derived Context Graph (weak graph first)","links":["adr/ADR-0004_file-first-architecture-with-sqlite-index","adr/ADR-0009_local-first-embedding-search-architecture","items/feature/0000/KABSD-FTR-0007_optional-db-index-and-embedding-rag-pipeline","items/feature/0000/KABSD-FTR-0023_graph-assisted-rag-planning-and-minimal-implementation"],"tags":[],"content":"Decision\nAdopt Graph-assisted retrieval as a minimal, local-first improvement to context quality:\n\nUse FTS/embeddings to retrieve seed nodes\nUse a derived Context Graph to expand to load-bearing neighbors (k-hop traversal)\nKeep everything derived/rebuildable from canonical Markdown (file-first)\n\nThis ADR explicitly chooses weak graph first: only structured relationships (no LLM entity extraction).\nContext\nWe already have a file-first backlog with optional derived indexes (SQLite / FTS / embeddings).\r\nVector-only retrieval often returns text-similar chunks but misses the structural context (parents, ADR decisions, dependency chains).\nWe want a deterministic, auditable way to expand context that is:\n\nlocal-first\nderived/rebuildable\nincrementally maintainable\nsafe (bounded expansion to avoid prompt bloat)\n\nDefinitions\n\nContext Graph: a derived, typed graph of artifact relationships (items, ADRs, dependencies, etc.).\nSeed set: top-N nodes from FTS/embedding retrieval.\nGraph expansion: k-hop traversal from seeds over allowlisted edges with limits.\n\nGraph model (v1)\nNodes\nMinimum node types:\n\nwork_item (Epic/Feature/UserStory/Task/Bug)\nadr\n\nOptional (for embedding/fts pipelines):\n\nchunk (document chunk tied to a parent doc)\n\nEdges\nMinimum edge types:\n\nparent (child → parent)\ndecision_ref (work_item → adr)\nrelates (work_item → work_item)\nblocks / blocked_by\n\nStorage (derived)\nThe Context Graph is derived data. Implementations may:\n\n\nMaterialize into SQLite\n\nreuse items as the node registry\nstore edges in a links-style table (source_uid, target_uid, type, optional weight, source_path)\n\n\n\nSidecar graph artifacts\n\n&lt;backlog-root&gt;/_index/graph_nodes.jsonl\n&lt;backlog-root&gt;/_index/graph_edges.jsonl\n\n\n\nBoth must be safe to delete and rebuild.\nRetrieval strategy (Graph-assisted RAG)\n\nSeed retrieval\n\nFTS and/or embeddings return top-N seed nodes/chunks\n\n\nExpand\n\ntraverse k-hop (default k=1)\nedge allowlist and fanout caps\n\n\nRe-rank\n\nweights by doctype and section (ADR decision &gt; item title/acceptance &gt; worklog)\noptionally prioritize Ready/InProgress items\n\n\nContext packing\n\nemit a context pack describing:\n\nseeds (why selected)\nneighbors (which edge pulled them in)\nminimal excerpts/anchors (title/ids/links)\n\n\n\n\n\nConfig surface (indicative)\n\nretrieval.graph.enabled\nretrieval.graph.k_hop\nretrieval.graph.edge_allowlist\nretrieval.graph.max_neighbors_per_seed\nretrieval.weights.* (doctype/section/state weights)\n\nConsequences\n\nGraph-assisted retrieval becomes the preferred way to preserve traceability (seed + neighbors).\nTooling must keep expansion bounded to avoid context explosions.\nThe design stays compatible with file-scan fallback and optional SQLite/embedding acceleration.\n\nNon-goals\n\nLLM/NLP entity extraction and automatic relation mining\nserver/MCP mode or cross-repo graphs\n\nReferences\n\nADR-0004 File-first + SQLite index\nADR-0009 Local-first embedding search\nRAG pipeline\nKABSD-FTR-0023 Graph-assisted RAG planning\n"},"adr/ADR-0031_per-product-isolated-index-architecture":{"title":"Per-Product Isolated Index Architecture","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","items/task/0000/KABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation"],"tags":[],"content":"Context\nIn a multi-product monorepo environment, we needed to decide between two architectural approaches for the SQLite index:\n\nPlatform-level shared index: All products write to a single _kano/backlog/_index/backlog.sqlite3\nPer-product isolated index: Each product owns its own index at products/&lt;name&gt;/_index/backlog.sqlite3\n\nDecision Criteria\nWe evaluated 7 dimensions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimensionPer-ProductPlatformWinnerIsolationComplete separationShared namespacePer-ProductConcurrencyNo locking contentionPotential locksPer-ProductPerformancePredictable, isolatedCan degrade with loadPer-ProductScalabilityLinear per productShared bottleneckPer-ProductComplexitySimple, independentComplex coordinationPer-ProductMaintenanceEasy (per-product)Harder (shared state)Per-ProductFuture ExtensibilitySupports embedding DBDifficult to addPer-Product\nOverall Score: Per-Product = 91%, Platform = 43%\nDecision\nImplement per-product isolated SQLite indexes.\nEach product will:\n\nMaintain its own SQLite database at products/&lt;product-name&gt;/_index/backlog.sqlite3\nHave independent schema with product column for self-documentation\nSupport autonomous indexing/rebuilding without affecting other products\nEnable future cross-product aggregation via product-aware queries\n\nImplementation Details\nSchema Design\nAll tables include product column:\nCREATE TABLE items (\n  product TEXT NOT NULL,\n  id TEXT NOT NULL,\n  PRIMARY KEY (product, id),\n  ...\n);\n \nCREATE TABLE item_tags (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  tag TEXT NOT NULL,\n  PRIMARY KEY (product, item_id, tag),\n  FOREIGN KEY (product, item_id) REFERENCES items(product, id)\n);\nIndex Rebuild Process\n\nscripts/indexing/build_sqlite_index.py --product &lt;name&gt;\nScans products/&lt;name&gt;/items/ directory\nExtracts product from file paths\nUpserts into product-specific SQLite database\nMaintains composite key integrity\n\nResolver Behavior\n\nresolve_ref(ref, index, product=None) accepts optional product filter\nIf product not specified, searches across all products\nProduct column enables cross-product queries when needed\n\nRationale\n\nIsolation ensures safety: No possibility of data leakage between products\nPredictable performance: Each product’s index grows independently\nSupports autonomy: Teams can rebuild their product’s index without coordination\nFuture-proof: Enables embedding databases per product or shared aggregation\nSimpler mental model: Products are truly independent\nBackward compatible: Product column facilitates future migrations\n\nAlternatives Considered\nPlatform-level shared index\n\nPros: Unified search across all products\nCons: Complex coordination, shared bottleneck, potential data leakage\nRejected: Architecture does not support team autonomy\n\nHybrid approach (shared metadata + per-product data)\n\nPros: Balanced complexity\nCons: Still requires coordination layer, adds complexity\nRejected: Per-product isolation is simpler and cleaner\n\nFuture Work\nWhen cross-product features are needed:\n\nCreate aggregation layer on top of per-product indexes\nOr implement global embedding database that reads from per-product sources\nProduct column in schema already prepared for this extensibility\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration feature\nKABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation.md: Indexer and resolver product isolation\n"},"adr/ADR-0032_per-product-isolated-index-architecture":{"title":"Per-Product Isolated Index Architecture","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","items/task/0000/KABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation"],"tags":[],"content":"Context\nIn a multi-product monorepo environment, we needed to decide between two architectural approaches for the SQLite index:\n\nPlatform-level shared index: All products write to a single _kano/backlog/_index/backlog.sqlite3\nPer-product isolated index: Each product owns its own index at products/&lt;name&gt;/_index/backlog.sqlite3\n\nDecision Criteria\nWe evaluated 7 dimensions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimensionPer-ProductPlatformWinnerIsolationComplete separationShared namespacePer-ProductConcurrencyNo locking contentionPotential locksPer-ProductPerformancePredictable, isolatedCan degrade with loadPer-ProductScalabilityLinear per productShared bottleneckPer-ProductComplexitySimple, independentComplex coordinationPer-ProductMaintenanceEasy (per-product)Harder (shared state)Per-ProductFuture ExtensibilitySupports embedding DBDifficult to addPer-Product\nOverall Score: Per-Product = 91%, Platform = 43%\nDecision\nImplement per-product isolated SQLite indexes.\nEach product will:\n\nMaintain its own SQLite database at products/&lt;product-name&gt;/_index/backlog.sqlite3\nHave independent schema with product column for self-documentation\nSupport autonomous indexing/rebuilding without affecting other products\nEnable future cross-product aggregation via product-aware queries\n\nImplementation Details\nSchema Design\nAll tables include product column:\nCREATE TABLE items (\n  product TEXT NOT NULL,\n  id TEXT NOT NULL,\n  PRIMARY KEY (product, id),\n  ...\n);\n \nCREATE TABLE item_tags (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  tag TEXT NOT NULL,\n  PRIMARY KEY (product, item_id, tag),\n  FOREIGN KEY (product, item_id) REFERENCES items(product, id)\n);\nIndex Rebuild Process\n\nscripts/indexing/build_sqlite_index.py --product &lt;name&gt;\nScans products/&lt;name&gt;/items/ directory\nExtracts product from file paths\nUpserts into product-specific SQLite database\nMaintains composite key integrity\n\nResolver Behavior\n\nresolve_ref(ref, index, product=None) accepts optional product filter\nIf product not specified, searches across all products\nProduct column enables cross-product queries when needed\n\nRationale\n\nIsolation ensures safety: No possibility of data leakage between products\nPredictable performance: Each product’s index grows independently\nSupports autonomy: Teams can rebuild their product’s index without coordination\nFuture-proof: Enables embedding databases per product or shared aggregation\nSimpler mental model: Products are truly independent\nBackward compatible: Product column facilitates future migrations\n\nAlternatives Considered\nPlatform-level shared index\n\nPros: Unified search across all products\nCons: Complex coordination, shared bottleneck, potential data leakage\nRejected: Architecture does not support team autonomy\n\nHybrid approach (shared metadata + per-product data)\n\nPros: Balanced complexity\nCons: Still requires coordination layer, adds complexity\nRejected: Per-product isolation is simpler and cleaner\n\nFuture Work\nWhen cross-product features are needed:\n\nCreate aggregation layer on top of per-product indexes\nOr implement global embedding database that reads from per-product sources\nProduct column in schema already prepared for this extensibility\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration feature\nKABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation.md: Indexer and resolver product isolation\n"},"adr/ADR-0033_decision-for-kabsd-tsk-0056":{"title":"Decision for KABSD-TSK-0056","links":[],"tags":[],"content":"Status\nProposed\nContext\nThis decision was extracted from workset notes for task/feature KABSD-TSK-0056.\nDecision\nUse fine-grained chunking with semantic section boundaries instead of fixed-size windows. This preserves logical document structure for better retrieval accuracy.\nRationale\nSee workset notes in _kano\\backlog\\sandboxes.cache\\019b8f52-9fc8-7c94-aa2d-806cacdd9086 for full context.\nConsequences\n\nPositive:\n\n…\n\n\nNegative:\n\n…\n\n\nNeutral:\n\n…\n\n\n\nAlternatives Considered\n\nAlternative A: …\nAlternative B: …\n\nReferences\n\nExtracted from workset for KABSD-TSK-0056 by copilot\n"},"adr/ADR-0034_conflict-handling-policy-for-duplicate-ids-and-uids":{"title":"Conflict handling policy for duplicate IDs and UIDs","links":[],"tags":[],"content":"Decision\nAdopt a configurable conflict policy for duplicate IDs and UIDs, with defaults:\n\nconflict_policy.id_conflict = &quot;rename&quot;: when duplicate display IDs are detected, rename duplicates to the next available ID.\nconflict_policy.uid_conflict = &quot;trash_shorter&quot;: when the same UID appears with differing content, keep the longer file and move the shorter file to _trash/&lt;YYYYMMDD&gt;/....\n\nTie-breaker for equal length: keep the lexicographically earliest path and trash the other(s).\nContext\nWe need deterministic, low-friction behavior for duplicate IDs/UIDs across agents. Recent link integrity repairs showed how ambiguity slows down remediation and risks cross-agent divergence.\nOptions Considered\n\nAlways report and require human intervention.\nAuto-rename duplicate IDs and auto-trash UID conflicts.\nAuto-rename for both ID and UID conflicts.\n\nPros / Cons\n\nAuto-rename reduces manual cleanup for benign ID collisions.\nTrashing UID conflicts is safer than deletion and preserves a recovery path.\nAuto-remediation can hide mistakes if applied without review; defaults should be documented and configurable.\n\nConsequences\n\nadmin links normalize-ids will apply these defaults unless overridden in config.\nUID conflict resolution will create _trash/ entries; audits should treat them as recoverable artifacts.\n\nFollow-ups\n\nDocument the policy in the skill and config defaults.\n"},"adr/ADR-0035_cross-lingual-retrieval-requirement-and-default-embedding-policy":{"title":"Cross-lingual retrieval requirement and default embedding policy","links":[],"tags":[],"content":"Decision\nCross-lingual retrieval is a requirement.\nDefault embedding policy must be multilingual-capable. Provider/model choices must be evaluated against a small multilingual benchmark corpus that includes CJK and mixed-language queries.\nContext\nThis repo’s backlog contains mixed-language content (English + CJK). The semantic retrieval system is local-first and derived from canonical Markdown (see ADR-0009).\nIf we choose an embedder that is not multilingual-capable, cross-lingual queries will fail or regress unpredictably, and benchmark results will not reflect real usage.\nOptions Considered\nOption A: English-only retrieval\nTreat cross-lingual retrieval as out-of-scope. Only optimize for English content.\nOption B: Cross-lingual retrieval required (multilingual embedder policy) [chosen]\nRequire cross-lingual retrieval and evaluate embedders using multilingual cases.\nPros / Cons\nOption A: English-only retrieval\nPros:\n\nPotentially smaller/faster embedding models.\n\nCons:\n\nFails for mixed-language backlog content.\nForces users to translate queries manually.\nMakes retrieval quality brittle as content language mix evolves.\n\nOption B: Cross-lingual retrieval required\nPros:\n\nMatches repository reality (mixed-language artifacts).\nMakes evaluation criteria explicit and repeatable.\n\nCons:\n\nMay increase model footprint/cost.\nRequires benchmark coverage for multilingual/cross-lingual cases.\n\nConsequences\n\nBenchmarks MUST include cross-lingual cases.\n\n\nThe benchmark harness (USR-0034) must include multilingual docs and cross-lingual queries.\n\n\nTelemetry MUST capture tokenizer behavior and truncation.\n\n\nToken inflation for CJK can reduce effective context windows.\nTelemetry must distinguish exact vs heuristic token counts (see tokenizer TokenCount).\n\n\nDefault embedder configuration is allowed to be “noop” for local tests.\n\n\nReal embedders remain optional dependencies.\nDecisions about a real default model should be benchmark-driven.\n\nFollow-ups\n\nKeep ADR-0036 aligned: per-model indexes are required for safe experimentation and rollback.\nUse kano-backlog benchmark run outputs under _kano/backlog/products/&lt;product&gt;/artifacts/KABSD-TSK-0261/runs/ as the evidence trail for model selection.\n"},"adr/ADR-0036_index-strategy-shared-index-now-per-model-indexes-later-via-config":{"title":"Index strategy: shared index now, per-model indexes later via config","links":[],"tags":[],"content":"Decision\nWe will use per-model indexes (per embedding space) as the default.\n\nA single “shared index” across different embedding models is not supported because vectors\nfrom different models are not generally comparable (dimension and semantic space differ).\n\nWe will keep an explicit configuration mechanism to switch index selection and to allow\ncontrolled “aliasing” (sharing) only when two model identifiers are proven to be the same\nembedding space (strict allowlist; no automatic inference).\nContext\nWe need a local-first semantic retrieval capability for backlog artifacts (items, ADRs, etc.).\r\nCross-lingual retrieval is required (see ADR-0035), which increases the likelihood that we\r\nwill evaluate and potentially switch between embedding models over time.\nThe index must remain:\n\nlocal-first (no server runtime required)\nrebuildable (derived from canonical Markdown)\ndeterministic enough for incremental rebuilds (chunk IDs + content hashes)\nconfigurable (so we can swap providers/models/backends without rewriting code)\n\nOptions Considered\nOption A: Single shared index across multiple embedding models\nStore all vectors in one ANN index, regardless of which model produced them.\nOption B: Per-model indexes (per embedding space) [chosen]\nMaintain separate ANN indexes keyed by an explicit “embedding space” identity.\nOption C: Shared index with automatic compatibility detection (future research)\nAttempt to infer which models are “compatible enough” to share one index by running\r\nstatistical tests (e.g., neighborhood agreement on a validation corpus).\nPros / Cons\nOption A: Single shared index across multiple embedding models\nPros:\n\nMinimal operational complexity (one index file)\n\nCons:\n\nGenerally invalid: vectors from different embedding models are not comparable\r\n(dimensions can differ; even with same dimension the spaces are different).\nProduces misleading similarity scores and unstable retrieval quality.\nMakes troubleshooting and benchmarking ambiguous.\n\nOption B: Per-model indexes (per embedding space)\nPros:\n\nCorrectness: avoids mixing incompatible vector spaces.\nOperational clarity: retrieval quality changes map to a single model+version.\nEnables safe experimentation: add a new index for a new model without corrupting the old one.\nSupports gradual rollout: switch default index via config, keep rollback path.\n\nCons:\n\nMore disk usage and rebuild time when multiple models are evaluated.\nRequires a routing key (index selection) to be part of pipeline config and metadata.\n\nOption C: Shared index with automatic compatibility detection\nPros:\n\nCould reduce the number of indexes in some cases.\n\nCons:\n\nHigh risk of false positives (appears compatible on a small corpus but fails in practice).\nAdds complexity and “magic” behavior that is hard to reason about.\nNeeds an evaluation harness and careful governance anyway.\n\nConsequences\n1) Define a stable embedding space identity\nIntroduce an embedding_space_id used for:\n\nselecting the vector index (routing)\nstoring metadata alongside chunks/vectors\npreventing accidental mixing\n\nDefinition (conceptual):\n\nembedding_space_id = sha256(provider_id + model_name + model_revision + dims + preprocessing_version + vector_norm + prompt_style_id)\n\nNotes:\n\n“model_revision” should include an immutable identifier when possible (HF revision hash, provider version).\npreprocessing_version covers normalization/prefixing decisions that affect embeddings.\nprompt_style_id matters for instruct-style embedders (query vs document templates).\n\n2) Index storage layout\nPersist one index per embedding_space_id (or per “model key” that maps to it), for example:\n\n.../vector_indexes/&lt;embedding_space_id&gt;/index.bin\n.../vector_indexes/&lt;embedding_space_id&gt;/mapping.sqlite (if needed)\n\n3) Config-driven routing and future “sharing” via allowlist aliasing\nConfiguration must allow:\n\nselecting the default embedder (and therefore default index)\nselecting a specific index key explicitly (for debugging/rollback)\ndefining alias mappings only when two identifiers are truly the same embedding space\n\nPolicy:\n\nDo not auto-merge indexes.\nIf aliasing is used, it must be explicit and reviewed (allowlist).\n\n4) Benchmark implications\nBenchmarks must report results per embedding_space_id so comparisons are reproducible.\nFollow-ups\n\nUpdate KABSD-USR-0035 to reference ADR-0036 as the index strategy baseline.\nEnsure KABSD-USR-0031 (telemetry) includes embedding_space_id and model revision in results.\nEnsure KABSD-USR-0034 (benchmark harness) compares multiple embedding_space_id runs.\nOptional (future): research Option C as a non-default experiment, producing an ADR addendum\r\nif the evidence supports safe aliasing beyond strict equality.\n\nOptions Considered\nPros / Cons\nConsequences\nFollow-ups"},"adr/ADR-0037_inspector-pattern-and-query-surface-architecture":{"title":"Inspector Pattern and Query Surface Architecture","links":["KABSD-EPIC-0011","KABSD-FTR-0055","KABSD-FTR-0056","ADR-0013","ADR-0004"],"tags":[],"content":"Decision\nThe kano-agent-backlog-skill will adopt an “Inspector Pattern” architecture where:\n\nSkill Core = Query Surface: Provides deterministic, evidence-based data extraction APIs\nExternal Agents = Inspectors: Consume query surface, produce conclusions with evidence trails\nEvidence-First: Every conclusion must cite traceable sources (file paths, line ranges, IDs)\nRead-Only Contract: Inspectors query canonical SoT, never write to it directly\n\nThis means:\n\nAll “expert judgment” (health assessment, review, refactor suggestions) lives in external agents\nCore skill provides only deterministic data + derived artifacts (audit, snapshot, constellation, indexes)\nInspector agents are replaceable (any agent can implement the contract)\nAll inspector outputs must include evidence attachments (no unsourced claims)\n\nContext\nProblem Statement\nOrigin: GPT-5.2 feedback on backlog discipline and architecture (2026-01-22)\nCurrent risk: encoding “judgment” (health assessment, review suggestions, refactor recommendations) into core skill logic creates:\n\nGoodhart’s Law risk: Metrics become targets, lose meaning when hardcoded\nTight coupling: Expert logic hardcoded into skill core, hard to extend/replace\nLimited extensibility: Can’t swap assessment strategies without modifying core\nInconsistent evidence: Conclusions without traceable sources\n\nKey Insight from GPT-5.2\n\n“Backlog skill’s responsibility: provide all tools for agents to reliably acquire information, not hardcode judgment into core.”\n\nAnalogy:\n\nSkill = Database with query API\nInspector Agents = Analytics tools that consume the database\nEvidence = Structured citations (table/row/column refs)\n\nCurrent State\nWhat exists:\n\nAudit primitives (rule checking, gap detection)\nSnapshot generation (state summaries)\nConstellation (relationship graphs)\nSQLite index for fast queries\n\nWhat’s missing:\n\nFormal Inspector Pattern contract\nUnified query surface with JSON output\nEvidence attachment standards\nReference inspector implementation\n\nRelated Work\n\n3+3 questions (health/ideas assessment): Proposed feature, now reframed as inspector agent\nDecision audit/write-back: Implemented ad-hoc, needs formalization\nExternal agent integration: No contract defined\n\nArchitecture\nPrinciple A: Core = Data + Derived Artifacts (Deterministic)\nCore Responsibilities (all deterministic, repeatable):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComponentPurposeOutputaudit.run()Rules enforcement (Ready gate, schema validation)Findings with IDs, categories, severitysnapshot.build()Current state extraction (item counts, distributions)Timestamped state summaryconstellation.build()Relationship graph (parent/child, blocks, relates)Graph with nodes, edges, metadataindexSearch acceleration (FTS, embedding, graph)Query results with relevance scores\nKey: No “expert opinion” - only facts derived from canonical files.\nPrinciple B: External Agents = Inspectors (Replaceable)\nInspector Types (all external to core):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInspectorPurposeExample OutputHealth/Ideas3+3 questions, gap analysis, anti-patterns”5 items missing Context field (AF-001, AF-002…)”ReviewerCode review suggestions, best practices”Consider extracting common logic (evidence: L45-67)“ArchitectRefactoring recommendations, design improvements”Detected circular dependency (items: X, Y, Z)“SecurityThreat model, vulnerability assessment”Exposed secrets in item TASK-042 (file: …, L25)”\nKey: These are separate processes/agents, not core modules.\nPrinciple C: Evidence = First-Class Citizen\nCore Principle: Every inspector finding must cite traceable evidence. Without evidence, conclusions are rejected or downgraded.\nEvidence Quality: Five Axes (Critical Thinking Foundation)\nBased on critical thinking principles, evidence quality must be evaluated across five axes:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAxisDefinitionSystem ApplicationRelevanceEvidence must directly support the claim, not just appear relatedclaim_id links evidence to specific findingReliabilitySource is real, traceable, verifiable (not “jargon credentialism”)provenance field: hash, commit, URL, timestamp, authorSufficiencyOne example ≠ universal rule (avoid survivor bias, no control group)coverage field: sample range, counter-examples, failure statsVerifiabilityOthers can check, repeat, measure the claimverification field: reproducible command, test, or check stepsIndependenceEvidence doesn’t collude with conclusion (avoid self-citation, astroturfing)independence field: source type, conflict-of-interest flags\nWhy This Matters:\n\nBayes’ prior problem: Math looks objective, but where priors come from is the landmine; priors can be manipulated → conclusions can be steered\nPeer review ideal vs reality: The idea is to shift credibility from individuals to community validation, but institutions develop interest chains, conservatism within paradigms\nTrust should be in continuous questioning + verifiable judgment, not fixed standards\n\nEvidence Schema (Extended with 5-Axes)\n@dataclass\nclass EvidenceRecord:\n    &quot;&quot;&quot;Full evidence record with quality metadata.&quot;&quot;&quot;\n    \n    # Core identity (existing)\n    type: str                 # &quot;item&quot;, &quot;adr&quot;, &quot;file&quot;, &quot;audit_finding&quot;, &quot;commit&quot;, &quot;log&quot;\n    id: str                   # Item/ADR ID or finding ID\n    file_path: str            # Relative path from backlog root\n    line_range: Optional[Tuple[int, int]] = None\n    excerpt: Optional[str] = None\n    timestamp: Optional[str] = None\n    \n    # Relevance (5-axis 1)\n    claim_id: Optional[str] = None       # Which claim/decision this supports\n    support_type: Optional[str] = None   # &quot;direct&quot;, &quot;indirect&quot;, &quot;counterpoint&quot;\n    \n    # Reliability (5-axis 2)\n    source_type: str = &quot;unknown&quot;         # &quot;repo_path&quot;, &quot;issue&quot;, &quot;pr&quot;, &quot;chat&quot;, &quot;doc&quot;, &quot;web&quot;, &quot;experiment&quot;\n    provenance: Optional[Dict] = None    # {&quot;hash&quot;: &quot;...&quot;, &quot;commit&quot;: &quot;...&quot;, &quot;url&quot;: &quot;...&quot;, &quot;author&quot;: &quot;...&quot;}\n    \n    # Sufficiency (5-axis 3)\n    coverage: Optional[Dict] = None      # {&quot;sample_size&quot;: N, &quot;has_counterexamples&quot;: bool, &quot;has_failure_stats&quot;: bool}\n    \n    # Verifiability (5-axis 4)\n    verification: Optional[Dict] = None  # {&quot;method&quot;: &quot;command|test|manual&quot;, &quot;steps&quot;: [...], &quot;reproducible&quot;: bool}\n    \n    # Independence (5-axis 5)\n    independence: Optional[Dict] = None  # {&quot;self_cited&quot;: bool, &quot;conflict_of_interest&quot;: bool, &quot;same_source_chain&quot;: bool}\n    \n    # Meta\n    confidence: Optional[float] = None   # 0.0-1.0, with explicit reasoning\n    confidence_reason: Optional[str] = None\nMinimum Evidence Requirements by Context\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContextRequired FieldsRecommended FieldsWorkset materialtype, id, file_path, source_typeprovenance, verificationInspector findingtype, id, file_path, line_range, claim_idcoverage, independenceADR decision supporttype, id, file_path, claim_id, verificationcoverage, independence, confidenceHealth reviewtype, id, source_type, independencecoverage, sufficiency analysis\nAnti-Patterns (Evidence Red Flags)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRed FlagDetectionRiskSingle source dependencyAll evidence from one file/authorLow reliability, no cross-validationSame-source masqueradeMultiple “evidence” items from same originFake sufficiencyMissing counterexamplesNo failure cases, only success storiesSurvivor biasNon-reproducible”Trust me” without verification stepsUnverifiable claimsSelf-citation loopAuthor cites own previous work exclusivelyIndependence violationJargon credentialismClaims backed by terminology, not dataReliability theater\nEvery inspector output MUST include evidence records:\n{\n  &quot;finding_id&quot;: &quot;F-001&quot;,\n  &quot;category&quot;: &quot;health&quot;,\n  &quot;assessment&quot;: &quot;Item missing required field&quot;,\n  &quot;evidence&quot;: [\n    {\n      &quot;type&quot;: &quot;item&quot;,\n      &quot;item_id&quot;: &quot;KABSD-TSK-0042&quot;,\n      &quot;file&quot;: &quot;_kano/backlog/items/task/0000/KABSD-TSK-0042.md&quot;,\n      &quot;line_range&quot;: [25, 30],\n      &quot;field&quot;: &quot;Context&quot;,\n      &quot;issue&quot;: &quot;Empty or missing&quot;\n    }\n  ],\n  &quot;timestamp&quot;: &quot;2026-01-22T08:51:00Z&quot;,\n  &quot;agent&quot;: &quot;health-inspector-v1&quot;\n}\nNo evidence = rejected or downgraded.\nInspector Agent Contract\nQuery Surface API (Existing + Planned)\nExisting APIs (already implemented):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOps FunctionCLI CommandJSON SupportNotessnapshot.generate_pack()kano-backlog snapshot✅ pack.to_json()Returns EvidencePack with stubs, capabilities, healthworkitem.list_items()kano-backlog item list✅ --format jsonFilters: type, state, parent, tagsworkitem.get_item()kano-backlog workitem read✅ --format jsonSingle item with full metadataworkitem.validate_ready()kano-backlog workitem validate✅ --format jsonReady gate validationtopic.decision_audit()kano-backlog topic decision-audit✅ --format jsonDecision write-back audittopic.export_context()kano-backlog topic export-context✅ --format jsonTopic context bundle\nPlanned APIs (to implement in KABSD-FTR-0055):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOps FunctionCLI CommandStatusNotesrelease_check.run_phase1/2()kano-backlog release check⚠️ Markdown onlyNeed --format jsonVarious health checkskano-backlog doctor⚠️ Plain textNeed --format jsonconstellation.build()kano-backlog constellation build❌ MissingRelationship graphdoc_resolve.resolve()kano-backlog workitem resolve❌ MissingStructured excerpts\nEvidence Schema (to standardize across all APIs):\nSee “Principle C: Evidence Quality Five Axes” above for the extended EvidenceRecord schema with 5-axis quality metadata.\nMinimal Evidence schema (for backward compatibility):\n@dataclass\nclass Evidence:\n    &quot;&quot;&quot;Minimal traceable source for inspector findings.&quot;&quot;&quot;\n    type: str           # &quot;item&quot;, &quot;adr&quot;, &quot;file&quot;, &quot;audit_finding&quot;\n    id: str             # Item/ADR ID or finding ID\n    file_path: str      # Relative path from backlog root\n    line_range: Optional[Tuple[int, int]] = None  # Start, end lines\n    excerpt: Optional[str] = None  # Text snippet\n    timestamp: Optional[str] = None\nAudit vs Health: Distinction\nBoth use the same query surface and evidence format, but differ in what they check:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAspectAuditHealth ReviewFocusConsistency / ConformanceCredibility / Risk / GapsQuestions”Does X conform to rule Y?&quot;&quot;Can we trust X? What are the risks?”ExamplesNaming conventions, directory structure, schema drift, deterministic pipeline completenessSingle-source dependency, survivor bias, unverifiable claims, conflicted evidenceOutputViolation list (pass/fail per rule)Risk and trust gap report (with severity)TriggerCI gate, pre-release check, schema migrationManual request, agent stuck, decision review\nAudit checks for “rule violations”:\n\nMissing required fields\nWrong directory structure\nSchema mismatch\nDeterministic pipeline gaps\n\nHealth checks for “trust gaps”:\n\nEvidence quality degradation (5-axis failures)\nSingle-narrative dependency\nMissing counterexamples\nUnstated assumptions (priors)\n\nInspector Output Contract\nStandard output schema:\n{\n  &quot;inspector&quot;: &quot;health-ideas-v1&quot;,\n  &quot;agent&quot;: &quot;antigravity&quot;,\n  &quot;timestamp&quot;: &quot;2026-01-22T08:51:00Z&quot;,\n  &quot;query_params&quot;: {\n    &quot;window&quot;: &quot;7d&quot;,\n    &quot;filters&quot;: {}\n  },\n  &quot;findings&quot;: [\n    {\n      &quot;finding_id&quot;: &quot;F-001&quot;,\n      &quot;category&quot;: &quot;health&quot;,\n      &quot;question&quot;: &quot;Are items well-formed?&quot;,\n      &quot;assessment&quot;: &quot;5 tasks missing Context field&quot;,\n      &quot;severity&quot;: &quot;warning&quot;,\n      &quot;evidence&quot;: [\n        {\n          &quot;type&quot;: &quot;audit_finding&quot;,\n          &quot;audit_finding_id&quot;: &quot;AF-001&quot;,\n          &quot;item_id&quot;: &quot;KABSD-TSK-0042&quot;,\n          &quot;file&quot;: &quot;_kano/backlog/items/task/0000/KABSD-TSK-0042.md&quot;,\n          &quot;line_range&quot;: [25, 30],\n          &quot;field&quot;: &quot;Context&quot;,\n          &quot;issue&quot;: &quot;Empty&quot;\n        }\n      ]\n    }\n  ],\n  &quot;summary&quot;: {\n    &quot;total_findings&quot;: 12,\n    &quot;by_severity&quot;: {&quot;error&quot;: 0, &quot;warning&quot;: 5, &quot;info&quot;: 7}\n  }\n}\nRequired fields:\n\ninspector: Inspector identity (name + version)\nagent: Which agent ran the inspector\ntimestamp: When inspection occurred\nfindings: Array of findings, each with evidence\nevidence: Array of traceable sources with file paths + line ranges\n\nIntegration Patterns\nPattern 1: Manual Invocation\n# Human asks agent to run inspector\nUser: &quot;Check backlog health and show me the report&quot;\n \n# Agent executes\n$ kano-backlog query snapshot --format json &gt; snapshot.json\n$ health-inspector --input snapshot.json --output report.json\n$ cat report.json\nPattern 2: CI Integration\n# .github/workflows/backlog-health.yml\n- name: Check backlog health\n  run: |\n    kano-backlog query snapshot --format json &gt; snapshot.json\n    health-inspector --input snapshot.json --output report.json\n    if grep -q &#039;&quot;severity&quot;: &quot;error&quot;&#039; report.json; then exit 1; fi\nPattern 3: Agent Self-Assessment\nAgent: &quot;I&#039;m stuck. Let me consult the inspector...&quot;\r\nAgent: &lt;runs inspector, gets findings&gt;\r\nAgent: &quot;Inspector found 3 items blocking my work (evidence: ...)&quot;\n\nWhere Inspectors Live\nOptions:\n\n\nSeparate skill: kano-inspector-health-ideas-skill/ (recommended)\n\nPro: Replaceable, versionable, independent lifecycle\nCon: Extra installation step\n\n\n\nReference implementation in core skill: examples/inspectors/health.py\n\nPro: Batteries-included for demos\nCon: Risk of coupling if not disciplined\n\n\n\nExternal repository: Community-maintained inspectors\n\nPro: Maximum flexibility\nCon: Discovery problem\n\n\n\nRecommendation: Start with (2) for reference, encourage (1) for production use.\nConsequences\nFor Skill Developers\n\n\nMust separate data from judgment:\n\n✅ Good: audit.run() returns list of “missing Context field” findings\n❌ Bad: audit.run() returns “backlog health is poor” conclusion\n\n\n\nMust provide evidence attachments:\n\nAll query APIs return structured data with file paths, line ranges, IDs\nNo APIs that return “summary strings” without evidence\n\n\n\nMust document query surface:\n\nInspector contract is a public API, needs versioning and docs\n\n\n\nFor Inspector Agents\n\n\nMust consume query surface, not parse files directly:\n\n✅ Good: Call workitem.query(filters={...}) API\n❌ Bad: Parse _kano/backlog/items/**/*.md directly\n\n\n\nMust attach evidence to all findings:\n\nEvery conclusion cites file path + line range (or stable anchor)\nNo “I think X” without “because I saw Y at Z”\n\n\n\nAre replaceable:\n\nAny agent can implement inspector contract\nMultiple inspectors can coexist (health, review, security, etc)\n\n\n\nFor End Users (Humans)\n\n\nInspector frequency is use-case specific:\n\nNOT “run daily” (avoid Goodhart’s Law)\nRun when: manual request, CI gate, agent stuck, pre-release audit\n\n\n\nInspector outputs are recommendations, not commands:\n\nHuman decides which findings to act on\nAgent may propose fixes but doesn’t auto-apply\n\n\n\nEvidence trail enables trust:\n\nEvery finding cites sources\nHuman can verify inspector claims\n\n\n\nMigration Strategy\nPhase 1: Define Contract (ADR + Documentation)\n\n Create this ADR\n Document query surface API spec\n Document inspector output schema\n Update ADR-0013 (Module Boundaries) with inspector pattern section\n\nPhase 2: Implement Query Surface (KABSD-FTR-0055)\n\n Add JSON output to existing audit/snapshot/constellation\n Implement workitem.query API\n Implement doc.resolve API\n Implement export.bundle API\n Add CLI commands under kano-backlog query\n\nPhase 3: Reference Inspector (KABSD-FTR-0056)\n\n Build health/ideas inspector as reference implementation\n Validate inspector contract through real usage\n Document integration patterns\n\nPhase 4: Documentation &amp; Tooling\n\n Update AGENTS.md with inspector pattern guidance\n Add inspector examples to SKILL.md\n Create inspector scaffolding tool (optional)\n\nAlternatives Considered\nAlternative A: Keep Assessment Logic in Core\nApproach: Encode health checks, review logic, etc directly in skill core.\nRejected because:\n\nCreates tight coupling (hard to extend/replace)\nGoodhart’s Law risk (metrics become targets when hardcoded)\nNo separation of concerns (data vs judgment)\n\nAlternative B: Build Generic “LLM Judge” Framework\nApproach: Create abstract framework for LLM-based assessment.\nDeferred because:\n\nToo abstract for initial implementation\nStart concrete (inspector contract), generalize later if needed\nRisk of over-engineering\n\nAlternative C: Use External Tool (Jira, Linear, etc)\nApproach: Export to external PM tool, use their analytics/dashboards.\nRejected because:\n\nViolates local-first principle\nExternal tools can consume inspector outputs via adapters (future work)\nWe control the query surface, not the external tool\n\nRelated\n\nKABSD-EPIC-0011: Inspector Pattern: External Agent Query Surface (parent epic)\nKABSD-FTR-0055: Query Surface API Implementation\nKABSD-FTR-0056: Inspector Agent Reference Implementation\nADR-0013: Codebase Architecture and Module Boundaries (will add inspector pattern section)\nADR-0004: File-First Architecture with SQLite Index (complements this ADR)\n\nStatus\nAccepted (2026-01-22)\nThis ADR establishes the architectural direction. Implementation will occur in phases via linked feature work items."},"adr/README":{"title":"README","links":[],"tags":[],"content":"_kano/backlog/decisions\nStore ADRs here. Link ADR IDs in work items via the decisions field and\r\nappend a Worklog entry when a decision affects scope or approach."},"adr/index":{"title":"Architecture Decisions","links":["adr/ADR-0001_backlog-structure-and-moc","adr/ADR-0002_decisions-as-adr-links","adr/ADR-0003-appendix_collision-report-cli-spec","adr/ADR-0003-appendix_id-resolver-spec","adr/ADR-0003-appendix_migration-plan-uid","adr/ADR-0003-appendix_ulid-vs-uuidv7-comparison","adr/ADR-0003_identifier-strategy-for-local-first-backlog","adr/ADR-0004_file-first-architecture-with-sqlite-index","adr/ADR-0004_per-product-isolated-index-architecture","adr/ADR-0005_product-column-retention-rationale","adr/ADR-0005_skill-versioning-and-release-policy","adr/ADR-0006_multi-product-directory-structure","adr/ADR-0007_vcs-as-source-of-truth-derived-commit-data","adr/ADR-0008_sqlite-schema-migration-framework","adr/ADR-0009_local-first-embedding-search-architecture","adr/ADR-0010_project-babylon-global-scale-collaboration-vision","adr/ADR-0011_graph-assisted-retrieval-and-context-graph","adr/ADR-0011_workset-graphrag-context-graph-separation-of-responsibilities","adr/ADR-0012_workset-db-canonical-schema-reuse","adr/ADR-0013_codebase-architecture-and-module-boundaries","adr/ADR-0014_plugin-and-hook-system-architecture","adr/ADR-0015_decision-for-kabsd-tsk-0056","adr/ADR-0015_skill-scoped-cli-namespace-convention","adr/ADR-0016_per-product-isolated-index-architecture","adr/ADR-0017_product-column-retention-rationale","adr/ADR-0018_decision-for-kabsd-tsk-0056","adr/ADR-0019_decision-for-kabsd-tsk-0056","adr/ADR-0020_decision-for-kabsd-tsk-0056","adr/ADR-0021_per-product-isolated-index-architecture","adr/ADR-0022_product-column-retention-rationale","adr/ADR-0023_graph-assisted-retrieval-and-context-graph","adr/ADR-0024_decision-for-kabsd-tsk-0056","adr/ADR-0025_decision-for-kabsd-tsk-0056","adr/ADR-0026_decision-for-kabsd-tsk-0056","adr/ADR-0027_decision-for-kabsd-tsk-0056","adr/ADR-0028_per-product-isolated-index-architecture","adr/ADR-0029_product-column-retention-rationale","adr/ADR-0030_graph-assisted-retrieval-and-context-graph","adr/ADR-0031_per-product-isolated-index-architecture","adr/ADR-0032_per-product-isolated-index-architecture","adr/ADR-0033_decision-for-kabsd-tsk-0056","adr/ADR-0034_conflict-handling-policy-for-duplicate-ids-and-uids","adr/ADR-0035_cross-lingual-retrieval-requirement-and-default-embedding-policy","adr/ADR-0036_index-strategy-shared-index-now-per-model-indexes-later-via-config","adr/ADR-0037_inspector-pattern-and-query-surface-architecture","adr/README"],"tags":[],"content":"Architecture Decisions\nArchitecture Decision Records and design rationale\n\nADR-0001_backlog-structure-and-moc\nADR-0002_decisions-as-adr-links\nADR-0003-appendix_collision-report-cli-spec\nADR-0003-appendix_id-resolver-spec\nADR-0003-appendix_migration-plan-uid\nADR-0003-appendix_ulid-vs-uuidv7-comparison\nADR-0003_identifier-strategy-for-local-first-backlog\nADR-0004_file-first-architecture-with-sqlite-index\nADR-0004_per-product-isolated-index-architecture\nADR-0005_product-column-retention-rationale\nADR-0005_skill-versioning-and-release-policy\nADR-0006_multi-product-directory-structure\nADR-0007_vcs-as-source-of-truth-derived-commit-data\nADR-0008_sqlite-schema-migration-framework\nADR-0009_local-first-embedding-search-architecture\nADR-0010_project-babylon-global-scale-collaboration-vision\nADR-0011_graph-assisted-retrieval-and-context-graph\nADR-0011_workset-graphrag-context-graph-separation-of-responsibilities\nADR-0012_workset-db-canonical-schema-reuse\nADR-0013_codebase-architecture-and-module-boundaries\nADR-0014_plugin-and-hook-system-architecture\nADR-0015_decision-for-kabsd-tsk-0056\nADR-0015_skill-scoped-cli-namespace-convention\nADR-0016_per-product-isolated-index-architecture\nADR-0017_product-column-retention-rationale\nADR-0018_decision-for-kabsd-tsk-0056\nADR-0019_decision-for-kabsd-tsk-0056\nADR-0020_decision-for-kabsd-tsk-0056\nADR-0021_per-product-isolated-index-architecture\nADR-0022_product-column-retention-rationale\nADR-0023_graph-assisted-retrieval-and-context-graph\nADR-0024_decision-for-kabsd-tsk-0056\nADR-0025_decision-for-kabsd-tsk-0056\nADR-0026_decision-for-kabsd-tsk-0056\nADR-0027_decision-for-kabsd-tsk-0056\nADR-0028_per-product-isolated-index-architecture\nADR-0029_product-column-retention-rationale\nADR-0030_graph-assisted-retrieval-and-context-graph\nADR-0031_per-product-isolated-index-architecture\nADR-0032_per-product-isolated-index-architecture\nADR-0033_decision-for-kabsd-tsk-0056\nADR-0034_conflict-handling-policy-for-duplicate-ids-and-uids\nADR-0035_cross-lingual-retrieval-requirement-and-default-embedding-policy\nADR-0036_index-strategy-shared-index-now-per-model-indexes-later-via-config\nADR-0037_inspector-pattern-and-query-surface-architecture\nREADME\n"},"cli/index":{"title":"index","links":[],"tags":[],"content":"CLI Reference\nkano-backlog\n                                                                               \r\n Usage: kano-backlog [OPTIONS] COMMAND [ARGS]...                               \r\n                                                                               \r\n kano-backlog: Backlog management CLI (MVP)                                    \r\n                                                                               \r\n+- Options -------------------------------------------------------------------+\r\n| --install-completion          Install completion for the current shell.     |\r\n| --show-completion             Show completion for the current shell, to     |\r\n|                               copy it or customize the installation.        |\r\n| --help                        Show this message and exit.                   |\r\n+-----------------------------------------------------------------------------+\r\n+- Commands ------------------------------------------------------------------+\r\n| doctor      Check environment health.                                       |\r\n| admin       Administrative and setup commands                               |\r\n| workitem    Work item operations                                            |\r\n| item        Work item operations (alias)                                    |\r\n| state       State transitions                                               |\r\n| worklog     Worklog operations                                              |\r\n| view        View and dashboard operations                                   |\r\n| snapshot    Snapshot and evidence operations                                |\r\n| workset     Workset cache operations                                        |\r\n| topic       Topic context operations                                        |\r\n| config      Config inspection and validation                                |\r\n| changelog   Changelog generation from backlog                               |\r\n| benchmark   Deterministic benchmark harness                                 |\r\n| embedding   Embedding pipeline operations                                   |\r\n| search      Vector similarity search                                        |\r\n| tokenizer   Tokenizer adapter configuration, testing, and diagnostics       |\r\n+-----------------------------------------------------------------------------+\r\n\n"},"demo/agents":{"title":"agents","links":["ADR-0037"],"tags":[],"content":"AGENTS\nExternal File Loading\nCRITICAL: When you encounter a file reference (e.g., @rules/general.md, @docs/architecture.md), use your Read tool to load it on a need-to-know basis. These references are relevant to the SPECIFIC task at hand.\nInstructions\n\nDo NOT preemptively load all references - use lazy loading based on actual need\nWhen loaded, treat content as mandatory instructions that override defaults\nFollow references recursively when the loaded file contains additional @ references\nPriority: External file content &gt; AGENTS.md defaults &gt; built-in instructions\n\nExample Usage\nUser mentions: &quot;Follow @rules/authentication.md for this task&quot;\r\nAgent: Reads @rules/authentication.md, applies its rules for this task only\n\n\nRepo purpose\nThis repo is a demo showing how to use kano-agent-backlog-skill to turn agent collaboration\r\ninto a durable, local-first backlog with an auditable decision trail (instead of losing context in chat).\nConversational-first documentation (human-agent collaboration)\nThis project’s primary value is human + AI collaboration, not just a CLI.\r\nTherefore, when writing or updating documentation (README, docs, SKILL.md, process notes), always include\r\ninstructions for how to drive the workflow through a conversation with an AI agent, not only how to run commands.\nRules:\n\nEvery workflow doc should contain both:\n\nCLI commands (for deterministic, auditable execution), and\nSuggested chat prompts (copy/paste) that a human can say to an agent.\n\n\nPrompts must be specific about inputs the agent needs: topic/item IDs, product, agent identity, expected outputs.\nDocument the expected artifacts and paths the agent will produce/update (e.g., reports under topic/publish/).\nPrefer a consistent pattern in docs:\n\n“Say this to your agent”\n“The agent will do” (explicit steps)\n“Expected output” (files/paths + how to verify)\n\n\n\nExample (decision audit + decision write-back):\n\nSay to agent: “Run a decision write-back audit for topic  and show me which work items are missing decisions.”\nAgent runs: kano topic decision-audit &lt;topic-name&gt; --format plain\nExpected output: _kano/backlog/topics/&lt;topic-name&gt;/publish/decision-audit.md\nSay to agent: “Write back this decision to &lt;ITEM_ID&gt; and include the synthesis file as source.”\nAgent runs: kano workitem add-decision &lt;ITEM_ID&gt; --decision &quot;...&quot; --source &quot;...&quot; --agent &lt;agent-id&gt; --product &lt;product&gt;\nExpected output: updated work item with a ## Decisions section + appended Worklog entry.\n\nAgent roster (from README.md)\n\nCodex\nGitHub Copilot\nGoogle Antigravity\nAmazon Q\nAmazon Kiro\nCursor\nWindsurf\nOpenCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgentPrimary Model(s)Alternative ModelsNotesCodexCodex-Max (Nov 2025)GPT-5.2-Codex, o1/o3-reasoningOpenAI’s dedicated line for software engineering and multi-file logicGitHub CopilotGPT-5.2 (Dec 2025)Claude 4.5, Gemini 3 Pro, GPT-5.1Multi-model picker; GPT-5.2 Pro available for advanced research/reasoningGoogle AntigravityGemini 3 Flash (Dec 2025)Gemini 3 Pro, Gemini 1.5 Pro (legacy)Optimized for low-latency planning and multimodal workspace reasoningAmazon QClaude 4 Sonnet (May 2025)Claude 4.5 Sonnet, Amazon NovaEnhanced for autonomous computer use and deep codebase integrationAmazon KiroAuto-Routing (Dynamic)Claude 4.5 (Opus/Sonnet/Haiku)Intelligent routing across Claude 4 family; ~23% cheaper than direct Opus 4.5 useCursorClaude 3.5 Sonnet, GPT-5cursor-small (V2), GPT-4.1 MiniStable support for 2024/2025 frontier models; high-freq codebase indexingWindsurfSWE-1.5 (Proprietary)Claude 4, GPT-5.1, BYOKSWE-1.5 is a frontier coding model with performance near Claude 4.5OpenCodeMulti-ProviderDeepSeek V3.2, Llama 4 Scout, Mistral Large 3Open-source champion; supports 75+ providers including regional models\nDevelopment Guidelines\nDerived Data and .gitignore Rules\nCRITICAL: Always exclude derived/generated data from version control to keep repositories clean and efficient.\nWhat to Exclude\nGenerated/Derived Data (always add to .gitignore):\n\nCache directories: .cache/, __pycache__/, .pytest_cache/\nBuild artifacts: dist/, build/, *.egg-info/\nTest outputs: htmlcov/, .coverage, .hypothesis/\nVector databases: .cache/vector/, *.sqlite3 (embedding indexes)\nCompiled files: *.pyc, *.pyo, *.so\nIDE files: .vscode/, .idea/, *.swp\nOS files: .DS_Store, Thumbs.db\nEnvironment files: .env, .venv/, venv/\n\nWhat to Include\nSource of Truth (always version control):\n\nSource code: src/, tests/, scripts\nConfiguration templates: config.toml.example, default configs\nDocumentation: README.md, references/, SKILL.md\nSchema definitions: data models, API specs\nTest fixtures: static test data, benchmark corpus\n\nImplementation Rules\n\n\nBefore adding any new feature that generates data:\n\nIdentify what files/directories will be created\nAdd appropriate .gitignore entries immediately\nDocument the regeneration process\n\n\n\nCommon patterns to exclude:\n# Caches and derived data\n.cache/\n__pycache__/\n*.pyc\n \n# Build outputs\ndist/\nbuild/\n*.egg-info/\n \n# Test artifacts\n.pytest_cache/\n.coverage\nhtmlcov/\n \n# Vector/embedding indexes\n.cache/vector/\n*.sqlite3\n \n# IDE and OS\n.vscode/\n.DS_Store\n\n\nDocumentation requirement:\n\nAlways document how to regenerate excluded data\nInclude regeneration steps in README or setup docs\nProvide example commands for rebuilding indexes/caches\n\n\n\nRationale\n\nRepository size: Derived data can be large and grows over time\nMerge conflicts: Generated files often cause unnecessary conflicts\nEnvironment differences: Derived data may be platform/environment specific\nReproducibility: Source code should be sufficient to recreate all derived data\nSecurity: Avoid accidentally committing sensitive generated data\n\nQuick Start Commands\n# Install dependencies (dev mode)\npython -m pip install -e skills/kano-agent-backlog-skill[dev]\n \n# Run a specific test\npython -m pytest tests/test_chunking_mvp.py -v\n \n# Run all tests\npython -m pytest tests/ -v\n \n# Run tests with coverage\npython -m pytest tests/ --cov=skills/kano-agent-backlog-skill/src --cov-report=html\n \n# Format code\nblack skills/kano-agent-backlog-skill/src tests/\n \n# Sort imports\nisort skills/kano-agent-backlog-skill/src tests/\n \n# Check types\nmypy skills/kano-agent-backlog-skill/src\n \n# Run all linting\nblack skills/kano-agent-backlog-skill/src tests/ &amp;&amp; \\\nisort skills/kano-agent-backlog-skill/src tests/ &amp;&amp; \\\nmypy skills/kano-agent-backlog-skill/src\nCode Style Guidelines\nType Hints\nAlways use type hints from typing module: List, Dict, Optional, Any, Union, Tuple.\nExample:\nfrom typing import List, Optional, Dict\n \ndef process_items(items: List[str]) -&gt; Dict[str, Any]:\n    &quot;&quot;&quot;Process items and return a dictionary.&quot;&quot;&quot;\n    result: Dict[str, Any] = {}\n    for item in items:\n        result[item] = len(item)\n    return result\nImport Conventions\n\nOrder: standard library → third-party → local modules\nUse absolute imports from skill packages: from kano_backlog_core import ...\nUse relative imports within packages: from .models import ...\n\nExample:\nimport json  # Standard library\nfrom pathlib import Path  # Standard library\nfrom frontmatter import load  # Third-party\nfrom kano_backlog_core import BacklogItem  # Absolute from skill root\nfrom .models import ItemState  # Relative within package\nNaming Conventions\n\nClasses: PascalCase (e.g., CanonicalStore, ChunkingOptions)\nFunctions: snake_case (e.g., read_item, validate_config)\nConstants: UPPER_SNAKE_CASE (e.g., TYPE_DIRNAMES)\nPrivate members: leading underscore _private_var, __dunder__\nModules: snake_case (e.g., kano_backlog_core, token_counter)\n\nFormatting Rules\n\nLine length: 88 characters (Black default)\nIndentation: 4 spaces\nNo trailing whitespace\nDocstrings: triple double quotes &quot;&quot;&quot;, Google style\nType hints: after function definition, before docstring\n\nExample:\ndef process_data(data: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n    &quot;&quot;&quot;Process data and return results.\n \n    Args:\n        data: Input data to process.\n \n    Returns:\n        Dictionary of processed results.\n    &quot;&quot;&quot;\n    result: {}\n    for item in data:\n        result[item[&quot;id&quot;]] = item[&quot;value&quot;]\n    return result\nError Handling\nUse exceptions from kano_backlog_core/errors.py:\n\nItemNotFoundError - Item file doesn’t exist\nParseError - Invalid frontmatter or markdown\nValidationError - Data validation failed\nConfigError - Configuration error\nWriteError - Write operation failed\n\nExample:\nfrom kano_backlog_core.errors import ItemNotFoundError\n \ndef load_item(item_path: Path) -&gt; BacklogItem:\n    if not item_path.exists():\n        raise ItemNotFoundError(item_path, f&quot;Item file not found: {item_path}&quot;)\nDocstring Conventions\n\nTriple double quotes &quot;&quot;&quot;\nGoogle Style Guide format\nInclude Args:, Returns:, and Raises: sections\nImperative mood: “Return”, not “Returns”\n\nExample:\ndef create_item(title: str) -&gt; BacklogItem:\n    &quot;&quot;&quot;Create a new backlog item.\n \n    Args:\n        title: The title of the item.\n \n    Returns:\n        The created BacklogItem object.\n    &quot;&quot;&quot;\n    return BacklogItem(title=title)\nTesting Guidelines\nTest Structure\n\nUse pytest as test runner\nPlace tests in tests/ directory\nTest filename: test_{module_name}.py\nUse Hypothesis for property-based testing\n\nProperty-Based Testing with Hypothesis\nfrom hypothesis import given, strategies as st, composite\nfrom kano_backlog_core import ChunkingOptions\n \n@composite\ndef valid_config_strategy(draw):\n    &quot;&quot;&quot;Generate valid config instances.&quot;&quot;&quot;\n    return ChunkingOptions(\n        target_tokens=draw(st.integers(min_value=50, max_value=2048)),\n        max_tokens=draw(st.integers(min_value=512, max_value=4096))\n    )\n \n@given(valid_config_strategy())\ndef test_valid_config_passes(config: ChunkingOptions):\n    &quot;&quot;&quot;Valid config should pass validation.&quot;&quot;&quot;\n    assert config.target_tokens &lt;= config.max_tokens\nUsing the Kano Backlog Skill\nBefore Making Changes\n\nCheck existing items:\n\npython -m kano_backlog_cli.main item list --product kano-agent-backlog-skill\n\nCreate or update work items before coding:\n\npython -m kano_backlog_cli.main item create \\\n    --type task \\\n    --title &quot;Implement X feature&quot; \\\n    --product kano-agent-backlog-skill \\\n    --agent &lt;your-agent-id&gt;\n\nEnforce the Ready gate on Task/Bug items:\n\n\nRequired fields: Context, Goal, Approach, Acceptance Criteria, Risks / Dependencies\nAll fields must be non-empty and written in English only\n\n\nUpdate item state when starting work:\n\npython -m kano_backlog_cli.main item update-state \\\n    --id KABSD-TSK-0146 \\\n    --state InProgress \\\n    --agent &lt;your-agent-id&gt;\nWorklog Discipline\nWorklog is append-only. Append when:\n\nA load-bearing decision is made\nAn item state changes\nScope/approach changes\nAn ADR is created/linked\n\nFormat:\nYYYY-MM-DD HH:MM [agent=&lt;agent-id&gt;] [model=&lt;model&gt;] description\n\nAlways provide explicit --agent &lt;id&gt; - never use placeholders like auto or &lt;AGENT_NAME&gt;.\nState Transitions\n# Move to InProgress\npython -m kano_backlog_cli.main item update-state --id &lt;ID&gt; --state InProgress --agent &lt;agent-id&gt;\n \n# Move to Done\npython -m kano_backlog_cli.main item update-state --id &lt;ID&gt; --state Done --agent &lt;agent-id&gt;\nADR Creation\npython -m kano_backlog_cli.main adr create \\\n    --title &quot;Decision title&quot; \\\n    --product kano-agent-backlog-skill \\\n    --agent &lt;agent-id&gt;\nAgent-Specific Rules\nGitHub Copilot\nFollow commit guidelines in .github/copilot-instructions.md:\n\nUse Kano backlog IDs directly in commit messages\nPreferred format: KABSD-TSK-0146: &lt;short summary&gt;\nMultiple items: KABSD-TSK-0146 KABSD-TSK-0147: &lt;short summary&gt;\nDo NOT use jira# prefix\n\nBacklog system note:\n\nThis repository uses kano-backlog as the system of record, not Jira.\nDo not add any jira# or JIRA: prefixes. Reference Kano IDs directly.\nExamples:\n\nGood: KABSD-TSK-0261: refine filename truncation\nBad: jira#KABSD-TSK-0261\n\n\n\nAgent Identity\nValid agent IDs for worklog entries:\n\ncopilot, codex, claude, goose, antigravity, cursor, windsurf, opencode, kiro, amazon-q\n\nForbidden: &lt;AGENT_NAME&gt;, $AGENT_NAME\nArchitecture Rules (ADR-0013)\nModule Boundaries\n\nkano_backlog_core/ - Import-only, no executable code\nkano_backlog_ops/ - Use cases and business logic\nkano_backlog_cli/ - Executable CLI commands\nscripts/ - Entry point scripts\n\nCross-Package Imports\nUse absolute imports from kano_backlog_core when importing from other packages:\nfrom kano_backlog_core import BacklogItem, ItemState\nNever import directly from kano_backlog_ops from CLI or scripts.\nInspector Pattern (ADR-0037)\nPrinciple: Skill core provides query surface (deterministic data extraction). All “expert judgment” lives in external inspector agents.\nWhat This Means:\n\nCore provides: audit, snapshot, constellation, workitem.query, doc.resolve APIs (all read-only, deterministic)\nInspector agents consume: Query APIs to produce health reports, review suggestions, refactor recommendations\nEvidence required: Every inspector finding must cite file path + line range + item/ADR ID\n\nPattern:\n# Inspector agent calls query surface\nkano-backlog query snapshot --format json &gt; snapshot.json\nhealth-inspector --input snapshot.json --output report.json\n \n# Report includes evidence\n{\n  &quot;finding_id&quot;: &quot;F-001&quot;,\n  &quot;assessment&quot;: &quot;5 tasks missing Context field&quot;,\n  &quot;evidence&quot;: [\n    {\n      &quot;item_id&quot;: &quot;KABSD-TSK-0042&quot;,\n      &quot;file&quot;: &quot;_kano/backlog/items/task/0000/KABSD-TSK-0042.md&quot;,\n      &quot;line_range&quot;: [25, 30]\n    }\n  ]\n}\nInspector Types (all external to core):\n\nHealth/Ideas: 3+3 questions, gap analysis, anti-patterns\nReviewer: Code review suggestions, best practices\nArchitect: Refactoring recommendations, design improvements\nSecurity: Threat model, vulnerability assessment\n\nKey: Inspectors are replaceable. Any agent can implement the contract. Core never hardcodes “this backlog is healthy/unhealthy” conclusions.\nSee ADR-0037 for full architecture.\nCommon Data Structures\n\nBacklogItem - Core work item model with frontmatter\nItemType - Enum: EPIC, FEATURE, USER_STORY, TASK, BUG\nItemState - Enum: Proposed, Planned, Ready, InProgress, Blocked, Done, Dropped\nChunkingOptions, TokenBudgetPolicy - Configuration for chunking system\n\nCommon Error Types\n\nItemNotFoundError - Item file doesn’t exist\nParseError - Invalid frontmatter or markdown\nValidationError - Data validation failed\nConfigError - Configuration error\nWriteError - Write operation failed\n\n\nRemember: This is a living document. Update it as patterns evolve in the codebase.\nCanonical + Adapters Architecture\n\n\n                  \n                  IMPORTANT\n                  \n                \n\nThis repo uses a “canonical source + adapters” layout to support multiple AI coding agents.\n\n\nCanonical Source (Single Source of Truth)\n\nAll skill documentation lives in: skills/&lt;skill-name&gt;/SKILL.md\nAlways read the canonical SKILL.md - adapters are just entry points\n\nAdapters (Entry Points for Different Agents)\n\nGitHub Copilot: .github/skills/&lt;skill-name&gt;/SKILL.md (thin wrapper with links to canonical)\nOpenAI Codex: .codex/skills/&lt;skill-name&gt;/SKILL.md (thin wrapper with name/description and links)\nAnthropic Claude: .claude/skills/&lt;skill-name&gt;/SKILL.md (compatible with Claude Code/Desktop)\nGoose: .goose/skills/&lt;skill-name&gt;/SKILL.md (open-source agent compatible with Claude skills)\nGoogle Antigravity: .agent/skills/&lt;skill-name&gt;/SKILL.md (native workspace skills)\nClaude Code: CLAUDE.md (root wrapper pointing back to this file)\nUniversal: AGENTS.md (this file) enforces workflow rules\nModular: Skills are self-contained in skills/ directory\n\nWorkflow Enforcement\n\nBefore using any skill: Open and read the canonical skills/&lt;skill-name&gt;/SKILL.md\nIf you only see a summary/wrapper, follow the links to canonical sections\nRun doctor or verification commands mentioned in canonical docs\n\nKey paths\n\nSkill (submodule): skills/kano-agent-backlog-skill/\n\nCanonical rules: skills/kano-agent-backlog-skill/SKILL.md ← READ THIS\nCopilot adapter: .github/skills/kano-agent-backlog-skill/SKILL.md\nCodex adapter: .codex/skills/kano-agent-backlog-skill/SKILL.md\nClaude adapter: .claude/skills/kano-agent-backlog-skill/SKILL.md\nGoose adapter: .goose/skills/kano-agent-backlog-skill/SKILL.md\nAntigravity adapter: .agent/skills/kano-agent-backlog-skill/SKILL.md\nReferences: skills/kano-agent-backlog-skill/references/\n\n\nSkill: skills/kano-commit-convention-skill/\n\nCanonical rules: skills/kano-commit-convention-skill/SKILL.md ← READ THIS\nCopilot adapter: .github/skills/kano-commit-convention-skill/SKILL.md\nCodex adapter: .codex/skills/kano-commit-convention-skill/SKILL.md\nClaude adapter: .claude/skills/kano-commit-convention-skill/SKILL.md\nGoose adapter: .goose/skills/kano-commit-convention-skill/SKILL.md\nAntigravity adapter: .agent/skills/kano-commit-convention-skill/SKILL.md\n\n\nUniversal Rules: AGENTS.md (this file)\nClaude Code: CLAUDE.md (root wrapper pointing to AGENTS.md)\nDemo backlog (system of record): _kano/backlog/\n\nItems: _kano/backlog/items/\nADRs: _kano/backlog/decisions/\nViews: _kano/backlog/products/&lt;product&gt;/views/\nTools (project-specific): _kano/backlog/tools/ (project-only views/dashboards)\n\n\n\nBacklog discipline (this repo)\n\nUse skills/kano-agent-backlog-skill/SKILL.md for any planning/backlog work.\nIf Python deps are missing, install them with python -m pip install -e skills/kano-agent-backlog-skill (add [dev] when developing the skill itself).\nBefore any code change, create/update items in _kano/backlog/items/ (Epic → Feature → UserStory → Task/Bug).\n\n\n\n                  \n                  IMPORTANT\n                  \n                \n\nStrictly English Only: All backlog item content (Context, Goal, Approach, Worklog, etc.) MUST be written in English. This is a hard requirement for this demo to ensure accessibility for all agents.\n\n\n\nEnforce the Ready gate on Task/Bug (required, non-empty): Context, Goal, Approach, Acceptance Criteria, Risks / Dependencies.\nWorklog is append-only; never rewrite history. Append a Worklog line whenever:\n\na load-bearing decision is made,\nan item state changes,\nscope/approach changes,\nor an ADR is created/linked.\n\n\nUse python skills/kano-agent-backlog-skill/scripts/kano item update-state ... for state transitions so state, updated, and Worklog stay consistent.\nNeed a new backlog product? Run python skills/kano-agent-backlog-skill/scripts/kano backlog init --product &lt;name&gt; --agent &lt;id&gt; to scaffold _kano/backlog/products/&lt;name&gt;/ before creating items.\nFor backlog/skill file operations, go through the kano CLI so audit logs capture the action (no ad-hoc file edits).\nSkill scripts refuse paths outside _kano/backlog/ or _kano/backlog_sandbox/.\nKeep backlog volume under control: only open new items for code/design changes; keep Tasks/Bugs sized to one focused session; avoid ADRs unless there is a real architectural trade-off.\nTicketing threshold (agent-decided):\n\nOpen a new Task/Bug when you will change code/docs/views/scripts.\nOpen an ADR (and link it) when a real trade-off or direction change is decided.\nOtherwise, record the discussion in an existing Worklog; ask if unsure.\n\n\nState ownership: the agent decides when to move items to InProgress or Done; humans observe and can add context.\n\nNaming and storage rules (short)\n\nStore items under _kano/backlog/items/&lt;type&gt;/&lt;bucket&gt;/ and bucket per 100 (0000, 0100, …).\nFilenames are stable: &lt;ID&gt;_&lt;slug&gt;.md (ASCII slug).\nFor Epics, create an adjacent &lt;ID&gt;_&lt;slug&gt;.index.md MOC and register it in _kano/backlog/_meta/indexes.md.\n\nViews (human-friendly)\n\nObsidian Dataview dashboards live under product view roots (e.g. _kano/backlog/products/&lt;product&gt;/views/Dashboard.md).\nGenerate the canonical dashboards via the CLI: python skills/kano-agent-backlog-skill/scripts/kano-backlog view refresh --agent &lt;id&gt; --backlog-root _kano/backlog --product &lt;name&gt;.\n\nNote: _kano/backlog/tools/*.sh are deprecated; use Python tools instead when needed (e.g. generate_demo_views.py, generate_focus_view.py).\n\n\n\nDemo principles\n\nKeep the demo backlog small and traceable; avoid ticket spam.\nAvoid unrelated refactors; every meaningful change should be explainable via a backlog item or ADR (with verification steps).\nIf you change the skill itself, commit inside the submodule skills/kano-agent-backlog-skill/ and update the parent repo submodule pointer.\nSelf-contained skill stance (this demo repo):\n\nPrefer adding automation as new kano subcommands so the skill is usable without manual setup.\nKeep _kano/backlog/tools/ for project-only dashboards/demos (wrapping skill scripts is OK when the behavior is demo-specific).\nOther projects may choose override-only usage; this repo does not. Treat the skill as the source of truth.\n\n\n\nTests\nNo tests or build steps are defined yet.\nTemporary Clause: Local-first First, No Server Implementation Yet\nEffective immediately, this project prioritizes local-first completion and hardening.\nAllowed (Encouraged)\n\nAny work that improves local-first workflows and quality, including:\n\nFile-based canonical data design, schema refinement, validation, and migration tooling\nLocal indexing/search (e.g., SQLite/FTS/sidecar ANN), ingest pipelines, and performance work\nCLI scripts, automation scripts, and developer tooling\nDocumentation, ADRs, threat models, and evaluations for future cloud/server support\nDesigning server interfaces (API/MCP schemas) as documentation/spec only\n\n\n\nNot Allowed (Hard Stop)\n\nDo not implement any server runtime or deployable server component, including but not limited to:\n\nHTTP server, REST API service, gRPC service\nMCP server (any transport)\nWeb UI that depends on a running server\nDocker/K8s deployment for a server component\nAuthentication/authorization implementation as runnable server code\n\n\nDo not add runtime dependencies whose primary purpose is server hosting (unless explicitly approved).\n\nRe-enabling Condition\n\nThis clause remains in effect until a human explicitly removes or disables it.\nAny request that appears to require server implementation must be treated as “spec-only” and should produce:\n\nan ADR and/or design doc,\na roadmap ticket proposal,\na clear note that implementation is deferred due to this clause.\n\n\n\nRationale\n\nKeep the project focused on local-first stability and usability before expanding to cloud/multi-remote deployments.\n\n\nProject backlog discipline (kano-agent-backlog-skill)\n\nUse skills/kano-agent-backlog-skill/SKILL.md for any planning/backlog work.\nBacklog root is _kano/backlog_sandbox/_tmp_tests/guide_test_backlog (items are file-first; index/logs are derived).\nBefore any code change, create/update items in _kano/backlog_sandbox/_tmp_tests/guide_test_backlog/items/ (Epic → Feature → UserStory → Task/Bug).\nEnforce the Ready gate on Task/Bug before starting; Worklog is append-only.\nUse the kano CLI (not ad-hoc edits) so audit logs capture actions:\n\nBootstrap: python skills/kano-agent-backlog-skill/scripts/kano backlog init --product &lt;name&gt; --agent &lt;agent-name&gt;\nCreate/update: python skills/kano-agent-backlog-skill/scripts/kano item create|update-state ... --agent &lt;agent-name&gt;\nViews: python skills/kano-agent-backlog-skill/scripts/kano view refresh --agent &lt;agent-name&gt; --product &lt;name&gt;\n\n\nDashboards auto-refresh after item changes by default (views.auto_refresh=true); use --no-refresh or set it to false if needed.\n\n"},"demo/claude":{"title":"claude","links":[],"tags":[],"content":"CLAUDE.md\n\n\n                  \n                  IMPORTANT\n                  \n                \n\nThis repo uses a “canonical source + adapters” layout.\n\n\n📖 Global Instructions\nFor project-wide rules, architecture, and backlog discipline, please READ AND FOLLOW:\r\n👉 @AGENTS.md\n📦 Agent Skills\nThis repo provides specialized skills in the following locations. Always read the canonical source within each skill folder:\n\nBacklog Management: @skills/kano-agent-backlog-skill/SKILL.md\nCommit Conventions: @skills/kano-commit-convention-skill/SKILL.md\n\n\n🛠️ Quick Start for Claude\n\nExplore the codebase: Read @AGENTS.md for terminal commands and style guides.\nBacklog first: Before taking any action, check the backlog in _kano/backlog/.\nCanonical skills: Refer to the skills/ directory for tool-specific instructions (e.g., @skills/kano-agent-backlog-skill/SKILL.md).\n"},"demo/index":{"title":"Demo & Examples","links":["/","demo/agents","demo/claude"],"tags":[],"content":"Demo &amp; Examples\nDemonstration repository and usage examples\n\nDemo Overview\nAgent Guidelines\nClaude Integration\n"},"examples/index":{"title":"Examples & Templates","links":[],"tags":[],"content":"Examples &amp; Templates\nCode examples and templates"},"index":{"title":"Kano Agent Backlog Skill","links":["demo/","skill/","adr/","examples/","references/"],"tags":[],"content":"Kano Agent Backlog Skill\nAI Agent Skills for Spec-Driven Agentic Programming\nNavigation\n\nDemo &amp; Examples - Demonstration repository and usage examples\nSkill Documentation - Core skill implementation and reference\nArchitecture Decisions - Architecture Decision Records and design rationale\nExamples &amp; Templates - Code examples and templates\nTechnical References - Technical documentation and references\n"},"references/index":{"title":"Technical References","links":[],"tags":[],"content":"Technical References\nTechnical documentation and references"},"skill/index":{"title":"Skill Documentation","links":[],"tags":[],"content":"Skill Documentation\nCore skill implementation and reference"}}